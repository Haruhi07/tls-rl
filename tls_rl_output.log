 
 University of Bristol ACRC HPC Bluepebble Service
 -------------------------------------------------
 Job tls_rl, jobid 3598235.bp1, username hs20307 - started execution at 03:41:17 Tue 31/08/21 on node bp1-gpu00030.data.bp.acrc.priv
 
adding bpoil_bbc into dataset...
topic:  ('bpoil_bbc',)
env initialized...

Before training:  88.8% (21555 out of 24268)
{'input_ids': tensor([[[  110,   105,  3893, 15737,  1034,  8255,   464, 16036,   134,   109,
           1816,  2447,   139,  1816,  2447,   649,   126,   117,  4562,   112,
          16036,   118,   203,   337,   983,   762,   121,  2134,  5626,   148,
            174, 12832,   279,   156,   113,   109,  1816,  2447,  1034,   116,
          18072,   141, 10162, 18232,   126,   112,   370,   203, 10971,   818,
            122, 16036,   110,   107,   139,  4635, 41588,   110,   108, 50595,
          81452, 60490,   457,   131,   304,   110,   108,   117,   114,  4055,
          17225,   113,   114,   883,   693,   111, 28286,   111,   117,   160,
           6155,   231,   459,   110,   107,   202,   456,   568,  7200,  9221,
           3893,  2777,   165,   109,  8255,   110,   107,   202,   984,  8255,
           4635, 20678,  4329,   115,  1185,   110,   107,   139,  1816,  2447,
            243,   109,  5626,  1065,   140, 39059,   110,   108,   162,   196,
            146,  3954,   109,  4976,  2098, 11976,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)

Everytime after generating next logits 88.8% (21555 out of 24268)
Haka Hoa Hoa Hoa Hoa Haka Haka Haka called a giant protest to the British Culture . Giant protest called the British Island . urging the British Culture . statue to the British Culture .
iter = 0 reward = 6
final_logits =  tensor([[[ 0.0000e+00,  3.6703e+00,  5.8588e-01,  ..., -2.2004e+00,
           1.3281e+00, -2.8097e+00],
         [ 0.0000e+00,  4.4577e+00, -9.0041e-02,  ..., -1.8934e+00,
          -5.8645e-01, -2.9442e+00],
         [ 0.0000e+00,  4.6796e+00, -1.0267e-01,  ..., -2.3314e+00,
          -9.1425e-01, -3.1820e+00],
         ...,
         [ 0.0000e+00,  6.0788e+00, -2.4887e-01,  ..., -3.7562e+00,
           1.6343e-01, -1.4650e+00],
         [ 0.0000e+00,  3.9407e+00,  7.0885e-02,  ..., -1.5233e+00,
           5.4544e-02, -1.3282e-03],
         [ 0.0000e+00,  1.2026e+01,  1.3557e-02,  ..., -2.8110e+00,
           1.0015e+00, -5.1865e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.7179, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(13.9624, device='cuda:0', grad_fn=<MseLossBackward>)
