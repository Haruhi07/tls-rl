 
 University of Bristol ACRC HPC Bluepebble Service
 -------------------------------------------------
 Job tls_rl, jobid 3595971.bp1, username hs20307 - started execution at 01:09:04 Fri 27/08/21 on node bp1-gpu00001.data.bp.acrc.priv
 
adding bpoil_bbc into dataset...
topic:  ('bpoil_bbc',)
env initialized...

Before training:  23.2% (2552 out of 11019)
{'input_ids': tensor([[[  110,   105,  3893, 15737,  1034,  8255,   464, 16036,   134,   109,
           1816,  2447,   139,  1816,  2447,   649,   126,   117,  4562,   112,
          16036,   118,   203,   337,   983,   762,   121,  2134,  5626,   148,
            174, 12832,   279,   156,   113,   109,  1816,  2447,  1034,   116,
          18072,   141, 10162, 18232,   126,   112,   370,   203, 10971,   818,
            122, 16036,   110,   107,   139,  4635, 41588,   110,   108, 50595,
          81452, 60490,   457,   131,   304,   110,   108,   117,   114,  4055,
          17225,   113,   114,   883,   693,   111, 28286,   111,   117,   160,
           6155,   231,   459,   110,   107,   202,   456,   568,  7200,  9221,
           3893,  2777,   165,   109,  8255,   110,   107,   202,   984,  8255,
           4635, 20678,  4329,   115,  1185,   110,   107,   139,  1816,  2447,
            243,   109,  5626,  1065,   140, 39059,   110,   108,   162,   196,
            146,  3954,   109,  4976,  2098, 11976,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 23.2% (2552 out of 11019)

Everytime after generating next logits 24.7% (2718 out of 11019)

Everytime after generating next logits 24.7% (2720 out of 11019)

Everytime after generating next logits 24.7% (2720 out of 11019)

Everytime after generating next logits 24.7% (2720 out of 11019)

Everytime after generating next logits 24.9% (2740 out of 11019)

Everytime after generating next logits 24.9% (2740 out of 11019)

Everytime after generating next logits 24.9% (2740 out of 11019)

Everytime after generating next logits 24.9% (2740 out of 11019)

Everytime after generating next logits 24.9% (2740 out of 11019)

Everytime after generating next logits 24.9% (2740 out of 11019)

Everytime after generating next logits 24.9% (2740 out of 11019)

Everytime after generating next logits 24.9% (2740 out of 11019)

Everytime after generating next logits 24.9% (2740 out of 11019)

Everytime after generating next logits 24.9% (2740 out of 11019)

Everytime after generating next logits 24.9% (2740 out of 11019)

Everytime after generating next logits 24.9% (2740 out of 11019)

Everytime after generating next logits 25.0% (2760 out of 11019)

Everytime after generating next logits 25.0% (2760 out of 11019)

Everytime after generating next logits 25.0% (2760 out of 11019)

Everytime after generating next logits 25.0% (2760 out of 11019)

Everytime after generating next logits 25.0% (2760 out of 11019)

Everytime after generating next logits 25.0% (2760 out of 11019)

Everytime after generating next logits 25.0% (2760 out of 11019)

Everytime after generating next logits 25.0% (2760 out of 11019)

Everytime after generating next logits 25.0% (2760 out of 11019)

Everytime after generating next logits 25.0% (2760 out of 11019)

Everytime after generating next logits 25.0% (2760 out of 11019)

Everytime after generating next logits 25.2% (2772 out of 11019)

Everytime after generating next logits 25.3% (2784 out of 11019)

Everytime after generating next logits 25.3% (2784 out of 11019)

Everytime after generating next logits 25.3% (2784 out of 11019)

Everytime after generating next logits 25.3% (2784 out of 11019)
Haka Hoa Hoa Hoa Hoa Haka Haka Haka called a giant protest to the British Culture . Campaigners called the British Island sponsorship to the British Culture .
iter = 0 reward = 5
final_logits =  tensor([[[ 0.0000,  3.6580,  0.5854,  ..., -2.1995,  1.3356, -2.8146],
         [ 0.0000,  4.4621, -0.0904,  ..., -1.8895, -0.5902, -2.9494],
         [ 0.0000,  4.6797, -0.1028,  ..., -2.3253, -0.9177, -3.1843],
         ...,
         [ 0.0000,  4.3877, -0.3404,  ..., -3.4207,  0.0982, -1.2656],
         [ 0.0000,  3.3689,  0.0333,  ..., -1.4281,  0.0344,  0.0885],
         [ 0.0000, 11.5623,  0.1148,  ..., -1.7734,  0.8034, -0.4692]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.6059, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.2559, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.8% (4278 out of 11019)

Everytime after generating next logits 38.8% (4278 out of 11019)

Everytime after generating next logits 38.8% (4278 out of 11019)

Everytime after generating next logits 38.8% (4278 out of 11019)
Protesters called British Culture sponsorship to the British Island . An protest called a giant protest called the British Island sponsorship to the British Culture .
iter = 1 reward = 7
final_logits =  tensor([[[-6.6948e-02,  4.1954e+00,  1.0083e+00,  ..., -4.3272e+00,
           3.6118e+00,  4.1354e-01],
         [-6.0465e-02,  4.6031e+00,  1.3976e+00,  ..., -4.2554e+00,
           2.3424e+00,  1.3241e+00],
         [-4.3776e-02,  3.0644e+00, -2.7923e-01,  ..., -3.2850e+00,
          -1.1000e+00, -2.6220e+00],
         ...,
         [-8.5718e-02,  4.0426e+00, -3.9628e-01,  ..., -7.5657e-01,
           9.1802e-01, -2.3569e+00],
         [-2.7120e-02,  3.1872e+00, -8.4380e-02,  ..., -1.0511e+00,
           1.8659e-01, -7.2610e-03],
         [-6.5830e-02,  1.1798e+01,  3.7215e-02,  ..., -2.0048e+00,
           5.2475e-01, -1.5937e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-246.1879, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(122339.8281, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)
The British Museum says a human oil relic is one of its . support for its deal against the BP .
iter = 2 reward = 5
final_logits =  tensor([[[ 6.9665e-03,  4.8446e+00,  1.0310e+00,  ..., -3.3492e+00,
           1.6728e+00, -8.5685e-01],
         [-8.8573e-04,  2.9748e+00,  6.6524e-01,  ..., -5.7331e+00,
           2.0414e+00, -2.3498e+00],
         [-1.9643e-02,  4.1785e+00,  2.6525e-01,  ..., -6.8396e-01,
           3.9025e-01,  3.9632e-01],
         ...,
         [-1.5332e-02,  4.4019e+00, -8.9692e-01,  ..., -4.1014e+00,
          -1.5617e+00, -1.6893e+00],
         [-5.5890e-03,  4.4604e+00, -1.1996e-01,  ..., -2.0334e+00,
           5.1884e-01, -4.9644e-01],
         [-1.7340e-03,  1.3092e+01, -2.7597e-01,  ..., -3.5961e+00,
          -5.8839e-01, -1.3233e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-35.1040, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(1664.2396, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)
Giant protest called a June sponsorship of a June sponsorship called a 'June sponsorship' has targeted activists .
iter = 3 reward = 4
final_logits =  tensor([[[ 6.4675e-02,  3.8958e+00,  1.1915e+00,  ..., -2.5908e+00,
           2.7159e+00,  1.1142e-01],
         [ 4.9887e-02,  2.3082e+00,  3.8236e-01,  ..., -9.5003e-01,
           2.8525e+00, -3.0408e+00],
         [ 4.4409e-02,  2.4136e+00, -4.4667e-02,  ..., -3.0647e+00,
           2.1308e-01, -3.1017e+00],
         ...,
         [ 2.5708e-02,  3.1253e+00, -2.1450e-01,  ..., -1.9887e+00,
           7.8081e-01, -4.7479e-01],
         [ 1.3795e-02,  3.1454e+00, -1.5610e-04,  ..., -9.0095e-01,
           6.4261e-01, -9.5555e-03],
         [ 5.2988e-02,  1.1046e+01,  4.3027e-01,  ..., -2.5627e+00,
           2.1026e-01, -1.6452e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(50.6999, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(1446.4330, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.9% (4284 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.2% (4314 out of 11019)

Everytime after generating next logits 39.3% (4330 out of 11019)

Everytime after generating next logits 39.3% (4330 out of 11019)

Everytime after generating next logits 39.3% (4330 out of 11019)

Everytime after generating next logits 39.3% (4330 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)
Giant protest called a June sponsorship of a June sponsorship in Britain has targeted a June protest called a June protest called a June sponsorship . A giant protest called a British Culture has targeted a British Easter Island protest called a June sponsorship .
iter = 4 reward = 13
final_logits =  tensor([[[ 0.0931,  3.1707,  1.1728,  ..., -2.9663,  2.8767, -0.6819],
         [ 0.0812,  2.3164,  0.4089,  ..., -0.9843,  3.2113, -3.3857],
         [ 0.0655,  2.0689, -0.0583,  ..., -2.9560,  0.5791, -3.0210],
         ...,
         [ 0.0734,  3.5141, -0.2571,  ..., -1.4108,  2.4479, -1.6520],
         [ 0.0329,  3.2006, -0.0678,  ..., -0.6751,  0.3946, -0.0576],
         [ 0.0945, 12.1897,  0.0890,  ..., -1.2132,  1.3912, -2.7853]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(51.5581, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(2274.3484, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  13
13
{'input_ids': tensor([[[ 4123,   113,   219,  6234,   195,  6908,   115,   109, 75158,  1153,
           3893,  1419, 16036,   148,  6305,  3906,   142, 10580,   805,   113,
            203,  7175,   113,  3064,   762, 14567,  1407,  1104,   124,   203,
            387,   110,   107,   139,  1082,   110,   108,  1155,   204,   109,
           1339,   110,   108,   939,  1841,   115,   683,   113,   114,  1679,
            113,   461,  6234, 10361,  1055,   113,   203,  3954,   210,   124,
            109,  1917,  1030,   110,   107, 16036,  9619,  3582,  7217,   243,
            120,   339,  6234,   195,  6908,   115,   109,   856,  1153,   111,
          10390,   680,   196,   174,   263,   112,   535,  1055,   110,   107,
            139, 10580,   805,   140,  3530,   122,   109,   856,   244,   114,
            787,  9024,  8666,   126,   110,   107,  1263,  7217,   243,   109,
           4787,   170,   635,   109,  1153,   140, 10361,   169,   766,   122,
          10390,   680,   111,   186,   140,   220,  5313,  6596,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)
Mr Dean spotted the blank blank blank software workers at the weekend .
iter = 0 reward = 1
final_logits =  tensor([[[ 4.7520e-02,  6.4825e+00,  8.0160e-01,  ..., -2.7225e+00,
           4.0272e-02,  1.3488e+00],
         [ 3.9859e-02,  5.8368e+00,  4.5093e-01,  ..., -3.0691e+00,
           4.1721e-03,  9.4519e-01],
         [ 4.2572e-02,  5.2314e+00,  2.1404e-01,  ..., -2.1508e+00,
          -8.0395e-01,  2.1712e-02],
         ...,
         [ 6.1803e-02,  5.8168e+00, -1.2243e-01,  ..., -2.4615e+00,
          -1.1201e+00, -7.9736e-01],
         [ 5.3657e-02,  6.2422e+00,  4.4522e-01,  ..., -2.5074e+00,
           1.3223e-01,  1.4628e+00],
         [ 4.8364e-02,  1.3607e+01,  4.8369e-01,  ..., -3.0190e+00,
          -3.6896e-01,  1.6528e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(27.8531, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(779.4804, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)
Mr Dean spotted three blank blank blank photo workers in Photoshop .
iter = 1 reward = 1
final_logits =  tensor([[[ 0.0503,  6.5839,  0.7929,  ..., -2.3275,  0.2162,  1.2865],
         [ 0.0418,  6.3414,  0.3267,  ..., -2.6094,  0.1049,  0.6823],
         [ 0.0489,  5.6201,  0.2114,  ..., -1.3392, -0.0318,  0.1088],
         ...,
         [ 0.0701,  6.1429, -0.2768,  ..., -3.2985, -1.7790, -0.3797],
         [ 0.0726,  8.4480,  0.6763,  ..., -2.6810,  0.4376,  1.9330],
         [ 0.0510, 13.2373,  0.4511,  ..., -2.8469, -0.3513,  1.6462]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(20.5440, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(429.7152, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)
Mr Dean spotted three blank blank photo workers on Sunday .
iter = 2 reward = 1
final_logits =  tensor([[[ 0.0452,  6.7617,  0.7481,  ..., -1.2659,  0.2979,  0.8105],
         [ 0.0401,  6.4894,  0.2317,  ..., -2.4432,  0.2920,  0.6050],
         [ 0.0475,  5.5731,  0.1609,  ..., -1.0498,  0.1900, -0.2475],
         ...,
         [ 0.0605,  5.1920, -0.1177,  ..., -0.6609, -0.7677, -1.2000],
         [ 0.0735,  8.5820,  0.6619,  ..., -1.8702,  0.8641,  1.1590],
         [ 0.0427, 13.3961,  0.3890,  ..., -1.7465, -0.6750,  1.4196]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(7.3510, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(125.3366, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)
Mr Dean spotted three blank blank photo workers on weekend spotted .
iter = 3 reward = 1
final_logits =  tensor([[[ 0.0389,  6.7756,  0.7349,  ..., -1.1820,  0.3494,  0.4844],
         [ 0.0392,  6.5938,  0.2394,  ..., -2.4557,  0.2363,  0.5837],
         [ 0.0437,  5.4963,  0.1267,  ..., -1.2521,  0.2220, -0.7167],
         ...,
         [ 0.0411,  6.6534,  0.0569,  ..., -1.7837,  0.7271, -1.2112],
         [ 0.0611,  9.0276,  1.1380,  ..., -2.3113,  1.7641,  1.9922],
         [ 0.0362, 13.5358,  0.3895,  ..., -1.4207, -0.6526,  1.2718]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.2915, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(10.1652, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)
Mr Dean spotted three blank blank photo workers on weekend .
iter = 4 reward = 1
final_logits =  tensor([[[ 0.0325,  6.7100,  0.6727,  ..., -1.1918,  0.2811,  0.0939],
         [ 0.0364,  6.5972,  0.2563,  ..., -2.3929,  0.1495,  0.4501],
         [ 0.0383,  5.2560,  0.1069,  ..., -1.3076,  0.0421, -0.7796],
         ...,
         [ 0.0237,  4.5585,  0.0387,  ..., -0.7198, -1.1117, -2.0379],
         [ 0.0622,  9.1379,  0.9346,  ..., -2.5195,  1.3430,  1.5685],
         [ 0.0301, 13.5069,  0.3166,  ..., -1.1544, -0.7874,  1.1679]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.3487, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(32.9401, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  1
1
{'input_ids': tensor([[[599, 960, 110,  ..., 109, 804,   1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 38.2% (4214 out of 11019)

Everytime after generating next logits 39.4% (4342 out of 11019)

Everytime after generating next logits 39.4% (4342 out of 11019)

Everytime after generating next logits 39.4% (4342 out of 11019)

Everytime after generating next logits 39.4% (4342 out of 11019)

Everytime after generating next logits 39.4% (4342 out of 11019)

Everytime after generating next logits 39.4% (4342 out of 11019)

Everytime after generating next logits 39.4% (4342 out of 11019)

Everytime after generating next logits 39.4% (4342 out of 11019)

Everytime after generating next logits 40.0% (4406 out of 11019)

Everytime after generating next logits 40.0% (4406 out of 11019)

Everytime after generating next logits 40.0% (4406 out of 11019)

Everytime after generating next logits 40.0% (4406 out of 11019)

Everytime after generating next logits 40.0% (4406 out of 11019)

Everytime after generating next logits 40.0% (4406 out of 11019)

Everytime after generating next logits 40.0% (4406 out of 11019)

Everytime after generating next logits 40.0% (4406 out of 11019)

Everytime after generating next logits 40.0% (4406 out of 11019)

Everytime after generating next logits 40.0% (4406 out of 11019)
BP hopes to halt cap cap on Gulf of Mexico's new drilling plan .
iter = 0 reward = 6
final_logits =  tensor([[[ 0.0329,  1.4787,  0.9781,  ..., -3.2731,  2.1876,  1.6701],
         [ 0.0638,  3.1178, -0.4532,  ..., -3.1201, -1.7735, -3.2969],
         [ 0.0589,  2.0623, -0.6607,  ..., -4.4486,  0.1045, -0.2187],
         ...,
         [ 0.0697,  3.6964, -1.0606,  ..., -1.6612, -1.1927, -2.1166],
         [ 0.0687,  4.9159,  0.1602,  ..., -1.0580, -0.0762,  0.6837],
         [ 0.0654, 11.1553, -0.6879,  ..., -3.4129,  0.4640, -1.9439]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(15.1647, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(132.6369, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4248 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)
BP hopes to halt cap cap on Gulf of Mexico's new drilling plan .
iter = 1 reward = 6
final_logits =  tensor([[[ 0.0273,  1.6099,  0.9525,  ..., -3.2414,  1.9949,  1.9545],
         [ 0.0641,  3.4596, -0.5348,  ..., -3.4522, -1.5677, -2.8381],
         [ 0.0568,  2.3613, -0.6845,  ..., -4.2860, -0.0761, -0.1477],
         ...,
         [ 0.0703,  3.7658, -1.0964,  ..., -1.5931, -1.0023, -1.8969],
         [ 0.0704,  4.9937,  0.1187,  ..., -1.0904, -0.0835,  0.7849],
         [ 0.0583, 11.7590, -0.6459,  ..., -3.3476,  0.1770, -1.7676]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.6931, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(16.5505, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4268 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)
BP hopes to halt cap cap on Gulf of Mexico's new drilling plan .
iter = 2 reward = 6
final_logits =  tensor([[[ 0.0334,  1.9279,  0.9844,  ..., -2.5266,  1.6542,  2.0071],
         [ 0.0609,  3.3474, -0.4539,  ..., -4.1441, -1.5440, -2.1782],
         [ 0.0513,  1.9051, -0.6461,  ..., -4.4058, -0.3296, -0.1168],
         ...,
         [ 0.0694,  3.8141, -1.0920,  ..., -2.1861, -1.1465, -1.8800],
         [ 0.0732,  4.8815,  0.1731,  ..., -0.9693, -0.2103,  0.8953],
         [ 0.0579, 11.5299, -0.6115,  ..., -3.2984,  0.2894, -1.5782]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.8546, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(49.0932, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4268 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)
Ad Kent giant Ad Kent Kent Kent, Ad Kent giant Ad Ad Ad Ad giant Ad Ad Ad Ad giant Ad Ad Ad Ad . BP hopes to halt cap on a new cap on a new US drilling plan . to stop the flow of a well from the first day .
iter = 3 reward = 8
final_logits =  tensor([[[ 0.0451,  2.2884,  0.8791,  ..., -2.6300,  2.1982,  1.2071],
         [ 0.0801,  3.4224,  0.5304,  ..., -2.0041, -0.1894, -0.3646],
         [ 0.0598,  2.4397, -0.0844,  ..., -3.4484, -1.0109, -3.3368],
         ...,
         [ 0.0372,  3.0991, -0.6258,  ..., -2.9695, -0.1955,  0.4729],
         [ 0.0692,  4.9550,  0.0225,  ..., -0.7612, -0.1210,  0.3374],
         [ 0.0508,  9.7887, -0.9073,  ..., -2.2168, -0.2899, -1.8723]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-15.4854, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(98.0623, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.8% (4386 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)
Ad. Wells has been linked to a new cap to the Gulf of Mexico .
iter = 4 reward = 4
final_logits =  tensor([[[ 5.5750e-02,  2.8909e+00,  7.9702e-01,  ..., -3.1895e+00,
           2.4203e+00,  8.1905e-01],
         [ 6.6210e-02,  3.2441e+00,  5.1710e-01,  ..., -2.2277e+00,
          -1.0687e-02, -7.3889e-01],
         [ 3.1601e-02,  5.0676e+00,  4.9633e-01,  ..., -2.3016e+00,
           4.4477e-01, -2.2108e-01],
         ...,
         [ 8.1725e-02,  1.1989e+00, -5.0542e-01,  ..., -3.5476e+00,
          -1.7096e+00, -4.0716e-01],
         [ 8.3029e-02,  4.4017e+00,  2.3603e-01,  ..., -2.0884e+00,
          -1.6945e-02,  1.1050e+00],
         [ 6.1267e-02,  1.1101e+01, -2.5619e-01,  ..., -3.6696e+00,
          -1.2493e-01, -1.4036e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-10.6739, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(101.4493, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  4
4
{'input_ids': tensor([[[6174,  692,  110,  ...,  526,  115,    1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 38.3% (4216 out of 11019)

Everytime after generating next logits 39.4% (4344 out of 11019)

Everytime after generating next logits 39.4% (4344 out of 11019)

Everytime after generating next logits 39.4% (4344 out of 11019)

Everytime after generating next logits 39.4% (4344 out of 11019)

Everytime after generating next logits 39.4% (4344 out of 11019)

Everytime after generating next logits 39.4% (4344 out of 11019)

Everytime after generating next logits 39.4% (4344 out of 11019)

Everytime after generating next logits 39.4% (4344 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)
U.S. expert David Overton: There's the biggest annual prediction of a spill on the Gulf of Mexico's coast to be a good environment .
iter = 0 reward = 6
final_logits =  tensor([[[ 5.8955e-02,  2.0520e+00,  7.7441e-01,  ..., -7.2387e+00,
           9.6141e-01, -4.3522e-01],
         [ 7.0647e-02,  3.7951e+00,  1.2798e+00,  ..., -2.2764e+00,
          -7.5685e-01, -1.1615e+00],
         [ 3.8750e-03,  2.6482e+00,  4.1213e-01,  ..., -7.9592e-01,
          -1.7045e+00, -9.8266e-03],
         ...,
         [ 7.7517e-02,  3.8513e+00, -7.8239e-01,  ..., -3.3904e+00,
           7.7044e-01, -1.9798e+00],
         [ 7.2297e-02,  3.9804e+00, -2.3190e-02,  ..., -2.6015e+00,
          -2.2606e-01,  6.2709e-01],
         [ 6.2240e-02,  1.2732e+01, -1.4513e-01,  ..., -6.5630e+00,
          -1.3025e+00,  3.9244e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.0880, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(8.5477, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.8% (4270 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 40.5% (4462 out of 11019)

Everytime after generating next logits 40.5% (4462 out of 11019)

Everytime after generating next logits 40.5% (4462 out of 11019)

Everytime after generating next logits 40.5% (4462 out of 11019)

Everytime after generating next logits 40.5% (4462 out of 11019)
Richard Overton: Deepwater spill is the biggest annual risk of a spill in the Gulf of Mexico's National Oceanic Society .
iter = 1 reward = 8
final_logits =  tensor([[[ 0.0685,  3.0226,  0.9352,  ..., -7.3492, -1.1591,  1.9186],
         [ 0.0387,  3.5043,  0.4293,  ..., -1.7023, -1.4125,  0.9674],
         [ 0.0522,  2.6260,  0.1612,  ..., -1.7633, -2.3035, -2.0378],
         ...,
         [ 0.0637,  3.0897, -0.7116,  ..., -2.9021, -2.3635, -1.0288],
         [ 0.0735,  4.1974,  0.2217,  ..., -2.5730, -0.3295,  1.5594],
         [ 0.0654, 13.4953,  0.2863,  ..., -5.7802, -2.1876,  2.1160]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(8.0187, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(46.2188, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.9% (4288 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)

Everytime after generating next logits 40.1% (4416 out of 11019)
U.S. U.S. University of Washington University of Washington University of Washington University of Washington .
iter = 2 reward = 0
final_logits =  tensor([[[ 5.2327e-02,  4.6387e+00,  1.3554e+00,  ..., -9.1088e+00,
          -6.4880e-01,  1.2421e+00],
         [ 6.9588e-02,  4.4948e+00,  1.2527e+00,  ..., -2.1708e+00,
          -9.2018e-01, -5.5195e-01],
         [-6.5534e-03,  3.4703e+00,  7.0288e-01,  ..., -1.0446e+00,
          -1.8097e+00,  5.8863e-02],
         ...,
         [ 7.0387e-02,  3.3627e+00, -2.4504e-01,  ..., -5.9143e+00,
          -2.6400e+00, -1.3903e+00],
         [ 9.3466e-02,  5.7340e+00,  3.5022e-01,  ..., -5.0685e+00,
          -1.4523e+00,  1.1431e+00],
         [ 5.5582e-02,  1.3966e+01,  6.1256e-01,  ..., -7.1777e+00,
          -1.3356e+00,  4.6585e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-1.1321, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(5.4373, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4262 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)
Gulf of Mexico Gulf of Mexico is the biggest environmental link to the spill in the year of 2010 .
iter = 3 reward = 6
final_logits =  tensor([[[ 0.0197,  3.6242,  0.8488,  ..., -5.4168,  0.6215,  0.7697],
         [ 0.0318,  1.9889,  0.1120,  ..., -4.4734, -1.0149, -0.6471],
         [ 0.0344,  0.8236,  0.1794,  ..., -3.2125, -0.1740, -0.5195],
         ...,
         [ 0.0450,  5.0248, -0.5963,  ..., -3.3922, -0.3301, -0.8968],
         [ 0.0654,  4.3739,  0.1453,  ..., -2.1946,  0.2601,  1.2892],
         [ 0.0388, 13.3662,  0.1101,  ..., -5.8475,  0.2463, -0.6415]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(11.6295, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(54.2103, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4260 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)
11 birds and seven birds were killed by more than a single spill in 2010 than the amount of the spill .
iter = 4 reward = 4
final_logits =  tensor([[[ 0.0159,  2.8291,  0.6275,  ..., -6.2682, -0.2288, -2.7582],
         [ 0.0393,  3.0516,  0.3118,  ..., -5.4524, -0.6465,  0.1594],
         [ 0.0368,  1.7114, -0.2072,  ..., -2.4018,  0.1238, -1.2169],
         ...,
         [ 0.0784,  3.3344, -0.9747,  ..., -2.4656, -0.5810, -2.4648],
         [ 0.0772,  5.0108,  0.0684,  ..., -2.5221,  0.1467,  0.3348],
         [ 0.0528, 12.6704, -0.1527,  ..., -5.3071, -0.8979, -1.3891]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(4.7487, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(10.9312, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  4
4
{'input_ids': tensor([[[  139,  5543,  9254,   320,  3702,   142,   865,   728,   113,   109,
          16036,   762, 14567,   110,   107,  8916,   114,  1713,   113,  5012,
            122,   662,   503, 32908,   110,   108,   330,   278,  2668,   116,
            122,  4605, 35522,   110,   108,   109,   177,  3378,   113, 16036,
            111,  6061, 32886,   110,   108,   169, 15978,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1]]])}

Before Sampling 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)
Tony Hayward is head of oil industry insiders and BP's chief of the oil industry .
iter = 0 reward = 3
final_logits =  tensor([[[ 0.0731,  4.5834,  0.3051,  ..., -2.4865,  1.8499, -2.3304],
         [ 0.0577,  4.0443, -0.0555,  ..., -2.9324,  0.4876, -0.5822],
         [ 0.0739,  4.4749, -0.4029,  ..., -3.9767, -1.7532, -1.2196],
         ...,
         [ 0.0867,  5.6323, -0.7340,  ..., -4.0296, -1.0225, -1.9782],
         [ 0.0704,  4.0489, -0.0743,  ..., -1.1649, -0.9228, -0.8393],
         [ 0.0840, 12.8533, -0.2684,  ..., -1.5264, -0.2862, -1.4877]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(4.0973, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(15.6887, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)
Money Head of Money and BP head of Dudley Head of Dudley head of oil industry insiders .
iter = 1 reward = 2
final_logits =  tensor([[[ 0.0718,  3.9726,  0.3855,  ..., -2.0741,  1.6345, -1.7018],
         [ 0.0691,  2.1216, -0.2576,  ..., -4.5516, -0.3191, -2.1598],
         [ 0.0499,  1.8657, -0.2907,  ..., -4.9954, -1.0345, -0.6342],
         ...,
         [ 0.0636,  5.3883, -0.6757,  ..., -3.8903, -2.3535, -1.5776],
         [ 0.0760,  4.9838,  0.0539,  ..., -1.4603, -1.0659, -0.2544],
         [ 0.0743, 12.6283, -0.2390,  ..., -1.8543, -0.8640, -1.0751]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.2397, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(10.1681, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)
Money Head of Dudley Head of oil industry insiders Tony Hayward and Tony Hayward .
iter = 2 reward = 3
final_logits =  tensor([[[ 0.0687,  3.6151,  0.3698,  ..., -1.8367,  1.0802, -1.5109],
         [ 0.0635,  1.8839, -0.2314,  ..., -4.7958, -0.5378, -2.2570],
         [ 0.0437,  1.6650, -0.3165,  ..., -4.8399, -1.6776, -0.6550],
         ...,
         [ 0.0934,  6.2574, -0.8201,  ..., -3.2964, -3.0115, -1.8524],
         [ 0.0897,  5.9572,  0.0594,  ..., -1.8841, -1.8237, -1.1568],
         [ 0.0751, 12.6511, -0.3121,  ..., -1.7037, -0.8776, -1.2972]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.7237, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(7.2612, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)
Money Head of Dudley Head of Dudley head of oil industry insiders Tony Hayward .
iter = 3 reward = 2
final_logits =  tensor([[[ 0.0652,  3.4607,  0.3221,  ..., -1.7929,  0.6539, -1.2947],
         [ 0.0572,  1.8748, -0.0915,  ..., -4.9689, -0.1911, -1.9660],
         [ 0.0388,  1.5791, -0.2972,  ..., -4.8171, -2.0809, -0.6238],
         ...,
         [ 0.0752,  5.6965, -0.6344,  ..., -2.3409, -2.4053, -1.6612],
         [ 0.0736,  5.6203,  0.0758,  ..., -2.1309, -1.8908, -0.7826],
         [ 0.0700, 12.6058, -0.2941,  ..., -1.7176, -1.1308, -1.1525]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-3.0937, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(14.6869, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)
Money Head Head interviews Tony Hayward, Tony Dudley and Tony Hayward .
iter = 4 reward = 1
final_logits =  tensor([[[ 0.0619,  3.5473,  0.3416,  ..., -2.1252,  0.7565, -1.3048],
         [ 0.0562,  2.2982,  0.0422,  ..., -4.9950,  0.1116, -1.5484],
         [ 0.0358,  1.7726, -0.2052,  ..., -4.7719, -2.1699, -0.5020],
         ...,
         [ 0.0736,  5.7185, -0.7348,  ..., -2.7052, -2.3859, -2.1501],
         [ 0.0801,  5.4841,  0.0589,  ..., -1.9131, -1.6391, -0.3908],
         [ 0.0738, 12.7472, -0.3255,  ..., -1.7689, -1.1732, -1.1725]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.9759, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(21.5735, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  1
1
{'input_ids': tensor([[[ 1084, 18756,   110,  ...,  5135,   378,     1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 38.1% (4196 out of 11019)

Everytime after generating next logits 39.2% (4324 out of 11019)

Everytime after generating next logits 39.2% (4324 out of 11019)

Everytime after generating next logits 39.2% (4324 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)
Environmental chief says BP will be responsible for spill spill in a spill .
iter = 0 reward = 6
final_logits =  tensor([[[ 0.0494,  2.2283,  0.8059,  ..., -6.8874,  1.7167,  0.7941],
         [ 0.0486,  0.8208,  0.3069,  ..., -3.4686,  0.0161,  1.2831],
         [ 0.0362,  2.6984, -0.3109,  ..., -5.6590, -0.8180, -1.9746],
         ...,
         [ 0.0532,  3.6115, -0.7935,  ..., -2.9220, -0.1286, -1.6019],
         [ 0.0985,  6.8579,  0.2643,  ..., -3.2983,  0.6045,  1.6213],
         [ 0.0660, 12.9028, -0.2249,  ..., -3.4987,  0.1435, -1.2484]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.0293, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.3957, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4268 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)
BP boss Tony Hayward says he will use box to stem spill spill spill on a spill .
iter = 1 reward = 9
final_logits =  tensor([[[ 0.0565,  1.9354,  0.7707,  ..., -6.9791,  2.6376,  0.1229],
         [ 0.0895,  1.1363, -0.2871,  ..., -4.8765, -2.1995, -2.7903],
         [ 0.0403,  2.2978, -0.4168,  ..., -5.3746, -1.1642, -2.0216],
         ...,
         [ 0.0409,  2.4777, -0.6206,  ..., -2.7906, -0.0329, -1.5116],
         [ 0.0944,  7.1244,  0.1506,  ..., -2.7435,  0.0956,  1.1888],
         [ 0.0677, 12.5049, -0.3792,  ..., -3.3943,  0.2547, -1.0775]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.6946, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(16.5323, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.8% (4270 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)

Everytime after generating next logits 39.9% (4398 out of 11019)
BP boss says he reached a US environmental conference on a box called a box called a ''box' that will stem a leak of a spill .
iter = 2 reward = 10
final_logits =  tensor([[[ 0.0529,  2.6900,  0.7856,  ..., -6.5701,  2.3366, -0.4123],
         [ 0.0866,  1.5877, -0.2725,  ..., -5.0956, -2.2475, -2.9981],
         [ 0.0368,  2.4414, -0.3650,  ..., -5.2155, -1.7565, -1.8769],
         ...,
         [ 0.0618,  2.9808, -0.7337,  ..., -2.8152,  1.5363,  0.7856],
         [ 0.0892,  5.6933,  0.1405,  ..., -2.8212,  0.3774,  1.7986],
         [ 0.0726, 11.6275, -0.6606,  ..., -3.8723,  0.4716, -0.6148]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.9234, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(17.9559, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4262 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)
BP boss John Hayward reveals a box called a box called a box called a ''box'
iter = 3 reward = 8
final_logits =  tensor([[[ 0.0530,  3.5886,  0.7776,  ..., -6.2930,  1.6740, -0.8385],
         [ 0.0850,  2.5771, -0.2179,  ..., -5.3190, -2.0740, -2.7381],
         [ 0.0361,  2.7723, -0.2003,  ..., -4.8643, -2.0683, -1.2560],
         ...,
         [ 0.0576,  6.2077,  1.3563,  ..., -0.6304, -0.2234,  0.5299],
         [ 0.0582,  6.0427, -0.6313,  ..., -3.8922, -0.7045, -0.6604],
         [ 0.0753, 11.7877, -0.7526,  ..., -1.9356, -1.5643, -0.3724]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.1520, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(10.4298, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4260 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)
BP boss reveals how BP will use a box to bury a spill to a spill .
iter = 4 reward = 5
final_logits =  tensor([[[ 0.0426,  5.1491,  0.6069,  ..., -4.9067, -0.2324, -1.4965],
         [ 0.0797,  3.3831, -0.1894,  ..., -4.8878, -0.7629, -2.8047],
         [ 0.0283,  3.7628, -0.0966,  ..., -3.4791, -1.4107, -0.9408],
         ...,
         [ 0.0444,  4.3329, -0.7807,  ..., -2.6632,  1.4760, -1.7453],
         [ 0.0802,  6.5285,  0.0776,  ..., -1.4645, -0.3670,  0.2882],
         [ 0.0572, 12.3044, -0.4170,  ..., -3.2274,  0.2273, -0.9613]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.3596, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(12.7326, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  5
5
{'input_ids': tensor([[[16036,   243,   186,  ...,   111,  2684,     1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 38.6% (4254 out of 11019)

Everytime after generating next logits 39.2% (4318 out of 11019)

Everytime after generating next logits 39.2% (4318 out of 11019)

Everytime after generating next logits 39.2% (4318 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)
Tony Hayward has seen a total of 20bn of the cost of his own drilling operations by US President Barack Obama .
iter = 0 reward = 8
final_logits =  tensor([[[ 0.0726,  3.5779,  0.2458,  ..., -2.0562,  1.5564, -1.0458],
         [ 0.0478,  3.4750,  0.1933,  ..., -2.4366,  0.5573, -2.0157],
         [ 0.0544,  1.7249, -0.0721,  ..., -0.0233,  0.3979, -0.5165],
         ...,
         [ 0.0853,  4.7582, -0.6036,  ..., -0.8645, -0.1167, -2.3376],
         [ 0.0647,  2.9035,  0.1386,  ..., -0.5357, -0.1372,  0.6362],
         [ 0.0594, 11.7105, -0.5994,  ..., -2.0122, -0.4975, -0.8679]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.1797, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(666.9115, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4260 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 39.8% (4388 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 40.4% (4452 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4516 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.0% (4522 out of 11019)

Everytime after generating next logits 41.1% (4524 out of 11019)

Everytime after generating next logits 41.1% (4524 out of 11019)

Everytime after generating next logits 41.1% (4524 out of 11019)

Everytime after generating next logits 41.1% (4524 out of 11019)

Everytime after generating next logits 41.1% (4524 out of 11019)

Everytime after generating next logits 41.1% (4524 out of 11019)

Everytime after generating next logits 41.1% (4524 out of 11019)

Everytime after generating next logits 41.1% (4524 out of 11019)

Everytime after generating next logits 41.1% (4526 out of 11019)

Everytime after generating next logits 41.1% (4526 out of 11019)

Everytime after generating next logits 41.1% (4528 out of 11019)

Everytime after generating next logits 41.1% (4528 out of 11019)

Everytime after generating next logits 42.3% (4660 out of 11019)

Everytime after generating next logits 43.5% (4792 out of 11019)

Everytime after generating next logits 43.5% (4792 out of 11019)

Everytime after generating next logits 43.5% (4792 out of 11019)

Everytime after generating next logits 43.5% (4792 out of 11019)

Everytime after generating next logits 43.5% (4792 out of 11019)

Everytime after generating next logits 44.7% (4928 out of 11019)

Everytime after generating next logits 46.0% (5064 out of 11019)

Everytime after generating next logits 46.0% (5064 out of 11019)

Everytime after generating next logits 46.0% (5064 out of 11019)

Everytime after generating next logits 46.0% (5064 out of 11019)

Everytime after generating next logits 47.2% (5204 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 49.8% (5488 out of 11019)

Everytime after generating next logits 51.1% (5632 out of 11019)

Everytime after generating next logits 51.1% (5632 out of 11019)

Everytime after generating next logits 51.1% (5632 out of 11019)

Everytime after generating next logits 51.1% (5632 out of 11019)

Everytime after generating next logits 51.1% (5632 out of 11019)

Everytime after generating next logits 52.5% (5780 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 55.2% (6080 out of 11019)

Everytime after generating next logits 56.6% (6232 out of 11019)

Everytime after generating next logits 56.6% (6232 out of 11019)

Everytime after generating next logits 56.6% (6232 out of 11019)

Everytime after generating next logits 56.6% (6232 out of 11019)

Everytime after generating next logits 56.6% (6232 out of 11019)

Everytime after generating next logits 58.0% (6388 out of 11019)

Everytime after generating next logits 59.4% (6544 out of 11019)

Everytime after generating next logits 59.4% (6544 out of 11019)

Everytime after generating next logits 59.4% (6544 out of 11019)

Everytime after generating next logits 59.4% (6544 out of 11019)

Everytime after generating next logits 60.8% (6704 out of 11019)

Everytime after generating next logits 62.3% (6864 out of 11019)

Everytime after generating next logits 62.3% (6864 out of 11019)

Everytime after generating next logits 62.3% (6864 out of 11019)

Everytime after generating next logits 62.3% (6864 out of 11019)

Everytime after generating next logits 62.3% (6864 out of 11019)

Everytime after generating next logits 63.8% (7028 out of 11019)

Everytime after generating next logits 65.3% (7192 out of 11019)

Everytime after generating next logits 65.3% (7192 out of 11019)

Everytime after generating next logits 65.3% (7192 out of 11019)

Everytime after generating next logits 65.3% (7192 out of 11019)

Everytime after generating next logits 66.8% (7360 out of 11019)

Everytime after generating next logits 68.3% (7528 out of 11019)

Everytime after generating next logits 68.3% (7528 out of 11019)

Everytime after generating next logits 68.3% (7528 out of 11019)

Everytime after generating next logits 68.3% (7528 out of 11019)

Everytime after generating next logits 68.3% (7528 out of 11019)

Everytime after generating next logits 69.9% (7700 out of 11019)

Everytime after generating next logits 71.4% (7872 out of 11019)

Everytime after generating next logits 71.4% (7872 out of 11019)

Everytime after generating next logits 71.4% (7872 out of 11019)

Everytime after generating next logits 71.4% (7872 out of 11019)

Everytime after generating next logits 73.0% (8048 out of 11019)

Everytime after generating next logits 74.6% (8224 out of 11019)

Everytime after generating next logits 74.6% (8224 out of 11019)

Everytime after generating next logits 74.6% (8224 out of 11019)

Everytime after generating next logits 74.6% (8224 out of 11019)

Everytime after generating next logits 74.6% (8224 out of 11019)

Everytime after generating next logits 76.3% (8404 out of 11019)

Everytime after generating next logits 77.9% (8584 out of 11019)

Everytime after generating next logits 77.9% (8584 out of 11019)

Everytime after generating next logits 77.9% (8584 out of 11019)

Everytime after generating next logits 77.9% (8584 out of 11019)

Everytime after generating next logits 79.6% (8768 out of 11019)

Everytime after generating next logits 81.2% (8952 out of 11019)

Everytime after generating next logits 81.2% (8952 out of 11019)

Everytime after generating next logits 81.2% (8952 out of 11019)

Everytime after generating next logits 81.2% (8952 out of 11019)

Everytime after generating next logits 82.9% (9140 out of 11019)

Everytime after generating next logits 84.7% (9328 out of 11019)

Everytime after generating next logits 84.7% (9328 out of 11019)

Everytime after generating next logits 84.7% (9328 out of 11019)

Everytime after generating next logits 84.7% (9328 out of 11019)

Everytime after generating next logits 84.7% (9328 out of 11019)

Everytime after generating next logits 86.4% (9520 out of 11019)

Everytime after generating next logits 88.1% (9712 out of 11019)

Everytime after generating next logits 88.1% (9712 out of 11019)

Everytime after generating next logits 88.1% (9712 out of 11019)

Everytime after generating next logits 88.1% (9712 out of 11019)

Everytime after generating next logits 89.9% (9908 out of 11019)

Everytime after generating next logits 91.7% (10104 out of 11019)

Everytime after generating next logits 91.7% (10104 out of 11019)

Everytime after generating next logits 91.7% (10104 out of 11019)

Everytime after generating next logits 91.7% (10104 out of 11019)

Everytime after generating next logits 91.7% (10104 out of 11019)

Everytime after generating next logits 93.5% (10304 out of 11019)

Everytime after generating next logits 95.3% (10504 out of 11019)

Everytime after generating next logits 95.3% (10504 out of 11019)

Everytime after generating next logits 95.3% (10504 out of 11019)

Everytime after generating next logits 95.3% (10504 out of 11019)

Everytime after generating next logits 97.2% (10708 out of 11019)

Everytime after generating next logits 99.0% (10912 out of 11019)

Everytime after generating next logits 99.0% (10912 out of 11019)

Everytime after generating next logits 99.0% (10912 out of 11019)

Everytime after generating next logits 99.0% (10912 out of 11019)

Everytime after generating next logits 99.0% (10912 out of 11019)

Everytime after generating next logits 42.4% (4672 out of 11019)

Everytime after generating next logits 44.3% (4880 out of 11019)

Everytime after generating next logits 44.3% (4880 out of 11019)
President Obama said he wants a total of 20B to a total of 20B to the company's entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire entire of the
iter = 1 reward = 4
final_logits =  tensor([[[ 0.0942,  6.1851,  0.4463,  ..., -2.6962,  2.6217, -0.2075],
         [ 0.0281,  5.0475,  0.0499,  ..., -2.0929,  0.6305, -0.6803],
         [ 0.0440,  3.8120, -0.0929,  ..., -1.9516, -0.3507, -1.5426],
         ...,
         [ 0.0442,  8.1585, -0.3787,  ..., -1.9221, -1.0662, -1.6440],
         [ 0.0468,  9.8331, -0.7131,  ..., -2.9350, -1.0946, -1.7347],
         [ 0.0486,  9.6529, -0.4474,  ..., -3.2245, -0.4395, -1.9628]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-8.5278, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(27.8386, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 44.4% (4892 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)

Everytime after generating next logits 45.6% (5020 out of 11019)
President Obama said he would make the highest amount of a company to pay for a ''well and a 'pen and a 'pen and a 'pen'' and a ''s' and a ''s' and a ''s' of the company are all of the company that is now a company that is affected by the company's share of the company that is the company that is the company that is the company that is the company that is the company that is the company that is the company that is the company that is the company that is the company that is the company that is the company that is the company that is the company that is the company that is the company that is the company that
iter = 2 reward = 26
final_logits =  tensor([[[ 1.0326e-01,  3.3473e+00,  7.8021e-01,  ..., -4.5014e+00,
           2.8573e+00,  1.0177e-02],
         [ 2.5632e-02,  4.5735e+00,  1.0702e-01,  ..., -2.4690e+00,
           6.1967e-01, -5.1812e-01],
         [ 6.1970e-02,  3.4779e+00, -8.9620e-02,  ..., -2.6029e+00,
          -1.4447e+00, -1.5934e+00],
         ...,
         [ 7.7428e-02,  9.4733e+00, -6.7381e-01,  ..., -4.6974e+00,
          -9.9995e-02, -1.4427e+00],
         [ 9.3069e-02,  1.0833e+01, -9.3651e-01,  ..., -3.8168e+00,
           6.8777e-01, -1.1296e+00],
         [ 1.2007e-01,  1.1567e+01, -6.6350e-01,  ..., -3.1841e+00,
           5.9036e-01,  9.3030e-02]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(10.2783, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(93.4441, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 42.1% (4644 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)

Everytime after generating next logits 43.3% (4772 out of 11019)
News of the US President Obama on the way to stop a spill - and the company will pay a fraction of the average of a single .
iter = 3 reward = 6
final_logits =  tensor([[[ 8.9076e-02,  4.3190e+00,  7.6354e-01,  ..., -3.3582e+00,
           2.8496e+00,  4.2950e-01],
         [ 1.1542e-02,  2.3000e+00,  1.7895e-01,  ..., -3.7897e+00,
          -2.0108e-01, -2.3511e+00],
         [ 4.0614e-02,  2.8757e+00, -2.4260e-01,  ..., -5.5374e+00,
           2.9291e+00, -1.7167e+00],
         ...,
         [ 4.0706e-02,  6.9794e+00, -2.0745e-01,  ..., -4.7380e+00,
           1.6919e+00, -1.4129e-01],
         [ 7.6545e-02,  9.5228e+00,  7.1658e-01,  ..., -4.1972e+00,
           1.1773e+00,  4.6441e-01],
         [ 7.3215e-02,  1.2198e+01, -3.7154e-01,  ..., -2.9621e+00,
           1.3126e+00, -1.0100e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.6618, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(26.6246, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.8% (4382 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)

Everytime after generating next logits 40.9% (4510 out of 11019)
BP's decision to suspend its dividend is a relief to the US president on the way it is a decision to stop a spill .
iter = 4 reward = 7
final_logits =  tensor([[[ 7.8730e-02,  5.6730e+00,  8.4078e-01,  ..., -2.2115e+00,
           2.6561e+00, -4.1160e-01],
         [ 1.2375e-01,  3.5705e+00,  8.7702e-02,  ..., -2.7126e+00,
           2.2108e-01, -3.9747e+00],
         [ 7.1530e-02,  5.1633e+00,  1.9510e-01,  ..., -2.9846e+00,
           2.9739e-01, -6.1039e-01],
         ...,
         [ 8.3589e-02,  6.5287e+00, -6.3430e-01,  ..., -2.2591e+00,
          -3.0848e-04, -2.0881e+00],
         [ 9.9544e-02,  7.1780e+00,  1.9836e-01,  ..., -3.1800e+00,
          -3.6203e-01,  3.3642e-02],
         [ 9.6527e-02,  1.1477e+01, -6.8461e-02,  ..., -1.9274e+00,
           1.2968e+00, -5.0158e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-5.0303, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(16.0272, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  7
7
{'input_ids': tensor([[[16036, 35640,   116,  ...,  3598, 32220,     1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 38.6% (4252 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)

Everytime after generating next logits 39.7% (4380 out of 11019)
Gulf Mexico has deployed top limit limit to top limit of top limit of top dispersant .
iter = 0 reward = 2
final_logits =  tensor([[[ 0.0561,  4.3875,  0.9449,  ..., -3.6935, -0.4485, -0.3091],
         [ 0.0555,  3.3546,  0.0395,  ..., -3.1109, -0.8817,  0.3173],
         [ 0.0919,  1.2249,  0.2138,  ..., -5.6727, -0.5227, -1.5596],
         ...,
         [ 0.0608,  4.8808, -0.4181,  ..., -4.4830, -0.6066,  0.5144],
         [ 0.1045,  7.1713,  0.5264,  ..., -3.3115,  0.4234,  1.3537],
         [ 0.0640, 12.7084,  0.3635,  ..., -2.6315, -0.2265,  0.1670]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-16.2341, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(67.8062, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4238 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)

Everytime after generating next logits 39.6% (4366 out of 11019)
Gulf Mexico has deployed top Mexico top Mexico has hoped top top limit of top limit .
iter = 1 reward = 1
final_logits =  tensor([[[ 0.0637,  2.9127,  0.9318,  ..., -3.2736, -0.3065, -0.6862],
         [ 0.0827,  2.9184, -0.0780,  ..., -3.0315, -1.0982, -0.1046],
         [ 0.1148,  1.0474,  0.2728,  ..., -5.0736, -0.5977, -1.7975],
         ...,
         [ 0.0768,  3.7876, -0.1965,  ..., -2.3421,  0.7371, -0.2206],
         [ 0.1040,  5.6355,  0.6628,  ..., -2.8362,  0.8358,  0.3298],
         [ 0.0788, 12.3970,  0.2119,  ..., -2.4909, -0.6237,  0.0457]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-6.9293, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(12.2654, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4236 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)

Everytime after generating next logits 39.6% (4364 out of 11019)
A number of BP workers are demanding a new device to kill a cut-- out the
iter = 2 reward = 4
final_logits =  tensor([[[ 8.2992e-02,  5.2484e+00,  8.9237e-01,  ..., -3.1086e+00,
           3.3647e-01, -6.7629e-01],
         [ 7.8345e-03,  3.7169e+00,  5.3899e-01,  ..., -3.7303e+00,
          -6.6368e-02,  7.7975e-01],
         [ 5.3479e-02,  5.1699e+00,  1.5705e-01,  ..., -2.0496e+00,
          -3.7567e-01,  8.0552e-01],
         ...,
         [ 8.1245e-02,  8.9146e+00,  4.0773e-01,  ...,  5.9156e-01,
          -1.8155e+00, -3.2266e-01],
         [ 9.6904e-02,  7.3462e+00, -4.1269e-01,  ..., -1.5414e+00,
           8.3765e-01,  2.6791e-01],
         [ 1.0248e-01,  9.0548e+00,  3.8864e-02,  ..., -8.9806e-01,
           9.6411e-01,  1.3754e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.5005, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(7.6736, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.0% (4302 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)
A number of BP workers are demanding a new device that will be used to kill a ''              9,500 barrels are are are are are are are are a           
iter = 3 reward = 6
final_logits =  tensor([[[ 0.0964,  4.8196,  0.8661,  ..., -3.5385, -0.2767, -0.9587],
         [ 0.0170,  3.4127,  0.4217,  ..., -3.2046, -1.2389,  1.0010],
         [ 0.0679,  4.9955,  0.1689,  ..., -2.5067, -0.1392,  0.7477],
         ...,
         [ 0.1125, 10.0334,  0.7268,  ..., -2.7320, -0.9831,  1.3502],
         [ 0.1128, 10.1992,  0.7452,  ..., -2.5592, -1.0760,  1.3357],
         [ 0.1130, 10.4944,  0.7353,  ..., -2.5010, -1.1248,  1.2437]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(14.0582, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(44.6838, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.7% (4372 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)

Everytime after generating next logits 40.8% (4500 out of 11019)
BP demands a British government commitment to to to to to a device that will be used to kill a well-like device       .
iter = 4 reward = 6
final_logits =  tensor([[[ 0.1092,  4.5626,  0.8005,  ..., -3.9177,  0.1776, -1.5857],
         [ 0.1332,  3.4501, -0.0235,  ..., -3.1588, -0.8809, -3.2388],
         [ 0.1032,  2.0229, -0.0901,  ..., -2.8691,  3.2049, -0.1483],
         ...,
         [ 0.1171, 10.2751,  0.4375,  ..., -2.0553,  0.1619,  0.6956],
         [ 0.1185, 10.1439,  0.3820,  ..., -1.9850,  0.1790,  0.4667],
         [ 0.1122, 12.0372, -0.1449,  ..., -1.6827,  0.2669, -0.6608]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(14.9429, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(105.3157, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  6
6
{'input_ids': tensor([[[  787,  1276, 12998,  3531,   148,  4486, 18205, 53857,  7028,   112,
          15079,   109,  3662,   599, 10970,   233, 20447,   788,   121,  1768,
          43304,   110, 10970,   233, 16567,   788,   121,  2617,   120, 16036,
            148,   323,   164,   112, 13889,  4807,   113,   109,  7175,   113,
           3064,   762, 14567,   110,   107,  1263, 53857,  7028,   148,   306,
            115,   253,  2887,   110,   151,   178,  3120,   109,  4807,  1034,
           1844,  2617,   323,   164,   115,   109,  4619,   113,   109,  1073,
           1338,  6687,  3613,   115,   351,   859,   111,  1741,   110,   107,
            285,   148,   243,   178,   117,   146, 10237,   122,   109,   657,
            132, 16036,   111,   138, 15079,   109,  2617,  7539,   110,   107,
           2632,   138,   719,  3916,   135,   109,  2617,   110,   152,   139,
            762, 14567,   148,  2145, 12071,   466,   109,   787,  7175,  3500,
            110,   108,  7271,  3070,   111,  5569,   111, 65047,   181,  4758,
            111, 44650, 18205, 53857,  7028, 35571,  3916,   118,  4807,   113,
            109,  1073,  1338,   110,   108,  6687,  3613,  7922,  8749, 18205,
          53857,  7028, 18097, 16915,   918,   111,   243,  2784,   192,   129,
            154,  5263,   197,   274,   120,   192,   129,  3366,   141,   114,
           1462,   110,   107,   343,   178,   243,   274,  2486,  3916,   355,
            361,   164,   153,   268,   112, 17984, 16036,   110,   107,   139,
           2617,   117,   112, 34262,  7175,   113,  3064,  1836,   111,  1098,
            118,  1166,  9125,   111,  5471,   111,   118,   510,  3207,   111,
           1003,   121,   768,   110,   108,   790,   176,  2242,   110,   107,
          16036,   148,   506,  1389,  3662,   110, 37622,   208,   115,  2242,
            381,   109,   960, 14567,   110,   107,   139,   762, 14567,   110,
            108,   162,  1219,   599,   960,   122,   109, 11335,   113,   109,
          16036,   121, 38539,   252, 81065, 18308,  9600, 13773,   110,   108,
           2145,  8010, 12071,   466,   109,   787,  7175,  3500, 18205, 53857,
           7028, 35571,  3916,   118,  4807,   113,   109,  1073,  1338,   110,
            108,  6687,  3613,   139,  8749,   113,   114,  3662,   599, 10970,
            233, 20447,   788,   121,  1768, 31197,   110, 10970,   233, 16567,
            788,   121,  2617,   112, 13889,  4807,   113,   109, 16036,   762,
          14567,   148,   243,   186,   117,   110,   105,   220,  2767, 16837,
            120, 15931,  2242,   127,   142,   797,   110,   107, 18205, 53857,
           7028, 18097,   112,   129, 24915,   204,   170,   915,  2784,   112,
           1480,   109,  4959,   113,   109,  2617,   110,   107,  3440,  3662,
          12622,   110, 10970,   148,   506,   174,  1389,   165,   115,  2280,
           2784,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 39.2% (4320 out of 11019)

Everytime after generating next logits 39.2% (4320 out of 11019)

Everytime after generating next logits 39.2% (4320 out of 11019)

Everytime after generating next logits 39.2% (4320 out of 11019)
Feinberg has awarded him a fee for the damages of the damage of the damage of the damage of the damage of the damage of the Gulf of the Gulf of the Gulf of the Gulf of the Gulf .
iter = 0 reward = 10
final_logits =  tensor([[[ 0.0675,  6.2315,  0.4846,  ..., -1.3038,  0.7263, -0.5237],
         [ 0.0457,  5.7104,  0.2715,  ..., -0.7518, -1.9342,  0.9844],
         [ 0.0779,  4.7770, -0.1265,  ..., -1.6042, -0.8354, -0.7907],
         ...,
         [ 0.0559,  7.3623,  0.2528,  ..., -4.9713, -0.7045,  0.8589],
         [ 0.1121, 10.0282,  0.9886,  ..., -3.1398,  1.3521,  4.2247],
         [ 0.0738, 11.4586, -0.1644,  ..., -1.5190,  1.4442,  0.6178]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(8.3446, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(40.0413, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)
Feinberg oversaw the September spill spill spill spill spill spill spill spill spill .
iter = 1 reward = 9
final_logits =  tensor([[[ 6.8734e-02,  4.2964e+00,  4.3178e-01,  ..., -1.7174e+00,
           5.1572e-01, -1.0143e+00],
         [ 2.5842e-02,  4.4067e+00,  4.7648e-03,  ..., -4.1997e-01,
          -2.5204e+00,  9.5370e-02],
         [ 7.9092e-02,  4.6045e+00,  8.6860e-02,  ..., -2.4992e+00,
          -2.0141e+00, -8.2671e-01],
         ...,
         [ 6.3498e-02,  3.3113e+00, -3.3156e-01,  ..., -1.3632e+00,
          -4.3018e-02,  1.0720e-01],
         [ 9.2778e-02,  8.9709e+00,  3.5312e-01,  ..., -1.5615e+00,
          -1.4232e-01,  1.3255e+00],
         [ 7.5645e-02,  1.2174e+01, -3.6498e-01,  ..., -1.8308e+00,
          -2.2180e-01, -1.0899e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(14.1044, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(61.6033, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)
Feinberg oversaw the September spill spill spill spill spill spill spill claims from September spill spill spill .
iter = 2 reward = 10
final_logits =  tensor([[[ 5.5913e-02,  4.4288e+00,  4.3425e-01,  ..., -1.5405e+00,
           1.8934e-01, -4.9967e-01],
         [ 2.7686e-02,  4.8027e+00, -5.6630e-02,  ..., -6.9799e-01,
          -2.5274e+00, -2.0848e-02],
         [ 6.7068e-02,  4.8098e+00, -5.3807e-03,  ..., -2.7740e+00,
          -2.1175e+00, -9.3048e-01],
         ...,
         [ 5.5023e-02,  3.7720e+00, -3.4620e-01,  ..., -3.2643e+00,
          -2.2921e-01,  7.8561e-01],
         [ 8.4167e-02,  9.3251e+00,  2.8361e-01,  ..., -2.8393e+00,
          -3.7691e-01,  9.3506e-01],
         [ 6.0897e-02,  1.2245e+01, -3.2846e-01,  ..., -2.1918e+00,
          -5.8580e-01, -8.9952e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(6.1342, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(24.9332, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)
President Obama has appointed a fund to to to
iter = 3 reward = 3
final_logits =  tensor([[[ 0.0809,  4.7700,  0.4939,  ..., -0.7131,  2.3648,  0.1182],
         [ 0.0503,  4.9467,  0.0812,  ..., -1.0489, -0.1607, -0.9155],
         [ 0.0602,  4.8366, -0.2452,  ..., -0.0981, -1.9243, -1.1916],
         ...,
         [ 0.0551,  6.5685, -0.2165,  ..., -1.6585,  1.2339, -0.7917],
         [ 0.0570,  7.8771, -0.2311,  ..., -1.2549,  1.2408, -0.5785],
         [ 0.0573,  8.8836, -0.2595,  ..., -1.3606,  1.1376, -0.2907]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-5.0158, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(9.4149, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)
President Obama has appointed a US spill-funded fund to
iter = 4 reward = 3
final_logits =  tensor([[[ 0.0781,  5.3076,  0.4180,  ..., -0.5848,  2.2286, -0.0844],
         [ 0.0465,  5.5023, -0.0232,  ..., -0.7641, -0.2552, -0.4165],
         [ 0.0551,  5.2737, -0.3138,  ..., -0.1910, -2.0761, -0.8176],
         ...,
         [ 0.0367,  9.9676, -0.1165,  ..., -1.4258,  1.8811, -0.9751],
         [ 0.0338,  6.0060, -0.7555,  ..., -2.2651,  1.5955, -0.0275],
         [ 0.0565, 10.9459, -0.3494,  ..., -0.9576,  0.8897, -0.5467]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-7.6446, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(26.8608, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  3
3
{'input_ids': tensor([[[  787,   762, 14567,   110,   151, 15265,  7788,   110,   105,   124,
            109,  2143,  1034,  1027,  3070,  6375,   127,   239,   270,   263,
            112,   225, 16036,   562,   109, 15737,  2584, 51128,   116, 10928,
           1034,   116,  3070,  2414,   148,  7646,   339,   698, 26680,   233,
          37399,   110,   108, 35493,   111, 23363,   110,   107,   110,   105,
            600,   480,   140, 12144,   110,   108,   155,   265,  7646,   110,
            108, 16837,   178,   649,   110,   108,  1132,   109,  2414,   114,
           4081, 15115,   110,   107,   343,  1263, 51128,   116, 10928,  7719,
            180,   138,  4428,   169,  1162,  3070,   260,   559,   111,   118,
            149,   117,   146,   114,   710,  5135,   110,   108,   155,   114,
            729,   121,  4109,   156,   110,   107,   202,   729,  1034,   116,
            419,   112,   133,   114,   715,   110,   108,   181,   660,   113,
            523,   134,   109,   370,   113,   109,  8483, 16837,  4645, 16848,
           2584, 51128,   116, 10928,  7915, 25996,   110,   105,  2184,  6021,
            134,   214,   173,   145,   416,   136,   762, 14567,   256,   129,
           3150,   197, 23363,   110,   108,   155,   145,   114,   457,  3178,
            131,   144, 34742,   110,   108, 16837,   178,   649,   110,   107,
            398,   178, 41593,   164,   114,  1124,  2944,   113,  9606,  6545,
            115,   109,  2414,  1034,   116,  7270,   110,   108,  2584,   649,
            178,   117,  5238,   120,   109, 14567,   138,   129,   289, 13502,
            118,   223,   200,   115,   136,  3820,   121, 23623,  3070,   427,
            113, 38584,  1946, 11958,   144,   216,   110,   108,  7915, 29546,
           3070,   148,   174,  7162,   115,  7915,   262,   113,   109, 14567,
          10250,  1034,   116,  1750,   110,   108,  1946, 74523,   110,   108,
           5765,   122,   342,   111,  2779,   112,  7904,   110,   107,   110,
            105, 15265,   117,   169,   271,   110,   108,   347,   126,   178,
            358,  3178,   131,   144,   235,   742,   997,   110,   108, 16837,
            265,   649,   110,   107,   110,   105,   285,  1034,   116, 13735,
            110,   107,   285,  1034,   116,  7432,   134,   488,   110,   107,
            285,  1034,   116,   146,   109,   310, 29546,   420, 15538, 90155,
            110,   151,   110,   105,   168,   978,   172,   114, 10447,  1120,
            279,   264, 16837, 16036,   148, 20350,  5478,   113, 63981,  7881,
            333,   109,  7175,   113,  3064,   762, 14567,   110,   108,   155,
            186,   127,   274,   170,   127,  1192,   126,  2773,  7427,   118,
            109,   201,   126,   117,   557,   110,   108,  6260,   109,  6442,
           1034,   116,  6869,  3544,   115,  7915,   110,   107, 46095,   126,
            110,   108,   155, 16036,   117,   146,   114,  6749,  1172,   264,
            115,   109,  6805,  1120,   113,  7026, 63116, 58439,   110,   107,
            222,   109, 10601,  1034,   116,   629,   110,   108, 14515,   429,
            115,   114,  1120,  4511,   120,   117,   239,   163,   238,   112,
          16036,  1034,   116,   648,   115,   136,   297,   113,  7915,   110,
            108,   623,   391,  1725,  2051,   279,   115,  4555,   523,  1490,
          21955,  8802,   110,   107,   110,   105,   184,  1034,   216,   146,
            314,   785,   118,  1609,   126,   110,   108,   155,   264, 16036,
           1034,   116,   557,   234,   110,   108, 16837,   156,   649,   110,
            107,   353,  1034,   116,   956,  2158,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.1% (4194 out of 11019)

Everytime after generating next logits 38.1% (4194 out of 11019)

Everytime after generating next logits 38.1% (4194 out of 11019)

Everytime after generating next logits 38.1% (4194 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)
'' '' '' '' made a huge   
iter = 0 reward = 0
final_logits =  tensor([[[ 0.0705,  5.6035,  0.5179,  ..., -2.4951, -2.3075, -1.4571],
         [ 0.0662,  7.4215,  0.7299,  ..., -3.2382, -2.5478, -1.9621],
         [ 0.0673,  7.5373,  0.6142,  ..., -2.8983, -2.7438, -1.8825],
         ...,
         [ 0.0552,  9.5533,  0.2394,  ..., -3.5362, -2.3977, -1.2109],
         [ 0.0614, 10.2395,  0.1442,  ..., -3.7316, -2.4910, -1.0285],
         [ 0.0661, 10.7438,  0.1263,  ..., -3.8061, -2.5183, -0.9374]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-28.8290, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(108.9294, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.3% (4220 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.9% (4284 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.2% (4322 out of 11019)

Everytime after generating next logits 39.4% (4342 out of 11019)

Everytime after generating next logits 39.4% (4342 out of 11019)

Everytime after generating next logits 39.4% (4342 out of 11019)

Everytime after generating next logits 39.4% (4342 out of 11019)

Everytime after generating next logits 39.8% (4386 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.2% (4430 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 41.1% (4526 out of 11019)

Everytime after generating next logits 41.1% (4526 out of 11019)

Everytime after generating next logits 41.1% (4526 out of 11019)

Everytime after generating next logits 41.1% (4526 out of 11019)

Everytime after generating next logits 41.5% (4578 out of 11019)

Everytime after generating next logits 42.0% (4630 out of 11019)

Everytime after generating next logits 42.0% (4630 out of 11019)

Everytime after generating next logits 42.0% (4630 out of 11019)

Everytime after generating next logits 42.0% (4630 out of 11019)

Everytime after generating next logits 42.5% (4686 out of 11019)

Everytime after generating next logits 43.0% (4742 out of 11019)

Everytime after generating next logits 43.0% (4742 out of 11019)

Everytime after generating next logits 43.0% (4742 out of 11019)

Everytime after generating next logits 43.0% (4742 out of 11019)

Everytime after generating next logits 43.0% (4742 out of 11019)

Everytime after generating next logits 43.6% (4802 out of 11019)

Everytime after generating next logits 44.1% (4862 out of 11019)

Everytime after generating next logits 44.1% (4862 out of 11019)

Everytime after generating next logits 44.1% (4862 out of 11019)

Everytime after generating next logits 44.1% (4862 out of 11019)

Everytime after generating next logits 44.7% (4926 out of 11019)

Everytime after generating next logits 45.3% (4990 out of 11019)

Everytime after generating next logits 45.3% (4990 out of 11019)

Everytime after generating next logits 45.3% (4990 out of 11019)

Everytime after generating next logits 45.3% (4990 out of 11019)

Everytime after generating next logits 45.3% (4990 out of 11019)

Everytime after generating next logits 45.9% (5058 out of 11019)

Everytime after generating next logits 46.5% (5126 out of 11019)

Everytime after generating next logits 46.5% (5126 out of 11019)

Everytime after generating next logits 46.5% (5126 out of 11019)

Everytime after generating next logits 46.5% (5126 out of 11019)

Everytime after generating next logits 47.2% (5198 out of 11019)

Everytime after generating next logits 47.8% (5272 out of 11019)

Everytime after generating next logits 47.8% (5272 out of 11019)

Everytime after generating next logits 47.8% (5272 out of 11019)

Everytime after generating next logits 47.8% (5272 out of 11019)

Everytime after generating next logits 47.8% (5272 out of 11019)

Everytime after generating next logits 48.5% (5348 out of 11019)

Everytime after generating next logits 49.2% (5424 out of 11019)

Everytime after generating next logits 49.2% (5424 out of 11019)

Everytime after generating next logits 49.2% (5424 out of 11019)

Everytime after generating next logits 49.2% (5424 out of 11019)

Everytime after generating next logits 50.0% (5504 out of 11019)

Everytime after generating next logits 50.7% (5584 out of 11019)

Everytime after generating next logits 50.7% (5584 out of 11019)

Everytime after generating next logits 50.7% (5584 out of 11019)

Everytime after generating next logits 50.7% (5584 out of 11019)

Everytime after generating next logits 50.7% (5584 out of 11019)

Everytime after generating next logits 51.4% (5668 out of 11019)

Everytime after generating next logits 52.2% (5752 out of 11019)

Everytime after generating next logits 52.2% (5752 out of 11019)

Everytime after generating next logits 52.2% (5752 out of 11019)

Everytime after generating next logits 52.2% (5752 out of 11019)

Everytime after generating next logits 53.0% (5840 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 54.6% (6020 out of 11019)

Everytime after generating next logits 55.5% (6112 out of 11019)

Everytime after generating next logits 55.5% (6112 out of 11019)

Everytime after generating next logits 55.5% (6112 out of 11019)

Everytime after generating next logits 55.5% (6114 out of 11019)

Everytime after generating next logits 56.4% (6210 out of 11019)

Everytime after generating next logits 57.2% (6306 out of 11019)

Everytime after generating next logits 57.2% (6306 out of 11019)

Everytime after generating next logits 57.2% (6306 out of 11019)

Everytime after generating next logits 57.2% (6306 out of 11019)

Everytime after generating next logits 58.1% (6406 out of 11019)

Everytime after generating next logits 59.0% (6506 out of 11019)

Everytime after generating next logits 59.0% (6506 out of 11019)

Everytime after generating next logits 59.0% (6506 out of 11019)

Everytime after generating next logits 59.0% (6506 out of 11019)

Everytime after generating next logits 59.0% (6506 out of 11019)

Everytime after generating next logits 60.0% (6612 out of 11019)

Everytime after generating next logits 60.9% (6716 out of 11019)

Everytime after generating next logits 60.9% (6716 out of 11019)

Everytime after generating next logits 60.9% (6716 out of 11019)

Everytime after generating next logits 60.9% (6716 out of 11019)

Everytime after generating next logits 61.9% (6824 out of 11019)

Everytime after generating next logits 62.9% (6932 out of 11019)

Everytime after generating next logits 62.9% (6932 out of 11019)

Everytime after generating next logits 62.9% (6932 out of 11019)

Everytime after generating next logits 62.9% (6932 out of 11019)

Everytime after generating next logits 62.9% (6932 out of 11019)

Everytime after generating next logits 63.9% (7044 out of 11019)

Everytime after generating next logits 64.9% (7156 out of 11019)

Everytime after generating next logits 64.9% (7156 out of 11019)

Everytime after generating next logits 64.9% (7156 out of 11019)

Everytime after generating next logits 64.9% (7156 out of 11019)

Everytime after generating next logits 66.0% (7272 out of 11019)

Everytime after generating next logits 67.0% (7388 out of 11019)

Everytime after generating next logits 67.0% (7388 out of 11019)

Everytime after generating next logits 67.0% (7388 out of 11019)

Everytime after generating next logits 67.0% (7388 out of 11019)

Everytime after generating next logits 67.0% (7388 out of 11019)

Everytime after generating next logits 68.1% (7508 out of 11019)

Everytime after generating next logits 69.2% (7628 out of 11019)

Everytime after generating next logits 69.2% (7628 out of 11019)

Everytime after generating next logits 69.2% (7628 out of 11019)

Everytime after generating next logits 69.2% (7628 out of 11019)

Everytime after generating next logits 70.4% (7752 out of 11019)

Everytime after generating next logits 71.5% (7876 out of 11019)

Everytime after generating next logits 71.5% (7876 out of 11019)

Everytime after generating next logits 71.5% (7878 out of 11019)

Everytime after generating next logits 71.5% (7878 out of 11019)

Everytime after generating next logits 71.5% (7878 out of 11019)

Everytime after generating next logits 72.7% (8006 out of 11019)

Everytime after generating next logits 73.9% (8138 out of 11019)

Everytime after generating next logits 73.9% (8138 out of 11019)

Everytime after generating next logits 73.9% (8138 out of 11019)

Everytime after generating next logits 73.9% (8138 out of 11019)

Everytime after generating next logits 75.1% (8270 out of 11019)

Everytime after generating next logits 76.3% (8402 out of 11019)

Everytime after generating next logits 76.3% (8402 out of 11019)

Everytime after generating next logits 76.3% (8402 out of 11019)

Everytime after generating next logits 76.3% (8402 out of 11019)

Everytime after generating next logits 76.3% (8402 out of 11019)

Everytime after generating next logits 77.5% (8538 out of 11019)

Everytime after generating next logits 78.7% (8674 out of 11019)

Everytime after generating next logits 78.7% (8674 out of 11019)

Everytime after generating next logits 78.7% (8674 out of 11019)

Everytime after generating next logits 78.7% (8674 out of 11019)

Everytime after generating next logits 80.0% (8814 out of 11019)

Everytime after generating next logits 81.3% (8954 out of 11019)

Everytime after generating next logits 81.3% (8954 out of 11019)

Everytime after generating next logits 81.3% (8954 out of 11019)

Everytime after generating next logits 81.3% (8954 out of 11019)

Everytime after generating next logits 82.6% (9098 out of 11019)

Everytime after generating next logits 83.9% (9242 out of 11019)

Everytime after generating next logits 83.9% (9242 out of 11019)

Everytime after generating next logits 83.9% (9242 out of 11019)

Everytime after generating next logits 83.9% (9242 out of 11019)

Everytime after generating next logits 83.9% (9242 out of 11019)

Everytime after generating next logits 85.2% (9390 out of 11019)

Everytime after generating next logits 86.6% (9538 out of 11019)

Everytime after generating next logits 86.6% (9538 out of 11019)

Everytime after generating next logits 86.6% (9538 out of 11019)

Everytime after generating next logits 86.6% (9538 out of 11019)

Everytime after generating next logits 87.9% (9690 out of 11019)

Everytime after generating next logits 89.3% (9842 out of 11019)

Everytime after generating next logits 89.3% (9842 out of 11019)

Everytime after generating next logits 89.3% (9842 out of 11019)

Everytime after generating next logits 89.3% (9842 out of 11019)

Everytime after generating next logits 89.3% (9842 out of 11019)

Everytime after generating next logits 90.7% (9998 out of 11019)

Everytime after generating next logits 92.1% (10154 out of 11019)

Everytime after generating next logits 92.1% (10154 out of 11019)

Everytime after generating next logits 92.1% (10154 out of 11019)

Everytime after generating next logits 92.1% (10154 out of 11019)

Everytime after generating next logits 93.6% (10314 out of 11019)

Everytime after generating next logits 95.1% (10474 out of 11019)

Everytime after generating next logits 95.1% (10474 out of 11019)

Everytime after generating next logits 95.1% (10474 out of 11019)

Everytime after generating next logits 95.1% (10474 out of 11019)

Everytime after generating next logits 95.1% (10474 out of 11019)

Everytime after generating next logits 96.5% (10638 out of 11019)

Everytime after generating next logits 98.0% (10802 out of 11019)

Everytime after generating next logits 98.0% (10802 out of 11019)

Everytime after generating next logits 98.0% (10802 out of 11019)

Everytime after generating next logits 98.0% (10802 out of 11019)

Everytime after generating next logits 99.6% (10970 out of 11019)

Everytime after generating next logits 40.9% (4512 out of 11019)

Everytime after generating next logits 41.7% (4598 out of 11019)

Everytime after generating next logits 41.7% (4598 out of 11019)

Everytime after generating next logits 41.7% (4598 out of 11019)

Everytime after generating next logits 41.7% (4598 out of 11019)

Everytime after generating next logits 43.3% (4770 out of 11019)

Everytime after generating next logits 44.8% (4942 out of 11019)

Everytime after generating next logits 44.8% (4942 out of 11019)

Everytime after generating next logits 44.8% (4942 out of 11019)

Everytime after generating next logits 44.8% (4942 out of 11019)

Everytime after generating next logits 46.4% (5118 out of 11019)

Everytime after generating next logits 48.0% (5294 out of 11019)

Everytime after generating next logits 48.0% (5294 out of 11019)

Everytime after generating next logits 48.0% (5294 out of 11019)

Everytime after generating next logits 48.0% (5294 out of 11019)

Everytime after generating next logits 48.0% (5294 out of 11019)

Everytime after generating next logits 49.7% (5474 out of 11019)

Everytime after generating next logits 51.3% (5654 out of 11019)

Everytime after generating next logits 51.3% (5654 out of 11019)

Everytime after generating next logits 51.3% (5654 out of 11019)

Everytime after generating next logits 51.3% (5654 out of 11019)

Everytime after generating next logits 53.0% (5838 out of 11019)

Everytime after generating next logits 54.7% (6022 out of 11019)

Everytime after generating next logits 54.7% (6022 out of 11019)
'' '' is a good good, but not a good, but a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or not a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a good, or a
iter = 1 reward = 0
final_logits =  tensor([[[ 9.3230e-02,  4.2640e+00,  8.1417e-01,  ..., -2.7303e+00,
          -1.5142e+00, -1.5324e+00],
         [ 8.8920e-02,  5.8514e+00,  7.8558e-01,  ..., -3.2005e+00,
          -1.9812e+00, -2.8246e+00],
         [ 8.5079e-02,  6.0590e+00,  5.9689e-01,  ..., -2.7119e+00,
          -2.1951e+00, -2.5113e+00],
         ...,
         [-3.2217e-03,  9.8659e+00, -9.8776e-01,  ..., -6.3195e+00,
          -3.2506e-01, -7.5189e-01],
         [ 5.9136e-02,  9.3479e+00, -7.5293e-01,  ..., -6.0411e+00,
          -7.3700e-01, -1.2350e+00],
         [-8.4499e-04,  1.1291e+01, -3.5139e-01,  ..., -3.8082e+00,
          -1.1993e+00, -1.8131e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-11.9296, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(181.7712, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)

Everytime after generating next logits 43.6% (4808 out of 11019)
Gulf Mexico has been made a tough to to to to to a point of the spill .
iter = 2 reward = 2
final_logits =  tensor([[[ 0.1153,  3.7097,  0.8274,  ..., -4.2115, -1.7239, -2.0925],
         [ 0.0632,  1.2670,  0.0903,  ..., -3.8075, -2.0609, -2.5008],
         [ 0.0756, -2.1765,  0.1285,  ..., -4.6612, -2.5839, -2.6080],
         ...,
         [ 0.0353,  4.5771, -0.9481,  ..., -3.6727, -1.3202, -2.2320],
         [ 0.0720,  7.2684, -0.1275,  ..., -3.2308, -1.6646, -0.6729],
         [ 0.0777, 10.8825, -0.2754,  ..., -3.7849, -2.4285, -1.4849]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-3.6675, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(4.6475, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)

Everytime after generating next logits 38.9% (4290 out of 11019)
Mayor of Louisiana says many of the Gulf spill is a ''made a .
iter = 3 reward = 3
final_logits =  tensor([[[ 0.1143,  2.6952,  0.7132,  ..., -4.9184, -1.9649, -1.7844],
         [ 0.0580,  2.4129, -0.2193,  ..., -4.5727, -3.5470, -1.7639],
         [ 0.0466,  1.6681, -0.2403,  ..., -6.5364, -2.3850, -2.2502],
         ...,
         [ 0.0186,  7.9705, -0.1463,  ..., -4.6393, -1.6609, -2.3832],
         [ 0.0607,  9.8669,  0.7258,  ..., -2.8170, -1.0615, -0.2280],
         [ 0.0701, 10.3750, -0.7065,  ..., -4.9199, -2.1351, -1.9993]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(12.1192, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(146.4455, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)

Everytime after generating next logits 39.1% (4304 out of 11019)
Mayor of Louisiana says many of the Gulf oil boat in this town is a ''good but not a good job'
iter = 4 reward = 3
final_logits =  tensor([[[ 0.1162,  2.3338,  0.8123,  ..., -5.0173, -1.6770, -1.1620],
         [ 0.0567,  2.0943, -0.2431,  ..., -5.0676, -3.3900, -1.7107],
         [ 0.0481,  1.7075, -0.2714,  ..., -7.3996, -2.7801, -2.3579],
         ...,
         [ 0.0347,  6.5513, -0.5341,  ..., -4.6238, -0.3297, -0.6051],
         [ 0.0388,  7.9247, -1.0911,  ..., -2.7698, -3.0844, -1.2789],
         [ 0.0740, 11.4816, -1.1544,  ..., -5.1051, -2.4166, -2.1807]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(14.6799, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(58.0204, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  3
3
{'input_ids': tensor([[[  608,  1338,  2652,  2882,  2033,   134,   305, 91516, 17403,  4596,
            202,   110,   105,  5936,   113, 10114,  4064,   114,   344,   113,
            291,  1829, 16837,   140,   112,  6511,   118,   109, 81065, 18308,
            762, 14567,   115,   109,  7175,   113,  3064,   110,   108, 16036,
            649,   110,   107,   983,  3244,  2777,   165,   141, 16036,   649,
            126,   140,  1470,   115,   297,   118,   109,  5135,   110,   108,
            155,   163,  1262,   181,  6511,   124,   176,   524,   375,   124,
            109,   210,   110,   107, 16036,  4121, 13353,   113,  2729,   121,
           8805,   113,  1165,  2242,   118,  3916,   204,   109, 14567,   110,
            108,   109,  3741,   115,   909,   787,   689,   110,   107,   608,
           1338,  2652,  2882,  2033,   134, 71437, 17403,  4596, 16036,   148,
           1788,   114,  1933,   545,   162,   939,   109,   301,  1034,   116,
            700,   113,   702,   964,   164,   112,   109,  7175,   113,  3064,
            762, 14567,   110,   107,  2973,   112,   109,   301,   110,   108,
            114,   110,   105,  5936,   113, 10114,  4064,   114,   344,   113,
            291,  1829, 16837,   140,   112,  6511,   118,   109, 81065, 18308,
            762, 14567,   110,   107,   983,  3244,  2777,   165,   141, 16036,
            243,   120,   126,   140,  1470,   115,   297,   118,   109,  5135,
            110,   108,   155,   126,   163, 17188,   176,   524,   375,   124,
            109,   210,   110,   107, 16036,  4121, 13353,   113,  2729,   121,
           8805,   113,  1165,  2242,   118,  3916,   204,   109, 14567,   110,
            108,   109,  3741,   115,   909,   787,   689,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)
Deepwater Horizon's last part part of a disaster over a Gulf Mexico oil has a disaster over a disaster over a Gulf Mexico oil .
iter = 0 reward = 8
final_logits =  tensor([[[ 0.1016,  5.8134,  0.7109,  ..., -4.5627,  0.3761, -1.1120],
         [ 0.0650,  3.9543,  0.1568,  ..., -3.6667, -0.8767, -0.5160],
         [ 0.0841,  3.9832, -0.1290,  ..., -4.7216, -1.2475, -1.6006],
         ...,
         [ 0.0854,  5.1022, -0.3664,  ..., -4.6955, -0.1062, -0.7409],
         [ 0.0952,  6.0569,  0.0490,  ..., -1.6238, -0.1500,  0.2482],
         [ 0.1053, 12.9996, -0.1124,  ..., -2.5281, -0.6463, -1.1032]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(7.7286, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(371.1522, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4236 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)
Deepwater Horizon faces a disaster over a disaster over a Gulf Mexico oil spill over a
iter = 1 reward = 7
final_logits =  tensor([[[ 1.0977e-01,  5.0291e+00,  7.6029e-01,  ..., -3.3764e+00,
           1.4631e+00, -1.0494e+00],
         [ 7.4192e-02,  4.6622e+00,  1.8928e-01,  ..., -2.0458e+00,
          -6.5063e-01, -8.4834e-01],
         [ 9.1053e-02,  4.4076e+00, -8.6131e-02,  ..., -3.0547e+00,
          -1.9347e-01, -5.8810e-01],
         ...,
         [-1.5130e-03,  5.3472e+00, -5.5415e-01,  ..., -3.5291e+00,
          -1.6314e-01, -9.0779e-01],
         [ 8.5738e-02,  6.0559e+00,  1.9957e-01,  ..., -2.2381e+00,
           9.1567e-01,  4.1804e-01],
         [ 4.5870e-02,  8.8689e+00,  4.2105e-01,  ..., -2.3911e+00,
           1.9101e+00, -1.3097e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.9752, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(10.6975, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)
Deepwater Horizon faces a disaster over a disaster over a Gulf Mexico oil spill over a Deepwater Horizon .
iter = 2 reward = 9
final_logits =  tensor([[[ 1.0485e-01,  4.8441e+00,  7.8237e-01,  ..., -3.4456e+00,
           5.6961e-01, -7.0434e-01],
         [ 6.7236e-02,  4.3926e+00,  1.8296e-01,  ..., -2.5991e+00,
          -7.9114e-01, -1.0764e+00],
         [ 8.1182e-02,  3.9264e+00, -9.3579e-02,  ..., -3.3862e+00,
          -7.0596e-01, -1.0899e+00],
         ...,
         [ 7.3006e-02,  6.7460e+00, -4.5357e-01,  ..., -4.7034e+00,
          -2.1639e-01, -1.9653e+00],
         [ 9.0056e-02,  8.4841e+00,  1.4196e-01,  ..., -1.1902e+00,
          -4.5009e-01,  1.0760e-01],
         [ 8.8992e-02,  1.2819e+01, -4.0761e-03,  ..., -1.8147e+00,
          -4.8251e-01, -9.0092e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.2137, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(10.7836, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)
Deepwater Horizon faces a disaster over a disaster over a Gulf Mexico oil company over a disaster over a Deepwater Horizon .
iter = 3 reward = 10
final_logits =  tensor([[[ 9.3183e-02,  5.0055e+00,  8.5015e-01,  ..., -3.8898e+00,
           9.6992e-01, -6.3147e-01],
         [ 6.4765e-02,  4.4004e+00,  2.0458e-01,  ..., -2.8116e+00,
          -7.2732e-01, -1.1913e+00],
         [ 7.7621e-02,  3.8049e+00, -1.2577e-01,  ..., -3.2899e+00,
          -4.8049e-01, -8.2785e-01],
         ...,
         [ 7.3050e-02,  6.5397e+00, -4.4213e-01,  ..., -4.4174e+00,
          -9.6175e-02, -2.0727e+00],
         [ 8.4791e-02,  7.2420e+00,  8.4122e-02,  ..., -1.3036e+00,
          -5.4117e-01,  5.4610e-02],
         [ 8.3672e-02,  1.2961e+01,  1.2071e-02,  ..., -2.0028e+00,
          -4.2991e-01, -1.1050e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.2156, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.1305, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)
Deepwater Horizon faces a disaster over a disaster over a Deepwater Horizon oil spill over a disaster over a Gulf Mexico oil company .
iter = 4 reward = 12
final_logits =  tensor([[[ 0.0839,  4.8408,  0.8619,  ..., -3.6710,  1.3056, -0.5866],
         [ 0.0618,  4.3801,  0.1873,  ..., -2.9993, -0.7078, -1.3675],
         [ 0.0728,  3.6378, -0.1646,  ..., -3.3268, -0.3188, -1.0823],
         ...,
         [ 0.0651,  6.8919, -0.4513,  ..., -2.6596, -0.3674, -0.9263],
         [ 0.0809,  6.9132,  0.0916,  ..., -0.9476, -0.6694, -0.2563],
         [ 0.0761, 13.1318,  0.0790,  ..., -1.8724, -0.2456, -1.4931]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.4537, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(8.6866, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  12
12
{'input_ids': tensor([[[  530,  1268,  2651,  2882,  2033,   134, 80621, 17403,  4596, 25688,
            115, 16036,  1963,   902,   124,  1789,  2409,   114,   787,  7501,
           5268, 79701,   109,   762,   301,  1034,   116,  4206,   111, 12856,
            130,   210,   130, 16036,   118,   109,  7175,   113,  3064,   762,
          14567,   110,   107,   139, 11335,   134,   109, 81065, 18308, 13773,
            115,   960,  3040,  1073,  1024,   111,  1358,   112,   109,  1368,
            521,  9134,   762,  8186,   110,   107,  4987,   109,  4469,   110,
            108, 16036,  2853,  3947,   607,  2527,   110, 43995,   118,   109,
            211,   166,   381,   913,   289,   232,   110,   107,  6442,   260,
          21089, 80736, 82734,  4278,  3886,   112,  5930, 37313,   110,   108,
            142,   762,  8962,   122, 85411,   116,  1714,   208, 59297, 20185,
            110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)
Maryam Maryam Maryam updates on BBC 6 on May 11 oil spill .
iter = 0 reward = 4
final_logits =  tensor([[[ 5.6108e-03,  2.7911e+00,  6.5868e-01,  ..., -3.0878e+00,
           2.7550e+00,  1.3456e-02],
         [ 4.9516e-02,  2.8983e+00,  3.0417e-01,  ..., -3.6629e+00,
          -4.3533e-01, -6.9333e-01],
         [ 4.6446e-02,  2.6960e+00,  2.4368e-01,  ..., -3.8338e+00,
          -7.6356e-01, -7.8056e-01],
         ...,
         [-4.9232e-02,  4.4225e+00, -6.1654e-01,  ..., -3.4326e+00,
           3.4150e-02, -1.3201e+00],
         [ 4.4167e-02,  7.6584e+00,  2.8834e-01,  ..., -1.6596e+00,
          -1.1424e-01, -9.4820e-02],
         [ 2.5450e-02,  1.2953e+01, -2.0717e-01,  ..., -1.5539e+00,
           2.0845e-01, -1.6113e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-21.6147, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(129.7623, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)
Red Horizon Horizon oil company shares share despite company shares criticising company company company company company .
iter = 1 reward = 11
final_logits =  tensor([[[ 2.0565e-02,  2.8138e+00,  7.2563e-01,  ..., -4.2009e+00,
           3.2786e+00,  1.0321e+00],
         [ 4.6069e-02,  2.8383e+00,  3.9652e-01,  ..., -5.4070e+00,
           5.8939e-01, -2.2780e+00],
         [ 6.0614e-03,  1.9528e+00, -5.5237e-02,  ..., -6.0184e+00,
           5.3363e-01, -2.3086e-01],
         ...,
         [ 1.1667e-02,  6.6558e+00, -4.4863e-01,  ..., -4.9246e+00,
           7.1326e-01, -1.1082e+00],
         [ 5.7703e-02,  9.0255e+00,  3.4902e-01,  ..., -4.1064e+00,
          -2.0688e-01,  6.2071e-01],
         [ 3.5927e-02,  1.2430e+01, -2.4186e-01,  ..., -2.7890e+00,
          -2.5580e-02, -8.4176e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-1.3164, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(16.4624, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)
Deepwater Horizon oil company shares up to 11 per cent since April 2012 .
iter = 2 reward = 5
final_logits =  tensor([[[ 2.5800e-02,  3.6972e+00,  6.0306e-01,  ..., -4.2204e+00,
           1.7720e+00,  1.6676e+00],
         [ 1.1285e-02,  3.2030e+00,  2.6272e-01,  ..., -4.8705e+00,
           7.2835e-01, -1.0521e+00],
         [ 1.4248e-03,  1.6931e+00, -1.6066e-01,  ..., -5.0331e+00,
           4.2948e-01,  3.6832e-01],
         ...,
         [-1.8293e-02,  5.6818e+00, -8.1715e-01,  ..., -3.5421e+00,
          -5.0752e-01, -2.7055e+00],
         [ 6.8119e-02,  9.4048e+00,  1.9165e-01,  ..., -1.6696e+00,
           2.0420e-01,  3.1665e-01],
         [ 4.5763e-02,  1.2048e+01, -3.3304e-01,  ..., -2.2902e+00,
          -3.3700e-01, -6.1523e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.7291, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(12.1870, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)
Last year Maryam 500 500 rose as it was the first time in the .
iter = 3 reward = 1
final_logits =  tensor([[[ 2.9425e-02,  3.6349e+00,  6.8799e-01,  ..., -4.4271e+00,
           1.5494e+00,  1.8842e+00],
         [-9.8892e-03,  2.6064e+00, -1.2673e-01,  ..., -9.7700e-01,
           4.9405e-01,  8.9591e-02],
         [ 7.2906e-03,  1.8601e+00, -4.4271e-01,  ..., -3.8182e+00,
          -1.4013e-01, -9.7467e-01],
         ...,
         [ 3.1472e-02,  8.6617e+00, -4.9791e-01,  ..., -2.5684e+00,
          -7.9080e-01,  3.9421e-02],
         [ 7.2877e-02,  9.5494e+00,  2.8705e-01,  ..., -1.6806e+00,
          -6.2747e-01, -1.6757e-01],
         [ 2.8958e-02,  1.2230e+01, -4.5646e-01,  ..., -2.5759e+00,
          -1.2561e+00, -7.5435e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.9438, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(8.9239, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)
Deepwater Horizon oil company shares up to 500 up to higher than the price of the year .
iter = 4 reward = 6
final_logits =  tensor([[[ 0.0325,  3.4327,  0.8667,  ..., -4.2915,  1.4501,  1.6901],
         [ 0.0128,  3.0666,  0.3729,  ..., -4.5563,  1.3400, -0.9267],
         [ 0.0141,  1.6005, -0.0408,  ..., -4.7034,  0.0786, -0.3600],
         ...,
         [ 0.0136,  9.4143, -0.6774,  ..., -1.3994, -1.4510,  0.1301],
         [ 0.0724,  9.0830, -0.0286,  ..., -1.4088, -0.1911,  0.0316],
         [ 0.0481, 12.2361, -0.3234,  ..., -2.0174, -0.4751, -0.5352]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(5.8267, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(21.7060, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  6
6
{'input_ids': tensor([[[ 3788,   110,   108,   258,   111,  1183,   109,  4193,   120,  5887,
            119,  3788, 22017,  3249,  2309,   114,  4340,   132,  2162,   110,
            108,   132,   989,  6502,   115,   150,   545,  1771,  6180,   114,
           4340,   493,   126,   586,   112,   258,   165,   241,   111,   173,
            157,   133,  6502,   110,   107, 14845,   127,  1661,   115,   156,
            295,   110,   108,  2063,   119,  1235,   111,   400,   489,   110,
            107, 21556,   116,  4170, 16036,   762, 14567,  1736,   651,  1265,
           1185,  2652,   110,   108,  3013,   111,  9792,  5297,  5299,  2346,
           3601,  2567,  7499,   114,  1736,   266,  1678,   115,   109, 11319,
            124,   109, 16036,   762, 14567,   110,   107,  2346,  3601,  2567,
            898,  6949,   120,   142,  8635,   933,   113,  1647,   196,   506,
            174,  9889,   110,   108,   155,   701, 51098,   192,   129,   656,
            130,  6031,  1219,   115,  4190,  6500,  3381,   113, 43278,   110,
            107,   139,   657,   148, 18097,   112,   110,   105,   543,   109,
           2919,   113,   219,  6209,   702, 16837,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)
Climate politics has a deeper topic of Marland affects oil politics allowing an urgent oil debate to
iter = 0 reward = 2
final_logits =  tensor([[[ 7.8574e-02,  3.9715e+00,  7.4653e-01,  ..., -1.8200e+00,
          -4.9120e-01, -2.0182e+00],
         [ 1.1349e-02,  3.0860e+00,  2.6778e-01,  ..., -1.9712e+00,
          -1.4348e+00, -9.5908e-01],
         [ 1.7203e-02,  1.9324e+00, -2.6613e-01,  ..., -1.5734e+00,
          -2.2289e+00, -1.5246e+00],
         ...,
         [-3.4029e-03,  6.7338e+00, -1.4614e-01,  ..., -1.6608e+00,
          -1.3535e+00, -6.9725e-01],
         [ 1.0970e-02,  8.3720e+00, -6.7908e-01,  ..., -1.8099e+00,
          -1.0695e+00, -2.0364e+00],
         [ 5.3777e-02,  1.2192e+01, -3.1861e-01,  ..., -9.4757e-01,
          -1.4320e+00, -1.9871e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.1325, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(3.7807, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)
Marland: Allowing an urgent oil debate allowing an urgent government debate allowing an an urgent access to
iter = 1 reward = 1
final_logits =  tensor([[[ 1.1760e-01,  4.9467e+00,  1.0978e+00,  ...,  1.7563e+00,
           7.5975e-02, -2.3111e+00],
         [ 6.5780e-02,  5.4216e+00,  5.7754e-01,  ...,  1.3449e+00,
          -1.4339e+00,  6.3208e-01],
         [ 1.1462e-01,  4.2899e+00,  8.9404e-02,  ..., -6.3552e-01,
          -2.0498e+00, -1.5963e+00],
         ...,
         [ 3.5749e-02,  8.8388e+00,  1.2719e-01,  ...,  2.4232e-03,
           7.2829e-01, -5.7003e-01],
         [ 3.3237e-02,  9.0342e+00, -4.3430e-01,  ..., -1.1044e-01,
          -1.1178e+00, -9.8089e-02],
         [ 6.5318e-02,  1.2108e+01, -2.1615e-01,  ..., -1.3012e+00,
          -7.7786e-01, -1.2772e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.3538, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(5.8583, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)
Marland listed as an urgent oil debate allowing an urgent oil debate listed by peers listed as a topic of the topic of Marland listed .
iter = 2 reward = 2
final_logits =  tensor([[[ 1.0261e-01,  2.9397e+00,  7.9764e-01,  ..., -1.4857e+00,
          -6.6181e-01, -3.0718e+00],
         [ 5.9846e-02,  3.7219e+00,  3.4452e-01,  ...,  3.6748e-01,
          -2.5137e-01,  7.3716e-01],
         [ 9.5771e-02,  2.8241e+00, -1.7301e-01,  ..., -2.9607e+00,
          -2.2854e+00, -3.5058e+00],
         ...,
         [ 6.8882e-02,  5.5631e+00, -4.7834e-01,  ..., -1.7216e+00,
          -6.8043e-01, -1.0516e+00],
         [ 9.5314e-02,  9.0543e+00,  1.7045e-01,  ..., -1.7912e-02,
          -6.3723e-01, -1.1996e-02],
         [ 7.8272e-02,  1.2567e+01, -1.1514e-01,  ..., -5.7591e-02,
           1.0081e-01, -1.9852e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(4.1765, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(19.9179, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)
Marland listed topic of an urgent debate allowing oil debate listed on oil debate listed on oil debate listed .
iter = 3 reward = 3
final_logits =  tensor([[[ 0.1017,  1.5865,  0.8998,  ..., -0.7596, -0.2914, -3.1687],
         [ 0.0549,  3.6516,  0.3645,  ...,  0.3010, -0.3690,  0.9718],
         [ 0.0910,  2.8493, -0.0903,  ..., -2.4953, -1.9599, -2.5803],
         ...,
         [ 0.0700,  5.8352, -0.3730,  ..., -1.9826, -1.2217, -2.1802],
         [ 0.0990, 11.1683,  0.3245,  ..., -0.5101, -1.1399, -0.7657],
         [ 0.0895, 12.8253,  0.0411,  ..., -0.1161, -0.3367, -2.9812]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(4.1127, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(15.4118, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)
Marland listed topic of an urgent debate allowing an urgent debate listed on oil debate listed on the topic of Marland listed .
iter = 4 reward = 1
final_logits =  tensor([[[ 0.0898,  1.6812,  0.7608,  ..., -1.3814,  0.1129, -3.4485],
         [ 0.0498,  3.6999,  0.3102,  ...,  0.4167, -0.5358,  1.0447],
         [ 0.0849,  2.4716, -0.1826,  ..., -2.5839, -1.5715, -3.0173],
         ...,
         [ 0.0617,  4.7839, -0.4361,  ..., -2.4993, -1.0259, -1.4028],
         [ 0.0941,  7.4024,  0.1961,  ..., -0.4030, -0.6505, -0.4997],
         [ 0.0895, 12.7743, -0.0927,  ..., -0.5797, -0.0539, -2.5850]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.9750, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(5.1826, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  1
1
{'input_ids': tensor([[[ 1428,  1307,  2652,  2882,  2033,   134, 84838,   726, 17403,  4596,
          16036,   148,  1939,   114,   177, 14464,  3889,   124,   109, 14154,
           7175,   113,  3064,   762,   210,   110,   108,   301,  2662,   416,
            110,   107,   168,   117,  8549,   109,   177,  3889,   138,   923,
            109,  8186,   111,  3155,   109,   762,   269,   154,   113,   126,
            137,  6218,   190,   109,  1917,   155,   126,   256,   129,   372,
            228,   390,   269, 16036,   235,   682,   109,   807,  2353,   117,
           1147,   110,   107,   139, 11335,   113,   109, 81065, 18308, 13773,
            115,   960,  3040,  1073,   200,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)
Deepwater Horizon has killed 11 people since April 3,
iter = 0 reward = 3
final_logits =  tensor([[[ 0.0651,  4.8275,  0.7344,  ..., -1.6588,  0.6549,  0.4728],
         [ 0.0450,  4.5027,  0.3209,  ..., -3.1288, -0.4573, -0.7194],
         [ 0.0628,  3.4165,  0.0211,  ..., -3.0458, -1.7441, -2.6463],
         ...,
         [ 0.0292,  5.4183, -0.2009,  ...,  1.0101, -3.9370,  0.9380],
         [ 0.0595,  7.0140, -0.2654,  ..., -2.9495, -2.8379, -2.4894],
         [ 0.0508, 10.1146, -0.0155,  ..., -2.5261, -0.2837, -0.3041]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.9181, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(5.3343, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)
Deepwater Horizon company has hoped 11 people have killed whether Deepwater Horizon killed whether
iter = 1 reward = 6
final_logits =  tensor([[[ 0.0712,  4.2777,  0.6214,  ..., -2.2053,  0.4339, -0.0836],
         [ 0.0411,  3.8123,  0.2878,  ..., -3.5807, -0.5631, -0.8006],
         [ 0.0577,  2.0060, -0.0775,  ..., -3.5338, -2.1852, -2.8045],
         ...,
         [ 0.0838,  4.9085, -0.1903,  ..., -1.2655, -0.9004, -2.8630],
         [ 0.0270,  8.8377, -0.2253,  ..., -3.3100, -0.6801, -0.4208],
         [ 0.0429, 10.6110, -0.1559,  ..., -2.9256, -0.1635,  0.2759]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.7356, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.0389, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)
Deepwater Horizon company has hoped 11 people have installed another successful oil company that
iter = 2 reward = 6
final_logits =  tensor([[[ 6.5174e-02,  3.6480e+00,  6.2622e-01,  ..., -2.8098e+00,
           4.2346e-01, -2.4796e-01],
         [ 4.2669e-02,  3.7774e+00,  2.6189e-01,  ..., -3.8792e+00,
          -6.3654e-01, -1.0062e+00],
         [ 6.1970e-02,  1.7723e+00, -6.4733e-02,  ..., -3.9063e+00,
          -2.1231e+00, -3.0295e+00],
         ...,
         [-7.7745e-05,  3.9704e+00, -3.1159e-01,  ..., -3.9970e+00,
          -2.2746e+00, -6.2638e-01],
         [ 5.7407e-02,  6.1721e+00, -6.2588e-01,  ..., -2.6590e+00,
          -1.8623e+00, -1.5678e+00],
         [ 7.3765e-02,  1.2149e+01, -2.4286e-01,  ..., -1.7412e+00,
          -1.6618e+00, -4.3057e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.1123, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(3.1814, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)
Deepwater Horizon company hopes 11 people have hoped 11 people have
iter = 3 reward = 5
final_logits =  tensor([[[ 5.7196e-02,  3.7010e+00,  5.8850e-01,  ..., -2.6728e+00,
           3.4711e-01, -6.4083e-01],
         [ 4.1748e-02,  3.8104e+00,  2.9042e-01,  ..., -3.8091e+00,
          -5.6848e-01, -7.2689e-01],
         [ 5.4578e-02,  1.9220e+00, -1.3877e-01,  ..., -4.0519e+00,
          -2.2570e+00, -3.1634e+00],
         ...,
         [ 5.8856e-02,  4.9389e+00,  4.7450e-03,  ..., -3.5698e+00,
          -2.9654e+00, -1.6662e+00],
         [ 3.4935e-02,  6.3292e+00, -4.8635e-01,  ..., -3.4620e+00,
          -4.1733e+00, -1.6668e+00],
         [ 6.4986e-02,  9.3216e+00, -1.1831e-01,  ..., -2.5860e+00,
          -2.4666e+00, -9.7212e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-2.6454, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(5.4621, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)
Deepwater Horizon oil company has hoped 11 people have hoped 11 successful company has been successful this month .
iter = 4 reward = 7
final_logits =  tensor([[[ 0.0586,  3.9863,  0.4988,  ..., -3.2127,  0.8542, -0.8057],
         [ 0.0423,  3.7450,  0.2742,  ..., -3.9525, -0.3456, -0.7478],
         [ 0.0469,  2.2600, -0.1418,  ..., -4.0433, -1.8068, -3.0048],
         ...,
         [ 0.0605,  8.1767, -0.4946,  ..., -2.3851, -1.5561, -0.8755],
         [ 0.0843,  8.2129,  0.2313,  ..., -1.9619, -1.1173, -0.1435],
         [ 0.0616, 12.6060, -0.1724,  ..., -2.2153, -0.9327, -0.7856]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.2972, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(4.6782, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  7
7
{'input_ids': tensor([[[16036,   762, 14567,   114,   711,   113,   110,   105,  2111,  1057,
            395,  1034,   139, 21549, 16036,   762, 14567,   115,   109,  7175,
            113,  3064,   148, 50233,   109,  4170,   160,  3079,   644,  2175,
            110,   108,   155,   109,  1319,  1977,   113, 11461,   649,   109,
          14567,   140,   109,   711,   113,  1025, 46366,   110,   107,  1084,
          44907, 41534,   140,  1977,   113, 11461,  1034,   116,   655,   260,
            135,  5109,   112,  3390,   111,   148,   243,   223,   644,   524,
            632,   112,   823,   114,   110,   105, 30493,   824,   113,   109,
           2379,   110,   107, 16837,   125,   116,  1086,   734,   112,   145,
           1321,  1110,   299,   114,   300,   121,  1704,  6030,   112, 11881,
          13922,   110,   152,   325,   117,   461,   762,   297,   113,   109,
            951,   110,   108,   132,   109,   575,   110,   152,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1]]])}

Before Sampling 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)
John has a big debate in the oil spill- which has been made in the
iter = 0 reward = 1
final_logits =  tensor([[[ 0.0967,  6.1199,  0.8139,  ..., -1.2268,  0.8930, -1.1194],
         [ 0.0799,  4.3028, -0.1673,  ..., -3.2317, -0.4968, -1.1934],
         [ 0.0660,  3.9618,  0.1061,  ..., -1.7574, -0.1636, -2.0721],
         ...,
         [-0.0197,  5.7436, -0.4736,  ..., -1.8672, -0.2671, -1.8406],
         [ 0.0161,  7.9310, -0.2620,  ..., -1.6138, -0.0418, -1.1925],
         [ 0.0257,  9.3285, -0.0308,  ..., -1.8823,  0.3579, -1.9923]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-2.5967, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(4.3778, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.2% (4212 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)
2008 spill spill has been blamed for a poor oil addiction and a 'fuels to a poor oil addiction'
iter = 1 reward = 4
final_logits =  tensor([[[ 9.0263e-02,  5.5118e+00,  8.6712e-01,  ..., -1.0254e+00,
           3.8907e-01, -6.0712e-01],
         [ 5.8342e-03,  3.5386e+00,  2.0395e-01,  ..., -2.4615e+00,
          -8.4256e-03, -1.1227e+00],
         [-4.2360e-02,  1.9358e+00, -4.9318e-01,  ..., -9.9173e-01,
          -6.9193e-01, -2.3074e+00],
         ...,
         [-4.8201e-02,  5.9122e+00, -5.0392e-01,  ..., -3.3921e+00,
          -3.6557e-01, -1.5832e+00],
         [ 3.8632e-02,  6.2100e+00, -5.9468e-01,  ..., -2.1873e+00,
          -7.1086e-01, -2.0373e+00],
         [ 3.5444e-02,  1.2322e+01, -5.8142e-01,  ..., -2.0810e+00,
          -9.1171e-01, -2.0927e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-3.3940, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.5275, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)
2008 spill spill has been blamed for a poor oil spill in a poor result of a poor oil spill .
iter = 2 reward = 6
final_logits =  tensor([[[ 9.2188e-02,  5.4701e+00,  9.3299e-01,  ...,  2.1137e-01,
           8.0344e-02, -1.0447e+00],
         [-4.1670e-03,  3.4782e+00,  3.8253e-01,  ..., -1.9980e+00,
          -6.3357e-01, -1.0443e+00],
         [-3.1620e-02,  2.4005e+00, -4.2845e-01,  ..., -1.1922e+00,
          -9.1376e-01, -2.6301e+00],
         ...,
         [-2.8971e-02,  4.9502e+00, -8.0148e-01,  ..., -9.4966e-01,
          -4.9365e-01, -3.3479e+00],
         [ 6.5488e-02,  6.4932e+00,  1.8252e-01,  ..., -7.4119e-01,
          -9.3159e-02, -4.7738e-01],
         [ 5.7762e-02,  1.2998e+01, -1.9913e-01,  ..., -3.6081e-01,
          -3.9041e-01, -1.3051e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-2.5042, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(5.8521, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.9% (4284 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)
Gulf Mexico has a big debate of the big oil spill- big debate with big debate, says Shell's claim to be a solution to a problem of a problem .
iter = 3 reward = 3
final_logits =  tensor([[[ 1.2249e-01,  5.0811e+00,  9.4216e-01,  ..., -2.6048e+00,
          -1.2402e+00, -1.1518e+00],
         [ 2.2126e-03,  4.0146e+00,  4.7093e-02,  ..., -2.1783e+00,
          -2.8005e+00, -2.4198e-01],
         [ 2.1124e-02, -2.3361e-01,  1.0283e-01,  ..., -3.9967e+00,
          -1.5959e+00, -1.7333e+00],
         ...,
         [ 4.7793e-02,  7.4767e+00, -5.0887e-01,  ..., -1.5248e+00,
          -1.1535e+00, -3.4590e+00],
         [ 7.0845e-02,  7.1216e+00,  2.7365e-01,  ..., -2.2634e+00,
          -6.2157e-02, -9.4703e-02],
         [ 6.2161e-02,  1.3453e+01, -1.2767e-01,  ..., -8.7361e-01,
          -9.7118e-01, -1.2653e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(5.4314, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(9.5677, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)
Gulf Mexico has guarded debate of the worst oil spill- result of John, the problem of a
iter = 4 reward = 3
final_logits =  tensor([[[ 7.6795e-02,  5.3768e+00,  7.4971e-01,  ..., -3.0125e+00,
          -1.7052e+00, -2.3796e+00],
         [-1.8319e-03,  4.1432e+00,  7.5970e-02,  ..., -2.4994e+00,
          -2.5614e+00,  1.3009e-01],
         [ 2.0148e-02,  1.6866e+00,  9.1485e-02,  ..., -2.4589e+00,
          -1.3494e+00, -1.9466e+00],
         ...,
         [ 3.7707e-02,  5.6046e+00, -4.1249e-01,  ..., -2.0406e+00,
          -9.4492e-01, -1.8146e+00],
         [ 4.0918e-02,  6.6377e+00, -2.1444e-02,  ..., -2.7665e+00,
          -2.4029e-01, -1.8125e+00],
         [ 2.5966e-02,  1.0901e+01,  8.9403e-02,  ..., -2.4874e+00,
           2.7733e-01, -2.3209e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(4.1798, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.9311, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  3
3
{'input_ids': tensor([[[ 1346,  6661,   118,   762, 14567,   945,   139,  2800,  1728,   112,
           2555,   110,   105, 41638, 16837,  1003,   121,   768,  1739,   120,
            133,   174,   263,   115,   109,  7175,   113,  3064,   139,  1346,
           6661,  2800,   110,   108,   229,   606,   118,  6957,   109,   808,
          70959,   503,   110,   108,   148,  2365,   114,  3662, 13602,   604,
            762,  1003,   121,   768,  1459,   110,   107,   139,  2800,   110,
            108,   162,  1653,   120,   203,  1962,  2560,   117,   110,   105,
            112,   650,   160,  8695, 33237,   118,   109,  1280,   113,  7633,
          16837,  1487,   203,   807,  4086,   134,   114,  1833,  1792,   115,
           1741,  3710,   110,   107,   182,   117,   203,  6932,   110,   105,
            698,  9501,  1702, 16837,   110,   107,  1810,  1518,   137,  2337,
            118,   109,  1702,   430,   960,  2651,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1]]])}

Before Sampling 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)
X Prize Foundation Foundation announced the best foundation to replace the .
iter = 0 reward = 0
final_logits =  tensor([[[ 5.7647e-02,  3.6772e+00,  7.6098e-01,  ..., -8.0755e-01,
           1.6070e+00, -2.0387e+00],
         [ 2.9813e-02,  4.6572e+00,  5.4158e-02,  ..., -1.2225e+00,
          -2.1044e+00, -2.0773e+00],
         [ 1.1208e-02,  3.3518e+00, -1.9865e-01,  ..., -2.5527e+00,
          -1.5456e+00, -9.4967e-01],
         ...,
         [ 4.3374e-02,  1.1002e+01,  4.2376e-01,  ..., -3.4457e+00,
           4.6145e-01, -1.1423e+00],
         [ 4.7499e-02,  1.2898e+01,  8.6407e-01,  ..., -8.2794e-01,
           4.2510e-01, -1.9888e-01],
         [ 3.3121e-02,  1.3162e+01,  8.0735e-02,  ..., -9.2430e-01,
          -6.8660e-01, -1.3391e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.2122, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(45.9134, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)
X Prize announced major spaceflight breakthroughs to the latest X Prize .
iter = 1 reward = 0
final_logits =  tensor([[[ 4.0250e-02,  2.5532e+00,  9.7754e-01,  ..., -6.3479e-01,
           8.3915e-01, -9.9952e-01],
         [ 1.6037e-02,  4.8405e+00,  5.8809e-02,  ..., -1.1428e+00,
          -1.6363e+00, -1.5406e+00],
         [ 4.8335e-03,  2.3875e+00, -5.2674e-02,  ..., -1.7346e+00,
          -7.7277e-01,  4.2365e-02],
         ...,
         [ 6.5699e-04,  5.5453e+00, -5.2412e-02,  ..., -2.0115e+00,
          -1.9407e+00,  1.8839e+00],
         [ 4.8902e-02,  1.1715e+01,  3.7979e-01,  ..., -1.0458e+00,
          -9.3054e-01,  5.6440e-01],
         [ 3.8603e-02,  1.2846e+01, -3.4708e-02,  ..., -1.0802e+00,
          -4.6455e-01, -1.9539e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-7.8839, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(46.7553, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)
1.4 million oil Prize oil Prize solutions announced major spill solutions to
iter = 2 reward = 4
final_logits =  tensor([[[ 4.5951e-02,  3.6852e+00,  9.4303e-01,  ..., -1.3877e+00,
           2.4278e-01, -1.5199e+00],
         [ 1.9556e-02,  5.6063e+00,  1.6699e-01,  ..., -1.9195e+00,
          -3.3798e+00,  8.1026e-01],
         [-1.3426e-02,  3.1118e+00,  2.2245e-02,  ..., -2.1652e+00,
          -2.0355e+00,  5.6733e-01],
         ...,
         [-5.1165e-02,  5.4827e+00,  4.9706e-01,  ...,  9.5848e-02,
          -1.4837e+00,  2.3072e+00],
         [ 9.0704e-03,  6.1078e+00, -2.8886e-01,  ..., -2.3069e+00,
          -1.2824e+00, -6.7758e-01],
         [ 4.8426e-02,  1.1369e+01, -6.8281e-02,  ..., -2.0712e+00,
          -2.0880e+00, -1.2594e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-3.1991, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(19.0418, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)
This competition announced major oil Prize breakthroughs major oil solutions .
iter = 3 reward = 2
final_logits =  tensor([[[ 0.1026,  4.8761,  1.0330,  ..., -0.8822,  0.6530, -2.2958],
         [ 0.0548,  4.3957,  0.0530,  ..., -1.7051, -0.6295, -1.0883],
         [ 0.0587,  3.7248,  0.0397,  ..., -0.9622, -1.2240, -1.6550],
         ...,
         [ 0.0301,  5.5335, -0.4447,  ..., -1.3859, -0.8100, -1.1443],
         [ 0.0591,  6.9603,  0.1407,  ..., -1.1931, -1.5393, -0.4001],
         [ 0.0599, 12.8949,  0.1078,  ..., -0.7685, -0.9904, -0.9606]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.5398, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(8.0110, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.2% (4214 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)
Gulf Prize Prize Prize announced latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest oil solutions .
iter = 4 reward = 2
final_logits =  tensor([[[ 0.0888,  3.2171,  0.7718,  ..., -0.3894,  0.5294, -2.4617],
         [ 0.0512,  3.9518,  0.2034,  ..., -0.8686, -1.7989,  0.5190],
         [ 0.0375,  2.5993, -0.0583,  ..., -1.5279, -2.2155, -1.9027],
         ...,
         [ 0.0376,  6.5369, -0.3211,  ..., -2.7758, -1.7867, -0.9305],
         [ 0.0636,  8.6213, -0.0155,  ..., -0.5507, -0.9931, -0.4748],
         [ 0.0407, 13.3903,  0.0217,  ..., -1.2488, -0.6758, -1.2049]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.7108, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(8.0278, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  2
2
{'input_ids': tensor([[[ 3211,  1307,  2652,  2882,  2033,   134,   280, 85536, 17403,  4596,
            398, 16036,  1034,   116, 11599,  2664,   109,   519,   113,   114,
           6033,   124,  9134,  9600,   110,   108,   109, 12220,   113, 14645,
           9727,   135,   109,   762, 14567,   115,   109,  7175,   113,  3064,
            148,  9380,   164,   115,   114,  2043, 22369,   115, 10792,   110,
            107,   353,  1761,  7690,   355,  1854,   199,   109, 18191,   113,
          14645,   246,   129,  8626,   122,   110,   107,  9903, 90966,  1574,
            135,   351,   859,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)
Gulf oil spill spill spill spill spill rivals Idaho, has seven counts counts, seven in seven states, seven in seven states .
iter = 0 reward = 12
final_logits =  tensor([[[ 1.0226e-01,  3.4304e+00,  8.3515e-01,  ...,  7.1362e-02,
          -2.4487e-01,  8.8012e-01],
         [ 3.0369e-03,  3.2438e+00, -2.0327e-01,  ..., -2.7450e+00,
          -1.9776e+00,  3.5761e-01],
         [-1.2908e-02,  2.9652e+00,  1.7058e-01,  ..., -2.6535e+00,
          -7.4177e-01,  5.2547e-02],
         ...,
         [ 6.3122e-02,  7.8042e+00, -5.6438e-01,  ..., -5.0008e-01,
          -5.8635e-01, -3.8660e-01],
         [ 9.3619e-02,  8.4889e+00,  3.3550e-01,  ..., -8.2724e-01,
          -4.9120e-01,  1.4032e+00],
         [ 7.5206e-02,  1.1394e+01, -1.5951e-01,  ...,  6.0902e-01,
          -3.3997e-01,  8.3438e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(26.3951, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(130.1841, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)
Gulf oil spill rivals Idaho report seven lawsuits are being counted in seven, seven, seven, count .
iter = 1 reward = 5
final_logits =  tensor([[[ 0.0992,  2.6928,  0.9152,  ...,  1.8798, -0.6266,  0.2251],
         [-0.0353,  2.0226, -0.1629,  ..., -1.6764, -1.5102,  0.1288],
         [-0.0475,  1.9927,  0.2092,  ..., -1.8216, -0.5557, -1.3066],
         ...,
         [ 0.0292,  6.3900, -0.3714,  ...,  1.1899, -0.4363, -1.0827],
         [ 0.0548,  5.9241,  0.3532,  ...,  0.2918,  0.1431,  0.2799],
         [ 0.0537, 12.1753, -0.3651,  ...,  0.5450, -1.1912, -2.6058]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(10.6131, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(46.5472, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)
July 29 spill spill spill rivals: seven, reports seven lawsuits arising on Idaho, seven, .
iter = 2 reward = 5
final_logits =  tensor([[[ 7.7600e-02,  5.6030e+00,  7.5313e-01,  ...,  1.2610e+00,
          -4.7373e-01,  3.1035e-01],
         [ 6.6270e-02,  6.8110e+00,  2.7986e-01,  ..., -1.2953e-01,
          -4.0676e+00, -1.4162e+00],
         [ 3.1659e-02,  5.6264e+00,  4.3021e-01,  ...,  6.8418e-01,
          -1.2962e+00, -1.4789e+00],
         ...,
         [ 5.5100e-02,  8.3793e+00, -4.9973e-02,  ..., -1.2081e+00,
           4.8697e-05, -2.3755e-01],
         [ 7.1165e-02,  9.1149e+00,  4.8359e-01,  ..., -3.9434e-01,
          -1.1395e+00,  1.3384e+00],
         [ 4.8663e-02,  9.4612e+00,  3.8948e-02,  ...,  1.4966e+00,
          -1.2432e+00, -1.1767e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(6.1918, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(11.1719, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)
Gulf spill spill spill reports seven lawsuits arising on seven states and seven in seven states .
iter = 3 reward = 9
final_logits =  tensor([[[ 7.3664e-02,  2.9794e+00,  9.9376e-01,  ...,  1.9970e+00,
          -1.3059e+00,  1.0714e+00],
         [-3.3272e-02,  3.0341e+00, -2.8689e-01,  ..., -3.2192e+00,
          -2.4259e+00, -3.0866e-01],
         [-6.2789e-02,  1.3907e+00, -1.4429e-01,  ..., -2.3311e+00,
           1.3297e-02, -1.4420e+00],
         ...,
         [-7.2620e-03,  7.4834e+00, -6.7931e-01,  ...,  1.8299e-02,
          -1.9164e+00, -1.3007e+00],
         [ 3.7345e-02,  5.4580e+00,  2.3338e-01,  ...,  5.9575e-02,
          -5.3405e-01,  3.8287e-01],
         [ 3.0042e-02,  1.2736e+01, -2.7702e-01,  ...,  6.0546e-01,
          -2.3170e+00, -1.4161e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.7862, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(3.2564, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)
July 29 spill spill rivals: seven oil rivals: seven Idaho spill rivals .
iter = 4 reward = 6
final_logits =  tensor([[[ 0.0473,  5.5347,  0.8764,  ...,  0.6532, -0.8502,  0.1882],
         [ 0.0157,  6.0863,  0.4079,  ..., -0.5963, -3.2637, -1.2229],
         [-0.0277,  5.1052,  0.4770,  ..., -0.3323, -1.1730, -0.7381],
         ...,
         [ 0.0157,  7.7189, -0.1027,  ..., -0.1494,  0.8856, -0.1908],
         [ 0.0319,  9.2085,  0.6014,  ...,  0.2148, -0.5366,  1.3513],
         [ 0.0212, 11.5973,  0.1716,  ...,  0.9155, -1.5109, -0.9921]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-11.3781, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(41.4745, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  6
6
{'input_ids': tensor([[[ 2571,  1508,  2652,  2882,  2033,   134,   305, 49218, 17403,  4596,
            222,  6358,   109, 40522, 34524,  4820,   164,   299,  4027,  1034,
            116, 62223,   454,  3682,  3793,   156,   113,   109,   278,  1034,
            116,  3741,   762, 12802,   110,   107,  1478,   115,   109,  5569,
            110,   108,  3070,   111,   176,  3217, 17659,   109,  1322,   192,
            394,  5097,   110,   107,   343, 62223,   148,   174, 71731,   115,
            109,  1965, 43983,   231,   110,   108,   112,   253,   142,  4156,
            120,   126,   117,   744,   130,   175,   109,  2648,   394,  2032,
            110,   107,   168,   117,  8549, 62223,  1034,   116,   584,   256,
            319,  2520,   118,   274,   115,   109,  7175,   113,  3064,   170,
            127, 48322,   120,   157,   138,   521,  5097,   135,   109,  2926,
          16036,   762, 14567,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1]]])}

Before Sampling 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)
Gulf Mexico oil region broke eight oil region almost almost 8 almost 8 years ago .
iter = 0 reward = 5
final_logits =  tensor([[[ 4.4003e-02,  2.9067e+00,  7.3585e-01,  ..., -2.5024e+00,
          -5.8996e-01, -2.6812e+00],
         [-3.4540e-02,  3.4461e+00,  3.9539e-02,  ..., -2.4917e+00,
          -1.7810e+00, -9.0899e-01],
         [-2.2770e-02,  1.7908e+00, -1.5824e-02,  ..., -2.4533e+00,
          -1.5501e+00, -1.8680e+00],
         ...,
         [ 1.6780e-03,  7.6478e+00, -5.0895e-01,  ..., -4.1508e-01,
          -2.4703e+00, -3.0027e+00],
         [ 4.8903e-02,  1.0578e+01,  2.3703e-01,  ..., -5.1096e-01,
          -1.4860e+00, -1.0096e+00],
         [ 2.5486e-02,  1.2905e+01, -1.4465e-01,  ..., -1.0308e+00,
          -1.5171e+00, -2.1198e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-7.5229, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(19.4056, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)

Everytime after generating next logits 38.4% (4234 out of 11019)
Galician 26 region broke an inspiration to Galician
iter = 1 reward = 1
final_logits =  tensor([[[ 6.0095e-02,  3.3400e+00,  9.3086e-01,  ..., -3.7747e+00,
          -9.1154e-02, -1.5782e+00],
         [ 4.8870e-02,  3.4704e+00,  1.8467e-01,  ..., -3.4154e+00,
          -1.8059e+00, -1.4909e+00],
         [ 1.4124e-02,  5.1948e+00,  1.5008e-01,  ..., -4.5265e+00,
          -1.1793e+00, -2.0695e+00],
         ...,
         [ 4.7007e-02,  8.3988e+00,  1.4619e-01,  ..., -2.7389e+00,
          -8.6939e-01, -3.3391e+00],
         [ 2.3827e-02,  4.9575e+00,  8.4683e-02,  ..., -2.7639e+00,
          -1.3344e+00, -1.6733e+00],
         [ 6.6207e-03,  1.0784e+01, -4.2266e-02,  ..., -3.0548e+00,
          -6.1700e-01, -2.0060e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-5.9194, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(39.2951, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)
Gulf Mexico oil region broke 26 oil region broke it an extent Spain never achieved .
iter = 2 reward = 5
final_logits =  tensor([[[ 5.8570e-02,  1.9614e+00,  1.2032e+00,  ..., -5.3236e+00,
          -1.6536e+00, -2.6385e+00],
         [-3.2397e-02,  3.0879e+00,  1.0892e-01,  ..., -2.7144e+00,
          -1.6768e+00, -5.8052e-02],
         [-2.6468e-02,  1.2678e+00,  1.7712e-01,  ..., -3.0456e+00,
          -1.8413e+00, -2.1360e+00],
         ...,
         [ 6.0562e-04,  7.6364e+00, -1.5961e-01,  ..., -9.6892e-01,
          -3.5049e+00, -2.8318e+00],
         [ 3.7373e-02,  1.2049e+01,  1.5820e-01,  ..., -8.1676e-01,
          -1.3292e+00, -1.2172e+00],
         [ 3.2605e-02,  1.2928e+01, -1.1842e-01,  ..., -2.3990e+00,
          -1.9198e+00, -2.8747e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.0162, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(3.6175, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)
Gulf Mexico oil region broke 26 oil region broke 26 oil region .
iter = 3 reward = 7
final_logits =  tensor([[[ 0.0750,  2.1233,  0.8119,  ..., -3.6577, -0.8234, -1.0860],
         [-0.0380,  2.7264,  0.1305,  ..., -2.9279, -1.8215, -0.1663],
         [-0.0243,  0.7129,  0.0400,  ..., -3.1857, -2.1904, -2.3048],
         ...,
         [-0.0189,  4.5614, -0.2868,  ..., -1.9500, -1.6245, -2.8326],
         [ 0.0491,  9.0418,  0.2385,  ..., -0.8349, -1.3386, -1.0838],
         [ 0.0380, 11.8118,  0.0621,  ..., -1.8017, -2.2742, -2.5127]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.7021, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(13.0364, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)
Gulf Mexico oil region broke 26 oil region broke 26 oil region .
iter = 4 reward = 7
final_logits =  tensor([[[ 0.0704,  2.1363,  1.0389,  ..., -3.4185, -1.4423, -0.6196],
         [-0.0374,  2.4785,  0.2057,  ..., -2.6251, -2.0235, -0.3633],
         [-0.0365,  1.3157,  0.1327,  ..., -3.7265, -2.5398, -2.5094],
         ...,
         [-0.0147,  5.2898, -0.2053,  ..., -1.7655, -1.5086, -2.8452],
         [ 0.0505,  9.2762,  0.3225,  ..., -1.2247, -1.3589, -1.1138],
         [ 0.0350, 12.4073,  0.1294,  ..., -2.1754, -2.4092, -2.2030]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.3348, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(5.5911, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  7
7
{'input_ids': tensor([[[  787, 17984,   116, 16036,   204,  7175,   113,  3064,   762,  5135,
           1195,  1408,  2652,  2882,  2033,   134, 79386,   914, 17403,  4596,
            139,   706,  1013,   117,   112, 17984, 16036,   111,  1965,   176,
            524,   204,   114,  1124,   762, 14567,   115,   109,  7175,   113,
           3064,   110,   107,   139,   657,   243,   126,   192,  1137,   109,
           3358,  1069,  9873,   118,   109, 13353,   113,  2729,  1363,   124,
           1496,   164,   109,  3741,  2249,  5135,   115,   787,   689,   110,
            107, 36347,  1024,  2342,   115,   109, 11335,   124,   109, 81065,
          18308,  9600, 13773,   115,   960,   110,   107,   139,  6442,  1034,
            116, 55065, 66707,  1574,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1]]])}

Before Sampling 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)
December 16 16 Mexico Mexico Mexico Mexico spill over Deepwater Mexico spill over Deepwater Mexico spill over Gulf Mexico spill over .
iter = 0 reward = 7
final_logits =  tensor([[[ 4.4111e-02,  4.9241e+00,  9.8764e-01,  ..., -1.5429e+00,
           3.2872e+00, -7.8100e-01],
         [ 3.8746e-02,  5.5778e+00,  3.0856e-01,  ..., -2.1691e+00,
          -1.2199e+00, -1.3994e+00],
         [-3.5531e-03,  5.0096e+00,  3.4787e-01,  ..., -1.7207e+00,
          -3.2049e-01, -2.3202e+00],
         ...,
         [-5.2341e-02,  6.5504e+00,  3.3362e-01,  ..., -1.5460e+00,
           2.6168e+00, -5.0178e-01],
         [-2.6358e-02,  9.6201e+00,  7.9837e-01,  ..., -1.9270e+00,
           7.7311e-01,  1.3385e+00],
         [ 4.6304e-02,  1.3015e+01,  2.7061e-01,  ..., -1.8581e-01,
           2.0357e+00, -6.4981e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-9.8726, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(54.6440, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)
Deepwater Horizon oil spill over December oil spill over December 16 disaster over Mexico Mexico oil spill over December 16 .
iter = 1 reward = 9
final_logits =  tensor([[[ 0.0619,  5.4393,  0.9762,  ..., -2.5726,  2.7547, -0.1300],
         [-0.0303,  4.5445,  0.2408,  ..., -4.1996,  1.1980, -0.8791],
         [-0.0442,  4.9473, -0.0629,  ..., -4.1964,  0.3051, -2.1736],
         ...,
         [-0.0671,  8.6007, -0.1340,  ..., -3.8897, -0.1224, -2.4441],
         [ 0.0216, 10.2849,  0.4990,  ..., -1.9723,  0.3703,  0.8239],
         [ 0.0720, 13.5704,  0.3626,  ..., -0.5808,  2.1678, -0.7244]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-2.6877, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(10.2339, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)

Everytime after generating next logits 38.6% (4252 out of 11019)
Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth
iter = 2 reward = 0
final_logits =  tensor([[[ 7.0108e-02,  5.5315e+00,  9.9186e-01,  ..., -2.2702e+00,
           2.1551e+00,  2.0778e-01],
         [ 7.7965e-03,  5.7293e+00,  5.6945e-03,  ..., -2.8772e+00,
          -1.6446e-01, -7.0748e-01],
         [ 3.5563e-03,  5.8292e+00, -1.8232e-01,  ..., -3.5835e+00,
           3.7152e-01, -1.9737e+00],
         ...,
         [ 5.4641e-02,  1.1034e+01, -1.5523e-01,  ..., -3.1417e+00,
          -8.5645e-01, -1.4360e+00],
         [ 5.5859e-02,  1.1172e+01, -1.5293e-01,  ..., -3.1285e+00,
          -9.2760e-01, -1.4196e+00],
         [ 5.6458e-02,  1.1227e+01, -1.5334e-01,  ..., -3.1259e+00,
          -9.9504e-01, -1.3982e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-2.9465, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.7457, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.8% (4274 out of 11019)

Everytime after generating next logits 38.9% (4288 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)
CNN's Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout Hangout
iter = 3 reward = 0
final_logits =  tensor([[[ 0.0597,  5.8407,  0.9623,  ..., -1.9844,  2.5954,  0.1508],
         [ 0.1382,  4.6421,  0.3819,  ..., -2.5190, -0.0557, -2.9391],
         [ 0.0705,  6.1455,  0.4747,  ..., -1.4056, -1.5116, -0.5573],
         ...,
         [ 0.0355, 10.9517, -0.3954,  ..., -2.0134, -3.4730, -2.1280],
         [ 0.0353, 11.2727, -0.4072,  ..., -1.9981, -3.5203, -2.0980],
         [ 0.0352, 11.5423, -0.4163,  ..., -1.9865, -3.5566, -2.0692]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(5.1282, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(38.6177, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)

Everytime after generating next logits 38.8% (4272 out of 11019)
Gulf disaster over over over over over over over year-long recovery,
iter = 4 reward = 2
final_logits =  tensor([[[ 0.0474,  5.2617,  0.8881,  ..., -1.2313,  2.5698, -0.8208],
         [-0.0361,  4.4690,  0.4814,  ..., -2.5360,  0.6365, -1.1743],
         [-0.0782,  2.5356,  0.2874,  ..., -2.6081,  0.3509, -3.4107],
         ...,
         [-0.0404,  4.7929,  0.6412,  ..., -1.3279,  1.5076,  0.3661],
         [-0.0908,  6.7827, -0.4124,  ..., -4.3347,  0.8320, -1.4439],
         [-0.0335,  8.0982,  0.0426,  ..., -2.3075,  0.9796, -0.9842]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(5.2460, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(8.4419, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  2
2
{'input_ids': tensor([[[ 7915, 26939,  4355,  1194,   244, 16036,   762, 14567,  5823,  8437,
            117,   364,   109, 19954,   113,   109,  7175,   113,  3064,   235,
            149,   314,   210,   110,   107,   139,   908,   317,  6299,   111,
           1174,   117,  8988,   153, 19347,   578,   110,   107,   343,   136,
            232,   205,   113,   109, 27736,   127,  2609,   110,   107,   139,
          27736,   127,   146,  1622,   115,   762,   110,   108,   130,   109,
           6442,  1034,   116,  2040,  9518,  1574,   135,  7915,   110,   108,
            155,  3040,   299,   141,  2926, 21615, 22611,   116,   120,   195,
           2443,   112,  2512,   109, 15737,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)
BBC reports reports Gulf oil hit by economic impact between oil and oil on BP and BP .
iter = 0 reward = 10
final_logits =  tensor([[[ 5.6434e-02,  5.1788e+00,  9.2466e-01,  ..., -1.9949e+00,
           1.6612e+00, -6.1612e-01],
         [ 2.8122e-02,  4.1180e+00,  9.6588e-01,  ..., -2.6211e+00,
          -1.3421e+00,  5.7352e-01],
         [-1.2592e-01,  3.6552e+00, -4.5925e-04,  ..., -3.3947e+00,
          -9.1574e-01, -3.0724e+00],
         ...,
         [-2.9751e-02,  5.8021e+00, -1.9830e-01,  ..., -5.4086e+00,
          -1.4150e+00, -4.5948e+00],
         [ 2.0058e-02,  8.8890e+00,  4.2426e-01,  ..., -3.2911e+00,
           7.2258e-01, -9.7103e-01],
         [ 4.2477e-02,  1.2391e+01,  3.7348e-01,  ..., -2.2527e+00,
           4.2231e-01,  1.5556e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(4.7933, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(20.1730, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)
BBC reports Gulf Mexico hit economic impact between oil oil and oil hit between oil and oil and oil .
iter = 1 reward = 11
final_logits =  tensor([[[ 0.0241,  6.5028,  0.7203,  ..., -0.5933,  1.0077, -2.2272],
         [ 0.0198,  4.8213,  0.7401,  ..., -2.5681, -1.4260, -0.8629],
         [-0.1172,  4.8803, -0.0233,  ..., -2.5321, -0.7497, -3.0266],
         ...,
         [-0.1068,  6.1676, -0.1232,  ..., -4.2820, -0.4517, -4.4177],
         [-0.0253,  9.0488,  0.5078,  ..., -2.8211,  0.5988, -2.0784],
         [ 0.0202, 12.5766,  0.4113,  ..., -0.9716, -0.1082, -0.9620]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.7673, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(10.8595, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.7% (4266 out of 11019)

Everytime after generating next logits 38.8% (4278 out of 11019)

Everytime after generating next logits 38.8% (4278 out of 11019)

Everytime after generating next logits 38.8% (4278 out of 11019)

Everytime after generating next logits 38.8% (4278 out of 11019)
BBC reports Gulf Mexico economic hit by economic impact between oil and oil hit by the most oil hit by the most economic economic impact between oil and oil .
iter = 2 reward = 11
final_logits =  tensor([[[-8.3662e-04,  5.1344e+00,  6.9687e-01,  ..., -1.3243e+00,
          -2.0581e-01, -2.2952e+00],
         [ 3.1742e-02,  4.8587e+00,  8.1885e-01,  ..., -2.2125e+00,
          -1.8354e+00, -6.3362e-01],
         [-1.1102e-01,  4.9971e+00,  8.5824e-02,  ..., -3.0816e+00,
          -2.1798e+00, -3.3187e+00],
         ...,
         [-1.0788e-01,  6.5245e+00, -2.2119e-01,  ..., -3.7710e+00,
          -1.6904e+00, -3.4470e+00],
         [-5.1853e-03,  7.6012e+00,  7.7383e-02,  ..., -2.1748e+00,
          -4.4805e-01, -1.6447e+00],
         [ 4.7473e-02,  1.3380e+01,  3.5929e-01,  ..., -1.3594e+00,
           2.0267e-01, -3.0098e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.5283, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(7.4328, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)

Everytime after generating next logits 38.7% (4260 out of 11019)
Gulf Mexico reports reports economic hit oil hit by economic oil hit by Gulf oil fishermen .
iter = 3 reward = 10
final_logits =  tensor([[[-2.0319e-02,  5.8777e+00,  7.3694e-01,  ..., -6.4292e-01,
          -3.6535e-01, -2.8115e+00],
         [-3.7476e-02,  4.4010e+00,  3.0315e-01,  ..., -3.0895e+00,
          -2.8179e+00,  2.2424e-01],
         [-3.1733e-02,  2.8189e+00,  3.7278e-01,  ..., -3.5391e+00,
          -1.9742e+00, -2.3041e+00],
         ...,
         [-7.2868e-02,  6.8289e+00, -8.3566e-02,  ..., -3.6511e+00,
          -1.9125e+00, -3.3985e+00],
         [-5.0983e-02,  9.5268e+00,  5.0038e-01,  ..., -3.3387e+00,
          -1.4393e+00, -2.3790e+00],
         [ 3.8000e-03,  1.3182e+01,  4.2606e-01,  ..., -9.0990e-01,
          -7.5220e-01, -1.3264e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-6.8236, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(18.3856, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.6% (4250 out of 11019)
The Gulf Mexico's most economic oil hit by the most economic oil hit by the Gulf spill .
iter = 4 reward = 7
final_logits =  tensor([[[-6.0108e-03,  5.8877e+00,  7.9422e-01,  ..., -1.4509e-01,
          -4.4192e-01, -2.8463e+00],
         [-8.7468e-03,  5.4985e+00,  9.5976e-01,  ..., -4.2034e+00,
           3.5082e-02, -1.0530e+00],
         [-2.7389e-02,  4.2775e+00,  2.3189e-01,  ..., -3.6301e+00,
          -2.7241e+00,  2.9063e-01],
         ...,
         [-9.6129e-02,  5.9866e+00, -3.5453e-01,  ..., -3.3714e+00,
          -1.3248e+00, -2.8291e+00],
         [-1.8210e-02,  9.1683e+00,  4.5377e-01,  ..., -3.1696e+00,
          -1.2150e+00, -7.5659e-01],
         [ 1.6038e-02,  1.2932e+01,  3.4550e-01,  ..., -7.5355e-01,
          -1.9347e-01, -9.4552e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-3.7213, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(10.8030, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  7
7
topic:  ('bpoil_bbc',)
env initialized...

Before training:  38.6% (4258 out of 11019)
{'input_ids': tensor([[[  110,   105,  3893, 15737,  1034,  8255,   464, 16036,   134,   109,
           1816,  2447,   139,  1816,  2447,   649,   126,   117,  4562,   112,
          16036,   118,   203,   337,   983,   762,   121,  2134,  5626,   148,
            174, 12832,   279,   156,   113,   109,  1816,  2447,  1034,   116,
          18072,   141, 10162, 18232,   126,   112,   370,   203, 10971,   818,
            122, 16036,   110,   107,   139,  4635, 41588,   110,   108, 50595,
          81452, 60490,   457,   131,   304,   110,   108,   117,   114,  4055,
          17225,   113,   114,   883,   693,   111, 28286,   111,   117,   160,
           6155,   231,   459,   110,   107,   202,   456,   568,  7200,  9221,
           3893,  2777,   165,   109,  8255,   110,   107,   202,   984,  8255,
           4635, 20678,  4329,   115,  1185,   110,   107,   139,  1816,  2447,
            243,   109,  5626,  1065,   140, 39059,   110,   108,   162,   196,
            146,  3954,   109,  4976,  2098, 11976,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)

Everytime after generating next logits 38.6% (4258 out of 11019)
An Easter Easter Easter Island sponsorship giant targeted a British Culture giant .
iter = 0 reward = 1
final_logits =  tensor([[[ 0.2252,  3.9453,  1.2299,  ..., -2.6227,  1.2716, -1.1831],
         [ 0.1112,  4.7279,  0.9280,  ..., -3.0831,  2.0018, -1.5812],
         [ 0.0347,  3.4784,  0.2409,  ..., -2.6915,  0.2143, -2.2887],
         ...,
         [ 0.0503,  5.1031, -0.2393,  ..., -1.4716, -0.2792, -2.6368],
         [ 0.0856, 10.9421,  0.0550,  ..., -1.5903, -0.2297, -1.8296],
         [ 0.1427, 13.1847,  0.1794,  ..., -1.6471, -0.2102, -2.2455]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(10.4921, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(56.0823, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)
An Easter Easter Island sponsorship giant targeted British Culture has targeted a British Easter Island .
iter = 1 reward = 2
final_logits =  tensor([[[ 2.1390e-01,  3.5412e+00,  1.2239e+00,  ..., -2.8312e+00,
           1.0831e+00, -6.1762e-01],
         [ 1.0454e-01,  4.5989e+00,  9.1182e-01,  ..., -3.5251e+00,
           1.8343e+00, -1.5796e+00],
         [ 4.0419e-02,  3.4002e+00,  2.3212e-01,  ..., -2.6851e+00,
           2.7972e-01, -2.0191e+00],
         ...,
         [ 6.9924e-02,  4.8928e+00, -4.2567e-01,  ..., -2.8088e+00,
          -1.1909e-02, -3.9834e+00],
         [ 7.6657e-02,  9.5664e+00,  7.3078e-02,  ..., -1.5712e+00,
          -1.0200e+00, -1.2357e+00],
         [ 1.3911e-01,  1.3194e+01,  2.3760e-01,  ..., -2.0111e+00,
          -5.4598e-01, -1.8279e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(8.1574, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(51.7283, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)
An Easter Island giant targeted a British Culture sponsorship giant has targeted a British Easter Island .
iter = 2 reward = 2
final_logits =  tensor([[[ 0.2003,  3.4347,  1.2031,  ..., -3.0633,  0.9931, -0.6571],
         [ 0.0944,  4.5414,  0.9007,  ..., -3.7329,  1.7259, -1.6498],
         [ 0.0415,  3.4680,  0.2309,  ..., -2.4209,  0.1945, -1.6468],
         ...,
         [ 0.0555,  4.9248, -0.4067,  ..., -2.6957, -0.5324, -3.8464],
         [ 0.0654, 10.0364,  0.0675,  ..., -1.2133, -1.3830, -0.9155],
         [ 0.1311, 13.2382,  0.2638,  ..., -2.0116, -0.7900, -1.7015]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.6443, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(26.0947, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)
An Easter Island sponsorship giant has targeted a British Culture giant .
iter = 3 reward = 1
final_logits =  tensor([[[ 0.1925,  3.3923,  1.1703,  ..., -3.5966,  0.8471, -1.0067],
         [ 0.0881,  4.5260,  0.9393,  ..., -3.8752,  1.6559, -1.7488],
         [ 0.0381,  3.5903,  0.1952,  ..., -2.3709, -0.1211, -1.3613],
         ...,
         [ 0.0356,  4.8587, -0.2023,  ..., -1.2638, -0.4414, -2.2239],
         [ 0.0616, 10.4511,  0.0433,  ..., -1.4971, -0.8337, -1.7510],
         [ 0.1274, 13.3397,  0.2159,  ..., -2.2888, -0.6840, -2.0081]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.3634, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(4.7761, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)
An Easter Island sponsorship giant has targeted a British Culture giant .
iter = 4 reward = 1
final_logits =  tensor([[[ 0.1903,  3.4289,  1.1979,  ..., -3.9352,  0.6599, -1.2398],
         [ 0.0854,  4.4122,  1.0050,  ..., -3.5977,  1.6044, -1.8033],
         [ 0.0374,  3.7110,  0.2044,  ..., -2.4552, -0.2896, -1.2355],
         ...,
         [ 0.0319,  4.4901, -0.1984,  ..., -1.3982, -0.3156, -2.3668],
         [ 0.0599,  9.2967,  0.0758,  ..., -1.7848, -0.7020, -1.8599],
         [ 0.1312, 12.8267,  0.2701,  ..., -2.4941, -0.7215, -2.1065]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-2.1432, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(11.3028, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  1
1
{'input_ids': tensor([[[ 4123,   113,   219,  6234,   195,  6908,   115,   109, 75158,  1153,
           3893,  1419, 16036,   148,  6305,  3906,   142, 10580,   805,   113,
            203,  7175,   113,  3064,   762, 14567,  1407,  1104,   124,   203,
            387,   110,   107,   139,  1082,   110,   108,  1155,   204,   109,
           1339,   110,   108,   939,  1841,   115,   683,   113,   114,  1679,
            113,   461,  6234, 10361,  1055,   113,   203,  3954,   210,   124,
            109,  1917,  1030,   110,   107, 16036,  9619,  3582,  7217,   243,
            120,   339,  6234,   195,  6908,   115,   109,   856,  1153,   111,
          10390,   680,   196,   174,   263,   112,   535,  1055,   110,   107,
            139, 10580,   805,   140,  3530,   122,   109,   856,   244,   114,
            787,  9024,  8666,   126,   110,   107,  1263,  7217,   243,   109,
           4787,   170,   635,   109,  1153,   140, 10361,   169,   766,   122,
          10390,   680,   111,   186,   140,   220,  5313,  6596,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)
Three blank blank blank photo blank photo blank blank photo shows blank picture of blank blank picture .
iter = 0 reward = 0
final_logits =  tensor([[[-1.3147e-02,  5.0933e+00,  1.6983e-01,  ..., -1.8187e+00,
          -2.6461e+00,  6.4724e-01],
         [-7.0758e-02,  3.4786e+00, -2.7868e-01,  ..., -2.1640e+00,
          -1.7409e+00, -5.8967e-01],
         [-7.6869e-02,  3.1405e+00, -2.6331e-02,  ..., -2.0923e+00,
          -2.3997e-01, -1.2565e+00],
         ...,
         [-8.3996e-02,  7.2636e+00, -9.1425e-01,  ..., -3.2818e+00,
          -2.2679e+00, -7.7094e-01],
         [-9.5673e-03,  1.0177e+01, -7.8569e-02,  ..., -2.4086e+00,
          -1.3457e+00,  1.5834e-01],
         [-5.2960e-03,  1.3550e+01, -1.7304e-01,  ..., -1.5703e+00,
          -1.4496e+00, -4.1504e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-20.2059, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(292.3608, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)
Gulf oil spill over weekend three big photo over the weekend, Dean Dean spotted .
iter = 1 reward = 3
final_logits =  tensor([[[-1.7702e-02,  5.6326e+00,  4.1939e-01,  ..., -3.4801e+00,
          -8.3752e-01,  9.1396e-01],
         [-4.2737e-02,  2.0239e+00,  1.3420e-01,  ..., -2.9771e+00,
          -2.0954e+00, -1.1835e+00],
         [-1.1902e-01,  1.8342e+00,  1.3931e-01,  ..., -4.1534e+00,
          -1.6621e+00, -1.6014e+00],
         ...,
         [-7.2988e-02,  6.6523e+00, -1.6513e-01,  ..., -3.2247e+00,
          -1.9526e+00, -1.2839e+00],
         [ 3.1130e-03,  7.8123e+00,  1.7611e-01,  ..., -2.0522e+00,
          -3.8053e-01,  5.2651e-01],
         [ 1.1332e-02,  1.3504e+01, -2.0889e-01,  ..., -1.9850e+00,
          -1.2209e+00,  6.7135e-02]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-10.1438, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(82.6370, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)
Dean Dean Dean has no photo of the Gulf spill altered .
iter = 2 reward = 2
final_logits =  tensor([[[ 1.1938e-02,  4.6718e+00,  2.3570e-01,  ..., -4.0392e+00,
          -2.2554e+00,  2.4322e-01],
         [ 2.9117e-02,  2.6101e+00, -4.7658e-03,  ..., -4.0198e+00,
          -1.7601e+00, -9.3650e-01],
         [ 1.4000e-02,  1.9024e+00, -3.3356e-01,  ..., -3.9693e+00,
          -1.8854e+00, -1.5438e+00],
         ...,
         [-5.6036e-02,  4.4528e+00, -5.8003e-01,  ..., -1.6473e+00,
          -1.0339e+00, -1.0903e+00],
         [ 2.9383e-02,  4.5467e+00,  4.1167e-02,  ..., -6.8037e-01,
          -5.5712e-01, -2.5805e-01],
         [ 1.4280e-03,  1.2701e+01, -2.7915e-01,  ..., -2.5732e+00,
          -2.1233e+00, -8.9812e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.7544, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(24.7915, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)
Dean Dean Dean of the Gulf spill has altered big photo .
iter = 3 reward = 2
final_logits =  tensor([[[ 0.0290,  4.2174,  0.3227,  ..., -3.7044, -2.1087,  0.4013],
         [ 0.0449,  2.9574,  0.1196,  ..., -3.8140, -1.8115, -0.8850],
         [ 0.0322,  2.2427, -0.1576,  ..., -4.4792, -2.0324, -1.6008],
         ...,
         [-0.0256,  6.0850, -0.4649,  ..., -1.9390, -1.7473, -2.3677],
         [ 0.0246,  6.9212,  0.1627,  ..., -1.8418, -0.7838, -0.7115],
         [ 0.0149, 12.6345, -0.1839,  ..., -2.6427, -2.3770, -0.6465]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.6682, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(4.6873, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)
Dean Dean Dean of the Gulf of Mexico has altered picture picture .
iter = 4 reward = 1
final_logits =  tensor([[[ 0.0428,  4.4229,  0.5092,  ..., -3.3342, -1.2538,  0.6652],
         [ 0.0448,  2.3453,  0.1822,  ..., -3.6131, -2.1040, -0.6898],
         [ 0.0342,  1.6078, -0.0781,  ..., -4.3699, -2.2976, -1.3316],
         ...,
         [-0.0267,  5.4230, -0.4632,  ..., -1.1628, -2.2916, -1.6376],
         [ 0.0311,  6.9589,  0.0785,  ..., -1.6259, -0.7748, -0.4500],
         [ 0.0248, 12.7031, -0.1193,  ..., -2.2322, -1.7592, -0.3599]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(8.4338, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(33.4078, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  1
1
{'input_ids': tensor([[[599, 960, 110,  ..., 109, 804,   1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 38.4% (4226 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)
BP's new plan to stop the . well on its . cap .
iter = 0 reward = 2
final_logits =  tensor([[[ 0.0225,  1.5661,  0.6957,  ..., -4.0769,  1.4278, -0.3910],
         [ 0.0483,  1.9633, -0.4710,  ..., -3.7358, -1.9531, -3.1397],
         [ 0.0142,  2.6378,  0.0523,  ..., -3.4444, -0.1539, -0.4183],
         ...,
         [-0.0561,  4.4529, -0.8069,  ..., -1.1370, -1.5190, -1.9458],
         [ 0.0463,  7.9109, -0.0631,  ..., -1.1629, -0.6000, -0.7405],
         [ 0.0153, 12.0791, -0.7505,  ..., -1.3737, -0.9471, -0.9444]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(13.3708, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(126.0555, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4268 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)
BP has stopped on its new cap on its cap .
iter = 1 reward = 4
final_logits =  tensor([[[ 1.6514e-02,  1.7031e+00,  7.9629e-01,  ..., -4.1078e+00,
           1.0382e+00, -2.9768e-01],
         [ 3.2404e-02,  2.2486e+00, -4.6557e-01,  ..., -2.7070e+00,
          -1.9012e+00, -3.0523e+00],
         [ 5.7963e-02,  1.5912e+00,  2.1041e-01,  ..., -1.5246e+00,
          -1.3336e+00, -1.0598e+00],
         ...,
         [-7.6593e-02,  3.1387e+00, -7.2611e-01,  ..., -1.1549e+00,
          -1.5255e+00, -1.0797e+00],
         [ 1.3972e-02,  7.0365e+00, -1.8526e-01,  ..., -1.0367e+00,
          -1.0524e+00, -9.1311e-01],
         [ 8.5436e-03,  1.2107e+01, -4.4287e-01,  ..., -2.2354e+00,
          -4.9977e-01, -3.6691e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(13.9806, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(117.5906, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4266 out of 11019)

Everytime after generating next logits 39.9% (4394 out of 11019)

Everytime after generating next logits 39.9% (4394 out of 11019)

Everytime after generating next logits 39.9% (4394 out of 11019)

Everytime after generating next logits 39.9% (4394 out of 11019)

Everytime after generating next logits 39.9% (4394 out of 11019)

Everytime after generating next logits 39.9% (4394 out of 11019)

Everytime after generating next logits 39.9% (4394 out of 11019)

Everytime after generating next logits 39.9% (4394 out of 11019)

Everytime after generating next logits 39.9% (4394 out of 11019)

Everytime after generating next logits 39.9% (4394 out of 11019)

Everytime after generating next logits 39.9% (4394 out of 11019)
BP has stopped on cap on its cap .
iter = 2 reward = 3
final_logits =  tensor([[[ 7.7542e-03,  1.4556e+00,  8.0038e-01,  ..., -4.7767e+00,
           8.9447e-01, -2.2339e-01],
         [ 2.3596e-02,  2.5642e+00, -4.2455e-01,  ..., -2.8527e+00,
          -2.1425e+00, -2.2705e+00],
         [ 5.1549e-02,  1.7080e+00,  2.4583e-01,  ..., -1.7198e+00,
          -1.5620e+00, -4.6984e-01],
         ...,
         [-9.4265e-02,  2.6593e+00, -7.3490e-01,  ..., -7.5259e-01,
          -1.5409e+00, -1.5598e+00],
         [ 9.5284e-03,  5.7210e+00, -4.7546e-02,  ..., -8.6663e-01,
          -9.5762e-01, -6.7161e-01],
         [-1.1139e-02,  1.1237e+01, -5.6952e-01,  ..., -2.3690e+00,
          -2.6411e-01, -6.9488e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(7.0825, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(55.4214, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4262 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)
BP stopped on cap on its cap on 20 hours before it was on its first stop on the cap on its .
iter = 3 reward = 5
final_logits =  tensor([[[-2.9919e-03,  1.0445e+00,  8.2749e-01,  ..., -4.9748e+00,
           1.3482e+00, -2.6764e-01],
         [ 1.4595e-02,  2.7811e+00, -4.1058e-01,  ..., -2.5734e+00,
          -2.4312e+00, -1.8074e+00],
         [-8.0952e-02,  1.9326e+00, -3.2738e-01,  ..., -1.2781e+00,
           4.9007e-01,  2.3750e-01],
         ...,
         [-3.9404e-02,  3.9959e+00,  1.2482e-01,  ..., -2.5678e+00,
           4.7635e-01,  8.5810e-01],
         [ 2.6571e-03,  8.3023e+00,  2.7920e-01,  ..., -1.1188e+00,
          -2.3289e-01, -5.5524e-01],
         [-5.4847e-02,  1.2116e+01, -5.5162e-01,  ..., -3.1339e+00,
          -1.4241e+00, -2.0554e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.6866, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(23.8170, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.8% (4280 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)

Everytime after generating next logits 40.0% (4408 out of 11019)
BP stopped on cap on its cap on its way to start .
iter = 4 reward = 4
final_logits =  tensor([[[-1.0786e-02,  5.1381e-01,  8.2538e-01,  ..., -4.1541e+00,
           2.0153e+00, -6.1387e-01],
         [ 1.3351e-03,  2.3462e+00, -4.8962e-01,  ..., -2.7864e+00,
          -2.0832e+00, -2.1523e+00],
         [-9.0056e-02,  2.1544e+00, -4.0260e-01,  ..., -1.4984e+00,
          -3.3953e-01, -2.1390e-01],
         ...,
         [-8.0353e-02,  3.0232e+00, -4.0564e-01,  ..., -1.4004e+00,
          -1.0502e+00, -2.0631e+00],
         [ 3.8014e-02,  5.8046e+00,  2.1766e-02,  ..., -4.7484e-01,
          -4.3969e-02, -3.4445e-01],
         [-1.3915e-02,  1.2602e+01, -4.4317e-01,  ..., -2.2483e+00,
           3.4834e-01, -1.3285e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.0471, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(9.5139, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  4
4
{'input_ids': tensor([[[6174,  692,  110,  ...,  526,  115,    1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 38.7% (4268 out of 11019)

Everytime after generating next logits 39.3% (4332 out of 11019)

Everytime after generating next logits 39.3% (4332 out of 11019)

Everytime after generating next logits 39.3% (4332 out of 11019)

Everytime after generating next logits 39.3% (4332 out of 11019)

Everytime after generating next logits 39.3% (4332 out of 11019)

Everytime after generating next logits 39.3% (4332 out of 11019)

Everytime after generating next logits 39.3% (4332 out of 11019)

Everytime after generating next logits 39.3% (4332 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

iter = 0 reward = 0
final_logits =  tensor([[[ 2.0104e-02,  6.3808e+00,  6.7454e-01,  ..., -6.4236e+00,
          -7.3613e-01,  3.3154e-01],
         [-2.5412e-04,  4.4276e+00,  1.1189e+00,  ..., -7.1269e+00,
          -7.4567e-02,  1.0122e+00],
         [-4.3229e-03,  4.2897e+00,  1.0408e+00,  ..., -6.9010e+00,
          -1.8666e-01,  8.3681e-01],
         ...,
         [ 2.0505e-02,  1.0616e+01,  2.5422e-01,  ..., -4.2836e+00,
           2.8605e-01, -7.3467e-01],
         [ 2.1747e-02,  1.0771e+01,  2.4494e-01,  ..., -4.2431e+00,
           3.0248e-01, -7.6049e-01],
         [ 2.2694e-02,  1.0986e+01,  2.3441e-01,  ..., -4.2000e+00,
           2.9844e-01, -7.7246e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-14.7376, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(54.9335, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.2% (4536 out of 11019)

Everytime after generating next logits 42.3% (4664 out of 11019)

Everytime after generating next logits 42.3% (4664 out of 11019)

Everytime after generating next logits 42.3% (4664 out of 11019)

Everytime after generating next logits 42.3% (4664 out of 11019)

Everytime after generating next logits 42.3% (4664 out of 11019)

Everytime after generating next logits 42.3% (4664 out of 11019)

Everytime after generating next logits 42.3% (4664 out of 11019)

Everytime after generating next logits 42.3% (4664 out of 11019)

Everytime after generating next logits 42.3% (4664 out of 11019)

Everytime after generating next logits 42.3% (4664 out of 11019)

Everytime after generating next logits 42.3% (4664 out of 11019)
Deepwater Horizon has been confirmed an oil spill .
iter = 1 reward = 4
final_logits =  tensor([[[ 1.8391e-02,  2.1522e+00,  7.7235e-01,  ..., -7.8236e+00,
          -1.3019e+00,  2.1853e+00],
         [-2.0551e-02,  1.5842e+00,  2.6939e-01,  ..., -5.1376e+00,
           2.0410e-01, -1.1693e+00],
         [-2.1153e-02, -1.5011e-01, -3.9405e-01,  ..., -6.9977e+00,
          -2.7287e+00, -2.2943e+00],
         ...,
         [-1.1428e-01,  2.0064e+00, -6.1813e-01,  ..., -8.8338e-01,
          -4.2578e-01, -3.3764e+00],
         [ 2.1329e-03,  5.4516e+00,  4.4255e-01,  ..., -5.3352e+00,
          -8.7591e-01,  4.3897e-01],
         [-2.2924e-02,  1.2087e+01, -4.3560e-01,  ..., -3.6521e+00,
          -1.2719e+00, -1.0105e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-6.1490, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(30.4258, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4234 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)
Gulf Mexico spill has been confirmed an oil spill .
iter = 2 reward = 4
final_logits =  tensor([[[ 2.2668e-02,  2.1851e+00,  6.4710e-01,  ..., -7.6505e+00,
          -1.3398e+00,  5.0546e-01],
         [ 1.3724e-03,  1.1903e+00,  1.4206e-02,  ..., -4.9970e+00,
          -1.8286e+00,  2.1377e-01],
         [-1.7634e-03, -4.2913e-02, -7.2031e-02,  ..., -7.0718e+00,
          -1.5604e+00, -1.4892e+00],
         ...,
         [-1.0774e-01,  2.9465e+00, -5.5148e-01,  ..., -1.0659e+00,
          -9.1847e-01, -3.3571e+00],
         [ 1.5396e-02,  5.0707e+00,  3.8154e-01,  ..., -3.6994e+00,
          -6.4744e-01,  2.4692e-01],
         [-8.5484e-03,  1.1568e+01, -3.5092e-01,  ..., -4.1914e+00,
          -1.4076e+00, -1.5037e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-3.3543, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(12.8812, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4228 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)
Gulf spill has been confirmed more than ever before .
iter = 3 reward = 2
final_logits =  tensor([[[ 4.7329e-02,  2.2986e+00,  8.2408e-01,  ..., -7.4885e+00,
          -7.0116e-01,  7.6747e-01],
         [ 1.0417e-02,  1.7145e+00,  1.0862e-01,  ..., -5.3084e+00,
          -1.6392e+00,  6.4184e-01],
         [-6.8960e-02,  6.2052e-01, -4.1170e-01,  ..., -3.7830e+00,
          -1.4771e+00, -1.7829e+00],
         ...,
         [-4.5110e-02,  3.9415e+00, -6.2566e-01,  ..., -3.7014e+00,
          -1.9500e+00, -2.1156e+00],
         [ 4.4114e-02,  7.4090e+00,  5.0223e-01,  ..., -5.7825e+00,
          -7.1560e-01,  1.3244e+00],
         [ 1.9972e-02,  1.2055e+01, -5.5977e-02,  ..., -4.4077e+00,
          -2.2563e-01, -5.1063e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-5.6881, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(16.4233, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4248 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)
Gulf spiller has finally been confirmed .
iter = 4 reward = 2
final_logits =  tensor([[[ 5.2106e-02,  1.5234e+00,  7.8757e-01,  ..., -7.3355e+00,
          -1.0309e+00,  7.2773e-01],
         [ 9.3008e-03,  1.1130e+00,  6.3286e-02,  ..., -5.2858e+00,
          -1.7851e+00,  7.8381e-01],
         [-5.8925e-02,  1.1841e-01, -4.4289e-01,  ..., -3.7948e+00,
          -1.4589e+00, -1.4766e+00],
         ...,
         [-5.1361e-02,  1.7845e+00, -4.8486e-01,  ..., -3.6845e+00,
          -1.6604e+00, -7.3045e-01],
         [ 6.4209e-02,  5.4949e+00,  6.1958e-01,  ..., -4.3130e+00,
          -7.4034e-01,  7.8424e-01],
         [ 8.4023e-03,  9.6132e+00, -3.5460e-01,  ..., -4.8635e+00,
          -1.8909e+00, -1.0746e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-6.4683, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(9.5408, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  2
2
{'input_ids': tensor([[[  139,  5543,  9254,   320,  3702,   142,   865,   728,   113,   109,
          16036,   762, 14567,   110,   107,  8916,   114,  1713,   113,  5012,
            122,   662,   503, 32908,   110,   108,   330,   278,  2668,   116,
            122,  4605, 35522,   110,   108,   109,   177,  3378,   113, 16036,
            111,  6061, 32886,   110,   108,   169, 15978,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1]]])}

Before Sampling 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)

Everytime after generating next logits 38.1% (4196 out of 11019)
Money Head Head of oil spill interviews inside oil spill insiders .
iter = 0 reward = 5
final_logits =  tensor([[[ 1.3441e-01,  2.4039e+00,  6.7088e-01,  ..., -3.7794e+00,
           2.0710e+00,  8.3255e-01],
         [ 8.7195e-02,  2.2889e+00,  5.5650e-02,  ..., -6.4792e+00,
          -9.8662e-01, -1.4671e+00],
         [ 8.3345e-02,  1.5836e+00,  1.1867e-01,  ..., -7.0739e+00,
          -1.4879e+00, -2.6998e-01],
         ...,
         [ 3.5250e-03,  4.8687e+00, -5.3428e-01,  ..., -4.6433e+00,
          -2.4191e+00, -1.0315e+00],
         [ 8.2999e-02,  6.1961e+00,  1.2724e-01,  ..., -2.3942e+00,
          -2.1320e+00,  8.6785e-01],
         [ 8.0352e-02,  1.3035e+01, -8.9831e-02,  ..., -2.4890e+00,
          -5.9236e-01, -9.7088e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.9633, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(17.0867, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.0% (4182 out of 11019)

Everytime after generating next logits 38.0% (4182 out of 11019)

Everytime after generating next logits 38.0% (4182 out of 11019)

Everytime after generating next logits 38.0% (4182 out of 11019)

Everytime after generating next logits 38.0% (4182 out of 11019)

Everytime after generating next logits 38.0% (4182 out of 11019)

Everytime after generating next logits 38.0% (4182 out of 11019)

Everytime after generating next logits 38.0% (4182 out of 11019)

Everytime after generating next logits 38.0% (4182 out of 11019)

Everytime after generating next logits 38.0% (4182 out of 11019)

Everytime after generating next logits 38.0% (4182 out of 11019)

Everytime after generating next logits 38.0% (4182 out of 11019)

Everytime after generating next logits 38.0% (4182 out of 11019)

Everytime after generating next logits 38.1% (4202 out of 11019)

Everytime after generating next logits 38.1% (4202 out of 11019)

Everytime after generating next logits 38.1% (4202 out of 11019)

Everytime after generating next logits 38.1% (4202 out of 11019)

Everytime after generating next logits 38.1% (4202 out of 11019)

Everytime after generating next logits 38.1% (4202 out of 11019)

Everytime after generating next logits 38.1% (4202 out of 11019)

Everytime after generating next logits 38.1% (4202 out of 11019)
Money Head Head of the head of the oil spill interviews oil insiders inside oil spill insiders .
iter = 1 reward = 6
final_logits =  tensor([[[ 1.2856e-01,  5.0128e+00,  6.0232e-01,  ..., -3.3165e+00,
           8.6140e-01, -4.4377e-01],
         [ 5.6935e-02,  3.8005e+00,  7.5066e-02,  ..., -6.3775e+00,
          -3.2484e-01, -1.4480e+00],
         [ 8.9790e-02,  4.1684e+00,  6.3475e-02,  ..., -7.7082e+00,
          -1.3883e+00, -9.9426e-01],
         ...,
         [ 1.5937e-02,  6.2596e+00, -4.9709e-01,  ..., -3.2520e+00,
          -1.6474e+00, -1.6751e+00],
         [ 1.0875e-01,  7.6550e+00,  5.6489e-02,  ..., -1.9626e+00,
          -1.2642e+00,  3.0355e-01],
         [ 9.7427e-02,  1.2623e+01,  1.3769e-01,  ..., -1.8498e+00,
          -6.7012e-03, -8.2255e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.3979, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(29.8610, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)
Oil spill insiders reveal oil spill insiders inside oil spill inside oil spill .
iter = 2 reward = 11
final_logits =  tensor([[[ 1.0962e-01,  3.8074e+00,  5.9523e-01,  ..., -3.1372e+00,
           1.9042e+00, -9.3200e-01],
         [-4.1529e-02,  2.3773e+00,  2.2116e-01,  ..., -3.6567e+00,
           1.5845e+00, -1.0257e+00],
         [-5.6366e-02,  1.1897e+00, -1.9616e-01,  ..., -5.0261e+00,
          -2.5845e-01, -1.0068e+00],
         ...,
         [-8.0322e-02,  3.8521e+00, -5.1661e-01,  ..., -6.0284e+00,
          -7.6788e-01, -2.0438e+00],
         [ 7.8427e-02,  5.8346e+00,  2.9157e-01,  ..., -3.2080e+00,
          -1.3800e+00, -1.0669e-01],
         [ 7.9798e-02,  1.1965e+01, -3.4843e-04,  ..., -2.4428e+00,
           5.3306e-01, -1.3905e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(7.1265, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(76.8531, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)

Everytime after generating next logits 38.3% (4222 out of 11019)
Money Head Bob Dudley interviews oil spill insiders inside oil spill insiders inside oil spill inside oil spill .
iter = 3 reward = 11
final_logits =  tensor([[[ 0.1227,  3.8725,  0.5821,  ..., -3.2717,  2.1030, -1.0895],
         [ 0.0566,  3.1418,  0.0267,  ..., -5.6211, -0.3697, -1.5762],
         [ 0.0882,  3.0210, -0.0561,  ..., -5.8315, -2.1228, -2.1167],
         ...,
         [-0.0458,  5.0112, -0.3626,  ..., -5.6648, -0.0842, -1.3427],
         [ 0.0977,  5.0462,  0.1237,  ..., -1.8818, -0.8925,  0.6138],
         [ 0.0919, 12.0811,  0.1611,  ..., -2.1216,  0.6040, -1.0331]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.8136, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(35.0846, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)
Money Head Bob Dudley interviews inside oil spill insiders inside oil spill inside oil spill .
iter = 4 reward = 9
final_logits =  tensor([[[ 0.1133,  2.1747,  0.7054,  ..., -2.9309,  2.4680, -0.3703],
         [ 0.0511,  2.4905, -0.0205,  ..., -5.7360, -0.2603, -1.9357],
         [ 0.0769,  1.4568, -0.0486,  ..., -6.0395, -1.6251, -1.8260],
         ...,
         [-0.0576,  4.0120, -0.4009,  ..., -6.3739, -0.1142, -1.7188],
         [ 0.0900,  5.3342,  0.2699,  ..., -2.6971, -1.2878,  0.7444],
         [ 0.0751, 12.2946,  0.0265,  ..., -2.2571, -0.1227, -1.2410]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.3007, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(11.9510, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  9
9
{'input_ids': tensor([[[ 1084, 18756,   110,  ...,  5135,   378,     1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 38.4% (4228 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 39.5% (4356 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)
Box is a box called a 'box' that may never be put off .
iter = 0 reward = 3
final_logits =  tensor([[[ 0.0404,  2.8886,  0.8338,  ..., -5.1437,  0.0325, -0.1087],
         [-0.0332,  1.0775, -0.2255,  ..., -3.3496, -3.2087, -1.3216],
         [-0.0261,  0.3861, -0.5584,  ..., -1.5565, -0.4436, -0.5393],
         ...,
         [ 0.0130,  4.9698, -0.8354,  ..., -1.3736, -0.1778, -1.1218],
         [ 0.0801,  3.8973,  0.0740,  ..., -1.1405,  0.1259,  0.0685],
         [ 0.0508, 12.6626, -0.3998,  ..., -3.4497, -0.3579,  0.3620]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-19.1507, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(3602.6873, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4248 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)
Box could be used to help BP to report environmental problems .
iter = 1 reward = 6
final_logits =  tensor([[[ 3.9408e-02,  2.8237e+00,  7.8633e-01,  ..., -4.5595e+00,
           1.2569e+00, -7.7361e-01],
         [-2.5018e-02,  6.8781e-01, -5.1280e-01,  ..., -3.0449e+00,
          -2.3493e+00, -2.1849e+00],
         [ 1.4654e-02, -6.6425e-02, -3.1113e-01,  ..., -1.1417e+00,
           5.4802e-01,  1.6216e-01],
         ...,
         [-5.8531e-02,  5.7264e+00, -7.7442e-01,  ..., -2.3305e+00,
           1.3561e+00, -1.9587e+00],
         [ 7.7678e-02,  7.5111e+00,  1.1941e-02,  ..., -1.4159e+00,
           2.9389e-01, -2.4920e-01],
         [ 5.0500e-02,  1.2872e+01, -3.0986e-01,  ..., -2.1435e+00,
           3.4609e-01, -8.1558e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-10.3812, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(58.9684, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4244 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 40.3% (4436 out of 11019)

Everytime after generating next logits 40.3% (4436 out of 11019)

Everytime after generating next logits 40.3% (4436 out of 11019)

Everytime after generating next logits 40.3% (4436 out of 11019)

Everytime after generating next logits 40.3% (4436 out of 11019)

Everytime after generating next logits 40.3% (4436 out of 11019)

Everytime after generating next logits 40.3% (4436 out of 11019)

Everytime after generating next logits 40.3% (4436 out of 11019)

Everytime after generating next logits 40.3% (4436 out of 11019)

Everytime after generating next logits 40.3% (4436 out of 11019)

Everytime after generating next logits 40.3% (4436 out of 11019)
BP says box is a ''box' that is a built in a year, and is a 'box' that is a  that is a     .
iter = 2 reward = 3
final_logits =  tensor([[[ 9.2472e-02,  4.3620e+00,  7.5186e-01,  ..., -3.5014e+00,
          -5.3745e-01, -1.7973e+00],
         [ 1.0603e-01,  2.6076e+00, -3.4503e-03,  ..., -3.8028e+00,
          -1.6323e+00, -2.5976e+00],
         [ 5.5106e-02,  2.4932e+00, -1.6499e-01,  ..., -4.6768e+00,
           1.0716e+00, -1.3660e+00],
         ...,
         [ 1.1498e-01,  8.3507e+00, -2.3641e-01,  ..., -4.0204e+00,
          -8.9840e-01, -1.5465e+00],
         [ 1.6411e-01,  9.8753e+00,  3.4496e-01,  ..., -3.4434e+00,
           7.4544e-01, -1.2151e+00],
         [ 1.0039e-01,  1.0132e+01, -3.5309e-01,  ..., -2.9171e+00,
          -1.5488e-01, -1.4305e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.0781, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(76.0348, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.1% (4306 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.2% (4434 out of 11019)

Everytime after generating next logits 40.8% (4498 out of 11019)
Company's top oil spill spill-topped Gulf Gulf coast officials missed a bid to contain over 5,000 barrels of oil spill spill spill-topped oil .
iter = 3 reward = 13
final_logits =  tensor([[[ 0.1006,  4.0967,  0.6880,  ..., -3.4702, -0.6808, -1.5109],
         [ 0.0970,  2.9772,  0.0421,  ..., -6.4513, -0.9797, -1.9196],
         [ 0.0717,  3.8673,  0.3204,  ..., -5.2878, -0.8133, -1.2782],
         ...,
         [-0.0305,  3.6983,  0.2453,  ..., -2.9554, -1.4372, -1.1856],
         [ 0.1368,  6.1404,  1.0937,  ..., -4.3772, -0.2586, -0.0326],
         [ 0.1229, 11.6721,  0.3152,  ..., -2.5514, -0.0262, -1.0877]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(5.9314, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(33.9641, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.3% (4332 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)
BP chief confirms no oil spill-term containment plan has been launched .
iter = 4 reward = 4
final_logits =  tensor([[[ 0.0455,  2.4242,  0.6568,  ..., -4.7182,  0.6161, -0.4986],
         [ 0.0389,  2.2620, -0.2830,  ..., -4.8344, -1.3625, -2.2249],
         [ 0.0741,  2.4357, -0.1917,  ..., -4.8762, -1.3837, -1.4302],
         ...,
         [-0.0775,  3.9399, -0.6818,  ..., -1.4234,  0.4802, -1.5625],
         [ 0.0699,  4.7755,  0.1050,  ..., -1.0915, -0.1772, -0.4687],
         [ 0.0549, 12.6513, -0.2457,  ..., -2.8912,  0.2027, -1.4295]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-2.2593, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(28.0776, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  4
4
{'input_ids': tensor([[[16036,   243,   186,  ...,   111,  2684,     1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 38.6% (4254 out of 11019)

Everytime after generating next logits 39.2% (4318 out of 11019)

Everytime after generating next logits 39.2% (4318 out of 11019)

Everytime after generating next logits 39.2% (4318 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)
Gulf spill giant has risen by 1.7 per cent since last month .
iter = 0 reward = 2
final_logits =  tensor([[[ 1.1372e-01,  4.3840e+00,  5.6355e-01,  ..., -2.7584e+00,
           1.6424e+00, -5.0072e-01],
         [ 8.3284e-02,  1.7141e+00,  2.1678e-01,  ..., -3.0594e+00,
          -1.9440e-01,  6.2522e-01],
         [-4.7883e-02, -2.7892e-01,  8.6308e-03,  ..., -2.3138e+00,
          -4.7900e-01, -6.1675e-01],
         ...,
         [ 3.8913e-02,  4.6422e+00, -4.2435e-01,  ..., -1.9168e+00,
          -4.3495e-01, -6.2992e-01],
         [ 8.8670e-02,  3.8258e+00,  8.7542e-02,  ..., -1.1831e+00,
          -5.8589e-03,  4.7843e-01],
         [ 8.7766e-02,  1.2683e+01, -2.0316e-01,  ..., -1.2509e+00,
           3.4264e-01, -1.1313e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.4669, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(4.2104, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4248 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)
Gulf spill giant has risen by 1.7% of shares of company shares .
iter = 1 reward = 3
final_logits =  tensor([[[ 1.1878e-01,  5.0299e+00,  4.8576e-01,  ..., -2.6759e+00,
           7.9107e-01, -1.4438e+00],
         [ 9.9299e-02,  1.8658e+00,  1.4203e-01,  ..., -3.0276e+00,
          -9.3578e-02,  3.6998e-01],
         [-1.8553e-02, -2.1563e-01,  3.4052e-02,  ..., -2.7598e+00,
          -1.8944e-01, -7.7794e-01],
         ...,
         [-1.2224e-02,  6.0978e+00, -5.6977e-01,  ..., -1.6492e+00,
          -6.2875e-01, -7.5236e-01],
         [ 8.7207e-02,  4.9844e+00,  1.6681e-01,  ..., -2.0763e+00,
           7.9322e-02,  7.8504e-01],
         [ 8.4199e-02,  1.2926e+01, -1.3235e-01,  ..., -1.2953e+00,
           7.4947e-02, -5.9443e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.3926, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.3796, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4250 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)
dividend dividend is a record of .
iter = 2 reward = 2
final_logits =  tensor([[[ 1.1424e-01,  5.8038e+00,  8.3566e-01,  ...,  1.1139e-01,
           1.1648e+00, -2.2120e+00],
         [-1.3999e-02,  3.0457e+00, -1.1302e-02,  ..., -2.1598e+00,
           7.2386e-02, -1.5110e+00],
         [-6.8669e-03,  3.0566e+00, -1.1155e-01,  ..., -1.7285e+00,
          -8.1029e-02, -1.5516e+00],
         ...,
         [ 5.0006e-02,  3.8271e+00, -5.5922e-02,  ..., -4.3341e-01,
           8.2311e-01, -1.8407e+00],
         [ 1.1183e-01,  6.8544e+00,  1.3631e+00,  ..., -1.2257e+00,
           2.2698e+00,  4.7573e-01],
         [ 9.2149e-02,  9.8748e+00,  4.5210e-01,  ...,  4.2962e-01,
           2.4516e+00, -6.0008e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(4.8869, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(7.9848, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4234 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)
Gulf Horizon Horizon CEO has paid Gulf spill spill spill spill-related dividend to record low of 1.7 .
iter = 3 reward = 8
final_logits =  tensor([[[ 0.1289,  6.4971,  0.7868,  ..., -1.9611,  0.5955, -0.4379],
         [ 0.1073,  3.2779,  0.3776,  ..., -3.1671, -0.6351,  1.2777],
         [ 0.0971,  2.0140,  0.2017,  ..., -3.8834, -1.6173, -2.1586],
         ...,
         [ 0.0638,  7.3580,  0.5232,  ..., -1.7502, -1.1970, -0.4729],
         [ 0.1340,  7.8997,  1.7025,  ..., -1.4676,  0.5302,  2.0860],
         [ 0.1331, 12.8133,  0.4193,  ..., -1.0799,  1.0669,  0.7306]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(15.3379, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(66.6738, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4254 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)
Gulf oil company has fallen by 1.7% since June .
iter = 4 reward = 5
final_logits =  tensor([[[ 0.0934,  4.0290,  0.5346,  ..., -2.2478,  1.4235, -1.5027],
         [ 0.0727,  1.7934,  0.1443,  ..., -2.8249, -0.3717,  0.8894],
         [-0.0243,  1.2420,  0.2200,  ..., -3.4973,  0.4040,  0.2246],
         ...,
         [ 0.0419,  3.0034, -0.1696,  ..., -1.2283, -0.8082, -1.4578],
         [ 0.1081,  5.2537,  0.6204,  ..., -1.5923,  1.0256,  1.6049],
         [ 0.0917, 12.1007, -0.1225,  ..., -0.9279,  0.8739, -1.3214]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(5.5779, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(27.8851, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  5
5
{'input_ids': tensor([[[16036, 35640,   116,  ...,  3598, 32220,     1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 38.6% (4250 out of 11019)

Everytime after generating next logits 39.2% (4314 out of 11019)

Everytime after generating next logits 39.2% (4314 out of 11019)

Everytime after generating next logits 39.2% (4314 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)
Gulf spill- Mexico spill- Mexico spill spill spill- surface of surface of May 5 spill spill spill .
iter = 0 reward = 8
final_logits =  tensor([[[ 9.0948e-02,  3.1289e+00,  6.7151e-01,  ..., -4.1931e+00,
          -1.4458e-02,  1.5176e-01],
         [ 3.1933e-02,  2.0080e+00,  8.1854e-03,  ..., -4.5467e+00,
          -2.4861e+00,  4.0619e-01],
         [-5.5739e-02,  1.5855e-01, -2.5649e-01,  ..., -2.4133e+00,
          -1.3034e+00, -7.6582e-01],
         ...,
         [-6.4794e-02,  2.1975e+00, -3.3872e-01,  ..., -1.7882e+00,
          -2.6242e-01, -5.3690e-01],
         [ 1.0282e-01,  5.7132e+00,  5.9713e-01,  ..., -2.0095e+00,
           9.5163e-01,  4.6109e-01],
         [ 7.8376e-02,  1.2031e+01,  1.3577e-01,  ..., -2.5064e+00,
           1.0859e-01, -1.3369e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.2679, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(9.6327, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4256 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)
Gulf oil giant has broken up spill spill spill spill surface of Gulf spill- Mexico .
iter = 1 reward = 9
final_logits =  tensor([[[ 8.8872e-02,  3.1351e+00,  6.4918e-01,  ..., -4.2625e+00,
          -1.7609e-02,  1.8665e-01],
         [ 2.5330e-02,  2.0160e+00, -1.4864e-02,  ..., -4.7389e+00,
          -2.5322e+00,  3.6012e-01],
         [-5.0312e-02,  1.3125e+00,  6.2269e-02,  ..., -3.1178e+00,
          -1.4001e+00,  7.5822e-01],
         ...,
         [-7.6631e-04,  1.6851e+00, -3.6511e-01,  ..., -3.9956e+00,
          -2.0464e-02, -1.9570e+00],
         [ 9.2843e-02,  4.9899e+00,  3.5110e-01,  ..., -1.9344e+00,
           3.5583e-01,  7.4563e-01],
         [ 7.8943e-02,  1.2359e+01,  8.9664e-02,  ..., -2.5000e+00,
          -4.0366e-01,  3.5811e-02]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.6556, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(19.3350, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4254 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)

Everytime after generating next logits 39.8% (4382 out of 11019)
Gulf oil spill- Mexico spill- Mexico spill spill spill- surface of Gulf spill- surface of Gulf spill- Mexico .
iter = 2 reward = 8
final_logits =  tensor([[[ 0.0922,  3.1284,  0.5870,  ..., -4.4622, -0.2452,  0.1665],
         [ 0.0248,  1.7212, -0.0333,  ..., -4.6216, -2.5319,  0.3321],
         [-0.0551,  1.0725, -0.0188,  ..., -2.9314, -1.8560,  0.3745],
         ...,
         [ 0.0310,  2.0941, -0.2624,  ..., -1.6087, -0.2552, -3.0057],
         [ 0.0967,  4.6520,  0.5301,  ..., -1.6543,  0.5428,  0.3171],
         [ 0.0737, 11.8328,  0.0461,  ..., -2.1959, -0.3251, -0.2811]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.0609, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.3259, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.9% (4282 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)

Everytime after generating next logits 40.0% (4410 out of 11019)
Gulf oil spill- Mexico spill spill- Mexico spill spill spill-toppedtoppedtoppedtoppedtoppedtoppedtoppedtoppedtopped by BP .
iter = 3 reward = 6
final_logits =  tensor([[[ 0.0975,  4.4517,  0.6470,  ..., -3.2933, -0.0269,  0.0253],
         [ 0.0265,  2.5567, -0.0342,  ..., -4.2788, -2.1875,  0.3957],
         [-0.0470,  1.6960,  0.0702,  ..., -3.0171, -1.4805,  0.5309],
         ...,
         [-0.0533,  3.1551, -0.6510,  ..., -3.0574,  0.0322, -1.5841],
         [ 0.1109,  6.2335,  0.8126,  ..., -2.1440,  1.6865,  1.4788],
         [ 0.0751, 11.7278,  0.0862,  ..., -2.0267,  0.7408, -0.0997]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-8.7843, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(35.6449, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4264 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)

Everytime after generating next logits 39.9% (4392 out of 11019)
Spill spill-toppedtopped Gulf Gulf oil spill spill spill spill spill spill spill spill spill spill spill .
iter = 4 reward = 15
final_logits =  tensor([[[ 0.0834,  2.5622,  0.6235,  ..., -5.2383,  0.3579,  0.0138],
         [-0.0857,  0.9813, -0.1992,  ..., -1.5248, -1.0051,  0.1337],
         [-0.0893, -0.5180, -0.2370,  ..., -1.2772, -0.8043, -0.5227],
         ...,
         [-0.1278,  0.2046, -0.3883,  ..., -0.9285, -0.2518, -1.2949],
         [ 0.0569,  4.2756,  0.5201,  ..., -1.8736,  0.7803,  0.2544],
         [ 0.0401, 11.3147,  0.0512,  ..., -1.7332, -0.0689, -0.3094]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-1.6125, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(33.4607, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  15
15
{'input_ids': tensor([[[  787,  1276, 12998,  3531,   148,  4486, 18205, 53857,  7028,   112,
          15079,   109,  3662,   599, 10970,   233, 20447,   788,   121,  1768,
          43304,   110, 10970,   233, 16567,   788,   121,  2617,   120, 16036,
            148,   323,   164,   112, 13889,  4807,   113,   109,  7175,   113,
           3064,   762, 14567,   110,   107,  1263, 53857,  7028,   148,   306,
            115,   253,  2887,   110,   151,   178,  3120,   109,  4807,  1034,
           1844,  2617,   323,   164,   115,   109,  4619,   113,   109,  1073,
           1338,  6687,  3613,   115,   351,   859,   111,  1741,   110,   107,
            285,   148,   243,   178,   117,   146, 10237,   122,   109,   657,
            132, 16036,   111,   138, 15079,   109,  2617,  7539,   110,   107,
           2632,   138,   719,  3916,   135,   109,  2617,   110,   152,   139,
            762, 14567,   148,  2145, 12071,   466,   109,   787,  7175,  3500,
            110,   108,  7271,  3070,   111,  5569,   111, 65047,   181,  4758,
            111, 44650, 18205, 53857,  7028, 35571,  3916,   118,  4807,   113,
            109,  1073,  1338,   110,   108,  6687,  3613,  7922,  8749, 18205,
          53857,  7028, 18097, 16915,   918,   111,   243,  2784,   192,   129,
            154,  5263,   197,   274,   120,   192,   129,  3366,   141,   114,
           1462,   110,   107,   343,   178,   243,   274,  2486,  3916,   355,
            361,   164,   153,   268,   112, 17984, 16036,   110,   107,   139,
           2617,   117,   112, 34262,  7175,   113,  3064,  1836,   111,  1098,
            118,  1166,  9125,   111,  5471,   111,   118,   510,  3207,   111,
           1003,   121,   768,   110,   108,   790,   176,  2242,   110,   107,
          16036,   148,   506,  1389,  3662,   110, 37622,   208,   115,  2242,
            381,   109,   960, 14567,   110,   107,   139,   762, 14567,   110,
            108,   162,  1219,   599,   960,   122,   109, 11335,   113,   109,
          16036,   121, 38539,   252, 81065, 18308,  9600, 13773,   110,   108,
           2145,  8010, 12071,   466,   109,   787,  7175,  3500, 18205, 53857,
           7028, 35571,  3916,   118,  4807,   113,   109,  1073,  1338,   110,
            108,  6687,  3613,   139,  8749,   113,   114,  3662,   599, 10970,
            233, 20447,   788,   121,  1768, 31197,   110, 10970,   233, 16567,
            788,   121,  2617,   112, 13889,  4807,   113,   109, 16036,   762,
          14567,   148,   243,   186,   117,   110,   105,   220,  2767, 16837,
            120, 15931,  2242,   127,   142,   797,   110,   107, 18205, 53857,
           7028, 18097,   112,   129, 24915,   204,   170,   915,  2784,   112,
           1480,   109,  4959,   113,   109,  2617,   110,   107,  3440,  3662,
          12622,   110, 10970,   148,   506,   174,  1389,   165,   115,  2280,
           2784,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)

Everytime after generating next logits 38.6% (4254 out of 11019)
Feinberg has lost 12.5 more than Deepwater spill victims of 2001 oil attacks .
iter = 0 reward = 4
final_logits =  tensor([[[ 5.0719e-02,  3.8402e+00,  8.4150e-01,  ..., -1.5348e+00,
           1.7691e+00,  2.3575e-01],
         [ 1.0981e-02,  4.4980e+00,  7.1385e-03,  ..., -6.2705e-01,
          -2.4567e+00, -6.5902e-02],
         [ 3.0064e-02,  2.7156e+00, -3.1168e-01,  ..., -2.1079e+00,
           6.4055e-02,  3.3722e-02],
         ...,
         [-9.2419e-02,  4.6871e+00, -7.8926e-01,  ..., -2.0444e+00,
           7.4192e-01, -1.3874e+00],
         [ 7.7992e-02,  4.0481e+00,  3.5411e-01,  ..., -1.9943e+00,
           2.9821e-02,  8.5964e-01],
         [ 6.5846e-02,  1.2513e+01,  2.1872e-01,  ..., -2.8953e-01,
           1.7070e+00,  2.1756e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-10.4399, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(72.5501, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)

Everytime after generating next logits 38.7% (4268 out of 11019)
BP, Kenneth Feinberg, awarded by Kenneth Feinberg, who has been awarded .
iter = 1 reward = 0
final_logits =  tensor([[[ 0.0540,  3.2394,  0.7133,  ..., -3.2982,  2.0181, -0.2733],
         [ 0.0114,  1.0520, -0.4188,  ..., -2.3623,  1.1741, -0.9548],
         [ 0.0298,  1.8794, -0.3729,  ..., -4.0585,  1.9687, -1.8509],
         ...,
         [-0.0200,  3.4267, -0.6311,  ..., -2.6878,  3.3006, -0.4695],
         [ 0.1120,  5.8029,  0.4740,  ..., -2.3374,  2.2923,  2.4886],
         [ 0.0545, 11.3554, -0.2907,  ..., -1.2175,  2.2883, -0.7610]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-8.3438, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(71.0786, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.2% (4208 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)
BP, who oversaw the US fund, has not paid a spill spill spill administrator of an April oil spill court .
iter = 2 reward = 5
final_logits =  tensor([[[ 4.8864e-02,  3.2024e+00,  5.3219e-01,  ..., -4.2569e+00,
           2.3704e+00, -7.5779e-01],
         [ 1.7591e-02,  1.0594e+00, -3.2464e-01,  ..., -2.6434e+00,
           1.4972e+00, -4.4997e-01],
         [ 8.3670e-03,  9.6025e-01, -3.9474e-01,  ..., -4.8653e+00,
           2.7312e+00, -1.4147e+00],
         ...,
         [-6.8596e-02,  2.2712e+00, -6.6937e-01,  ..., -3.5135e+00,
           1.8542e+00, -4.1971e-01],
         [ 7.4622e-02,  2.8670e+00,  1.8848e-01,  ..., -1.1838e+00,
           4.0035e-01,  4.4535e-01],
         [ 5.2217e-02,  1.1609e+01, -2.5331e-01,  ..., -1.4866e+00,
           1.9062e+00, -1.5681e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.1244, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(15.7393, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)
BP, who oversaw the bn fund already has 13.5bn of payments for environmental damage .
iter = 3 reward = 4
final_logits =  tensor([[[ 5.7243e-02,  3.5935e+00,  4.2804e-01,  ..., -3.9314e+00,
           2.3823e+00, -3.8360e-01],
         [ 2.1656e-02,  9.9255e-01, -2.4881e-01,  ..., -2.5596e+00,
           1.5176e+00, -2.7616e-02],
         [ 4.3753e-03,  5.9024e-01, -3.5598e-01,  ..., -5.0843e+00,
           2.8308e+00, -1.0948e+00],
         ...,
         [-7.4636e-02,  3.3704e+00, -8.9451e-01,  ..., -1.7843e+00,
          -2.2576e-01, -7.7449e-01],
         [ 6.4910e-02,  4.3298e+00,  1.1611e-01,  ..., -1.7699e+00,
           5.1175e-01,  1.1068e+00],
         [ 6.1561e-02,  1.1482e+01, -3.4958e-01,  ..., -1.2401e+00,
           1.6305e+00, -4.1058e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-3.1846, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(20.4763, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)
Claims of BP, who oversaw a September rig, will be suspicious .
iter = 4 reward = 0
final_logits =  tensor([[[ 0.0717,  3.9433,  0.4059,  ..., -3.1180,  2.5640, -0.1291],
         [ 0.0259,  3.3288, -0.6086,  ..., -1.9260,  0.8335, -1.3246],
         [ 0.0175,  1.7104, -0.3255,  ..., -3.4763,  2.1276, -0.2652],
         ...,
         [ 0.0262,  5.7482, -0.7841,  ..., -1.5414, -0.3606, -1.7105],
         [ 0.0892,  7.1115,  0.3769,  ..., -1.5525,  0.8100,  0.3752],
         [ 0.0881, 12.3218, -0.2182,  ..., -0.5320,  1.6417, -0.6506]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.1159, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(9.1775, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  0
0
{'input_ids': tensor([[[  787,   762, 14567,   110,   151, 15265,  7788,   110,   105,   124,
            109,  2143,  1034,  1027,  3070,  6375,   127,   239,   270,   263,
            112,   225, 16036,   562,   109, 15737,  2584, 51128,   116, 10928,
           1034,   116,  3070,  2414,   148,  7646,   339,   698, 26680,   233,
          37399,   110,   108, 35493,   111, 23363,   110,   107,   110,   105,
            600,   480,   140, 12144,   110,   108,   155,   265,  7646,   110,
            108, 16837,   178,   649,   110,   108,  1132,   109,  2414,   114,
           4081, 15115,   110,   107,   343,  1263, 51128,   116, 10928,  7719,
            180,   138,  4428,   169,  1162,  3070,   260,   559,   111,   118,
            149,   117,   146,   114,   710,  5135,   110,   108,   155,   114,
            729,   121,  4109,   156,   110,   107,   202,   729,  1034,   116,
            419,   112,   133,   114,   715,   110,   108,   181,   660,   113,
            523,   134,   109,   370,   113,   109,  8483, 16837,  4645, 16848,
           2584, 51128,   116, 10928,  7915, 25996,   110,   105,  2184,  6021,
            134,   214,   173,   145,   416,   136,   762, 14567,   256,   129,
           3150,   197, 23363,   110,   108,   155,   145,   114,   457,  3178,
            131,   144, 34742,   110,   108, 16837,   178,   649,   110,   107,
            398,   178, 41593,   164,   114,  1124,  2944,   113,  9606,  6545,
            115,   109,  2414,  1034,   116,  7270,   110,   108,  2584,   649,
            178,   117,  5238,   120,   109, 14567,   138,   129,   289, 13502,
            118,   223,   200,   115,   136,  3820,   121, 23623,  3070,   427,
            113, 38584,  1946, 11958,   144,   216,   110,   108,  7915, 29546,
           3070,   148,   174,  7162,   115,  7915,   262,   113,   109, 14567,
          10250,  1034,   116,  1750,   110,   108,  1946, 74523,   110,   108,
           5765,   122,   342,   111,  2779,   112,  7904,   110,   107,   110,
            105, 15265,   117,   169,   271,   110,   108,   347,   126,   178,
            358,  3178,   131,   144,   235,   742,   997,   110,   108, 16837,
            265,   649,   110,   107,   110,   105,   285,  1034,   116, 13735,
            110,   107,   285,  1034,   116,  7432,   134,   488,   110,   107,
            285,  1034,   116,   146,   109,   310, 29546,   420, 15538, 90155,
            110,   151,   110,   105,   168,   978,   172,   114, 10447,  1120,
            279,   264, 16837, 16036,   148, 20350,  5478,   113, 63981,  7881,
            333,   109,  7175,   113,  3064,   762, 14567,   110,   108,   155,
            186,   127,   274,   170,   127,  1192,   126,  2773,  7427,   118,
            109,   201,   126,   117,   557,   110,   108,  6260,   109,  6442,
           1034,   116,  6869,  3544,   115,  7915,   110,   107, 46095,   126,
            110,   108,   155, 16036,   117,   146,   114,  6749,  1172,   264,
            115,   109,  6805,  1120,   113,  7026, 63116, 58439,   110,   107,
            222,   109, 10601,  1034,   116,   629,   110,   108, 14515,   429,
            115,   114,  1120,  4511,   120,   117,   239,   163,   238,   112,
          16036,  1034,   116,   648,   115,   136,   297,   113,  7915,   110,
            108,   623,   391,  1725,  2051,   279,   115,  4555,   523,  1490,
          21955,  8802,   110,   107,   110,   105,   184,  1034,   216,   146,
            314,   785,   118,  1609,   126,   110,   108,   155,   264, 16036,
           1034,   116,   557,   234,   110,   108, 16837,   156,   649,   110,
            107,   353,  1034,   116,   956,  2158,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)
'I'm worried about the huge disaster that will be that will will that will that be worse worse worse worse worse than the worst .
iter = 0 reward = 7
final_logits =  tensor([[[ 0.1550,  5.1807,  0.8104,  ..., -3.5646, -0.4741, -0.8199],
         [ 0.2075,  3.8716,  1.9856,  ..., -4.0122,  1.9715,  0.0745],
         [ 0.1971,  2.5810,  0.5971,  ..., -2.5400, -3.0097, -1.0685],
         ...,
         [ 0.0854,  8.1705, -0.0822,  ..., -4.9429,  1.5273,  0.5991],
         [ 0.1419,  9.7624,  0.3818,  ..., -2.0671, -0.3360,  1.3383],
         [ 0.1945, 12.8333,  0.2886,  ..., -2.7319,  0.2056,  0.3763]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(15.7081, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(159.1704, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4234 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)

Everytime after generating next logits 38.7% (4262 out of 11019)
Residents of the Gulf blue boat of Mexico have agreement .
iter = 1 reward = 1
final_logits =  tensor([[[ 1.3092e-01,  3.1973e+00,  6.5082e-01,  ..., -6.0510e+00,
          -1.4888e+00, -1.2312e+00],
         [-8.7503e-03,  3.6635e+00, -2.4011e-01,  ..., -2.5980e+00,
          -1.6717e+00, -1.8264e+00],
         [ 4.4799e-02,  3.3998e+00, -1.7775e-01,  ..., -4.8157e+00,
          -1.8968e+00, -1.6618e+00],
         ...,
         [ 2.3726e-02,  5.4994e+00, -7.3086e-01,  ..., -4.4702e+00,
          -1.9602e+00, -2.4554e+00],
         [ 1.2900e-01,  5.2530e+00,  4.3062e-01,  ..., -2.8930e+00,
          -1.2138e+00,  5.2640e-01],
         [ 1.2864e-01,  1.3150e+01,  3.8033e-03,  ..., -2.3328e+00,
          -1.9462e+00, -1.8549e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(13.6958, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(97.0784, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4212 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)
Mayor of Louisiana writes 'knit tunnel like natural' in his office .
iter = 2 reward = 1
final_logits =  tensor([[[ 1.5663e-01,  4.8966e+00,  9.2255e-01,  ..., -3.7589e+00,
           1.0344e+00, -1.3925e+00],
         [ 7.5350e-02,  2.2705e+00,  1.6906e-01,  ..., -2.4772e+00,
          -1.9253e+00,  4.6749e-01],
         [ 4.1867e-02,  3.9303e+00,  1.2229e-01,  ..., -6.3499e+00,
          -3.0395e-01, -1.7696e+00],
         ...,
         [ 4.9543e-03,  5.9011e+00, -6.8583e-01,  ..., -4.3372e+00,
          -2.2050e-01, -9.2574e-01],
         [ 1.2295e-01,  6.4268e+00,  2.0050e-01,  ..., -2.4173e+00,
          -6.2715e-01, -9.9011e-01],
         [ 1.5642e-01,  1.3008e+01,  1.2780e-01,  ..., -2.1436e+00,
           5.9872e-01,  4.9247e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(15.7639, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(425.6168, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.1% (4200 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)

Everytime after generating next logits 38.5% (4242 out of 11019)
Gulf oil boat of Mexico's ghost boat of Mexico is in town of depressed town .
iter = 3 reward = 2
final_logits =  tensor([[[ 1.6133e-01,  4.9041e+00,  9.2326e-01,  ..., -2.5740e+00,
          -6.8187e-01, -3.3397e-01],
         [ 2.8257e-02,  2.2720e+00,  3.8806e-01,  ..., -6.2188e+00,
          -1.8900e+00, -9.2390e-01],
         [-7.7405e-02,  2.4624e+00,  6.9037e-01,  ..., -4.1969e+00,
          -1.1061e+00, -4.3300e-01],
         ...,
         [-2.1377e-03,  5.8731e+00, -4.6226e-01,  ..., -2.8672e+00,
          -2.0548e+00, -1.0164e+00],
         [ 9.5535e-02,  5.5873e+00,  2.0745e-01,  ..., -1.5640e+00,
          -9.4946e-01,  6.2422e-02],
         [ 1.7125e-01,  1.3796e+01,  4.2031e-01,  ..., -1.6499e+00,
          -8.9922e-01,  1.0540e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(9.7752, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(35.0944, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4204 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.4% (4232 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)

Everytime after generating next logits 38.5% (4246 out of 11019)
George Baris, a Louisiana, says he survived a fishing in his office .
iter = 4 reward = 2
final_logits =  tensor([[[ 0.1567,  4.1410,  0.8407,  ..., -1.3769, -1.7102, -0.3885],
         [ 0.1330,  3.2021,  0.1895,  ..., -0.6874, -1.8125, -0.5262],
         [ 0.0916,  3.8098,  0.5654,  ..., -1.6311, -2.4743,  0.0737],
         ...,
         [-0.0240,  4.6394, -0.7582,  ..., -3.4559, -1.3936, -0.6471],
         [ 0.0938,  6.1593,  0.1378,  ..., -2.2132, -0.6769, -0.1829],
         [ 0.1323, 13.3111,  0.1570,  ..., -0.5918, -1.3255,  0.1833]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(7.2529, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(35.6588, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  2
2
{'input_ids': tensor([[[  608,  1338,  2652,  2882,  2033,   134,   305, 91516, 17403,  4596,
            202,   110,   105,  5936,   113, 10114,  4064,   114,   344,   113,
            291,  1829, 16837,   140,   112,  6511,   118,   109, 81065, 18308,
            762, 14567,   115,   109,  7175,   113,  3064,   110,   108, 16036,
            649,   110,   107,   983,  3244,  2777,   165,   141, 16036,   649,
            126,   140,  1470,   115,   297,   118,   109,  5135,   110,   108,
            155,   163,  1262,   181,  6511,   124,   176,   524,   375,   124,
            109,   210,   110,   107, 16036,  4121, 13353,   113,  2729,   121,
           8805,   113,  1165,  2242,   118,  3916,   204,   109, 14567,   110,
            108,   109,  3741,   115,   909,   787,   689,   110,   107,   608,
           1338,  2652,  2882,  2033,   134, 71437, 17403,  4596, 16036,   148,
           1788,   114,  1933,   545,   162,   939,   109,   301,  1034,   116,
            700,   113,   702,   964,   164,   112,   109,  7175,   113,  3064,
            762, 14567,   110,   107,  2973,   112,   109,   301,   110,   108,
            114,   110,   105,  5936,   113, 10114,  4064,   114,   344,   113,
            291,  1829, 16837,   140,   112,  6511,   118,   109, 81065, 18308,
            762, 14567,   110,   107,   983,  3244,  2777,   165,   141, 16036,
            243,   120,   126,   140,  1470,   115,   297,   118,   109,  5135,
            110,   108,   155,   126,   163, 17188,   176,   524,   375,   124,
            109,   210,   110,   107, 16036,  4121, 13353,   113,  2729,   121,
           8805,   113,  1165,  2242,   118,  3916,   204,   109, 14567,   110,
            108,   109,  3741,   115,   909,   787,   689,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)

Everytime after generating next logits 38.4% (4236 out of 11019)
Help is a video of the damage damage the company says is caused by the damage the company says is to the bottom of the BP BP BP says .
iter = 0 reward = 13
final_logits =  tensor([[[ 0.1361,  4.8941,  0.7282,  ..., -2.8517,  0.0624, -0.5317],
         [ 0.0437,  3.0822, -0.3640,  ..., -1.6206, -0.4446, -0.2355],
         [ 0.0554,  2.1778, -0.0554,  ..., -3.2523,  0.5291, -1.3956],
         ...,
         [ 0.0310,  6.4245, -0.7160,  ..., -3.0187, -0.9532, -1.8284],
         [ 0.0996,  6.0058,  0.1364,  ..., -2.6766,  0.2186,  0.2804],
         [ 0.1285, 12.0124,  0.0601,  ..., -2.3679, -0.6934, -0.4773]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(20.0237, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(131.5150, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)

Everytime after generating next logits 38.5% (4240 out of 11019)
Deepwater Horizon over oil-worth of compensation .
iter = 1 reward = 3
final_logits =  tensor([[[ 0.0606,  5.9907,  0.3236,  ..., -4.1413,  1.3430,  0.8346],
         [ 0.0340,  4.5183, -0.0312,  ..., -4.1206, -0.3793, -0.8580],
         [ 0.0142,  3.7647, -0.2677,  ..., -5.2377, -0.8542, -1.1661],
         ...,
         [-0.0700,  3.6711, -0.7103,  ..., -3.1507, -2.3605, -0.0866],
         [ 0.0887,  4.8655, -0.0470,  ..., -1.3904,  0.0714,  0.4582],
         [ 0.0429, 13.2142, -0.2449,  ..., -3.6507,  0.2115, -0.2044]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-1.8785, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(13.7506, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)

Everytime after generating next logits 38.3% (4216 out of 11019)
Gulf Horizon oil company over legal number of . legal video of spill-worth of .
iter = 2 reward = 4
final_logits =  tensor([[[ 4.3082e-02,  6.3252e+00,  3.6356e-01,  ..., -2.5721e+00,
           1.2844e+00,  1.5176e-01],
         [ 1.7214e-02,  3.8594e+00,  2.3116e-04,  ..., -5.2024e+00,
          -1.1994e+00, -7.6927e-02],
         [-1.9669e-02,  3.6796e+00, -2.2776e-01,  ..., -5.1727e+00,
          -1.2522e+00, -3.6434e-01],
         ...,
         [-5.4550e-02,  4.0768e+00, -2.6017e-01,  ..., -2.9881e+00,
           1.5678e+00, -1.0559e+00],
         [ 6.7415e-02,  5.8137e+00,  1.5178e-01,  ..., -2.0282e+00,
           9.1479e-01,  5.1907e-01],
         [ 9.2466e-04,  1.0892e+01, -1.9699e-01,  ..., -2.2619e+00,
           2.0057e+00, -1.0220e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-6.0969, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(38.3837, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)

Everytime after generating next logits 38.5% (4238 out of 11019)
Gulf Horizon company blamed company of Mexico over legal video of spill .
iter = 3 reward = 5
final_logits =  tensor([[[ 0.0448,  5.0711,  0.4007,  ..., -3.2445,  1.5238,  0.7742],
         [ 0.0140,  3.3286, -0.0273,  ..., -4.7259, -1.6799, -0.1344],
         [-0.0279,  3.6331, -0.3745,  ..., -4.3249, -1.7982, -0.3787],
         ...,
         [-0.1310,  2.1748, -0.7881,  ..., -0.3281, -0.8130, -0.7367],
         [ 0.0902,  4.5814,  0.1141,  ..., -1.2125,  0.1495,  0.3183],
         [ 0.0475, 13.5966,  0.0654,  ..., -1.6717,  0.4459, -0.4340]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-5.6932, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(29.1382, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)

Everytime after generating next logits 38.4% (4230 out of 11019)
Video of 2010 Gulf Horizon oil company's oil-worth of Mexico blamed on spill .
iter = 4 reward = 4
final_logits =  tensor([[[ 0.0435,  5.2033,  0.4482,  ..., -3.0358,  1.4408,  1.1183],
         [-0.0435,  3.4891, -0.3670,  ..., -3.4792, -0.3638, -0.8984],
         [-0.0889,  1.3336, -0.2810,  ..., -5.1951,  0.4217, -0.9522],
         ...,
         [-0.1472,  2.3140, -0.6339,  ..., -1.4887, -1.3679, -0.9908],
         [ 0.0763,  4.3971,  0.1681,  ..., -1.8448,  0.6099,  0.3889],
         [ 0.0322, 12.9812,  0.1004,  ..., -2.5539,  0.1195, -0.2719]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-6.4038, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(29.0031, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  4
4
{'input_ids': tensor([[[  530,  1268,  2651,  2882,  2033,   134, 80621, 17403,  4596, 25688,
            115, 16036,  1963,   902,   124,  1789,  2409,   114,   787,  7501,
           5268, 79701,   109,   762,   301,  1034,   116,  4206,   111, 12856,
            130,   210,   130, 16036,   118,   109,  7175,   113,  3064,   762,
          14567,   110,   107,   139, 11335,   134,   109, 81065, 18308, 13773,
            115,   960,  3040,  1073,  1024,   111,  1358,   112,   109,  1368,
            521,  9134,   762,  8186,   110,   107,  4987,   109,  4469,   110,
            108, 16036,  2853,  3947,   607,  2527,   110, 43995,   118,   109,
            211,   166,   381,   913,   289,   232,   110,   107,  6442,   260,
          21089, 80736, 82734,  4278,  3886,   112,  5930, 37313,   110,   108,
            142,   762,  8962,   122, 85411,   116,  1714,   208, 59297, 20185,
            110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)
Nick Bentley rose from Deepwater Horizon to Deepwater Horizon oil company .
iter = 0 reward = 6
final_logits =  tensor([[[ 4.4558e-02,  3.2929e+00,  6.2801e-01,  ..., -4.3724e+00,
           1.0686e+00,  8.7267e-01],
         [ 2.2422e-02,  1.4554e+00,  1.7927e-02,  ..., -2.4977e+00,
           3.2429e-01, -1.0566e-02],
         [-4.5378e-02,  4.7289e-01, -4.8104e-01,  ..., -2.7991e+00,
          -2.1941e+00, -1.1595e+00],
         ...,
         [-1.0019e-01,  2.8181e+00, -6.3883e-01,  ..., -3.7434e+00,
           2.0876e+00, -1.7640e+00],
         [ 5.6305e-02,  3.3878e+00,  9.3286e-02,  ..., -1.6100e+00,
           4.3454e-01,  4.9797e-01],
         [ 5.0476e-03,  1.2469e+01, -1.8767e-01,  ..., -2.8803e+00,
           1.0571e-01, -1.1826e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-9.5070, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(45.6371, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)
Despite higher oil analyst, shares of BP's share price .
iter = 1 reward = 3
final_logits =  tensor([[[ 0.0274,  5.5571,  0.5156,  ..., -2.3159,  1.4061,  1.0445],
         [-0.0780,  2.9955,  0.0825,  ..., -4.2401,  1.2567,  1.1359],
         [-0.0952,  0.9098,  0.2912,  ..., -1.2823,  0.6106,  1.3169],
         ...,
         [-0.0515,  4.7949, -0.6232,  ..., -3.4553, -0.4192, -0.7817],
         [ 0.0683,  5.4260,  0.1400,  ..., -2.1107,  0.4998,  0.3902],
         [ 0.0397, 13.1896, -0.0337,  ..., -1.5272,  1.0188, -0.3051]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.3164, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(18.4879, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)

Everytime after generating next logits 38.3% (4218 out of 11019)
Despite higher oil price, shares of the company's share price .
iter = 2 reward = 3
final_logits =  tensor([[[ 0.0195,  5.6964,  0.4640,  ..., -2.3021,  1.7672,  0.3999],
         [-0.0709,  3.6817,  0.0405,  ..., -4.4237,  0.8356,  0.9626],
         [-0.0891,  1.6460,  0.2580,  ..., -0.7947,  0.4862,  1.0529],
         ...,
         [-0.0380,  5.4463, -0.7039,  ..., -3.8066,  0.6548, -1.4136],
         [ 0.0676,  6.9623,  0.3533,  ..., -2.2017,  1.5570,  0.6411],
         [ 0.0415, 13.0321,  0.0875,  ..., -1.3141,  1.5626, -0.0559]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-3.1887, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(10.0962, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)

Everytime after generating next logits 38.3% (4224 out of 11019)
Despite higher oil analyst's record, shares of the company's stock market .
iter = 3 reward = 3
final_logits =  tensor([[[ 0.0287,  5.2571,  0.4643,  ..., -2.2844,  1.7653, -0.1809],
         [-0.0614,  3.3658,  0.1126,  ..., -5.1230,  0.5161,  1.1539],
         [-0.0774,  1.3297,  0.2649,  ..., -1.3359,  0.4889,  1.1764],
         ...,
         [-0.1012,  6.3381, -0.4649,  ..., -4.5419, -0.7521,  0.6043],
         [ 0.0388,  6.7339,  0.4024,  ..., -3.1718,  0.2973,  0.3498],
         [ 0.0563, 12.3378,  0.1128,  ..., -1.9047,  1.4267, -0.7368]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-1.9385, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.1645, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)

Everytime after generating next logits 38.6% (4248 out of 11019)
Despite Deepwater oil spill, BBC's Red Horizon 500 correspondent rose largest oil spill time .
iter = 4 reward = 6
final_logits =  tensor([[[ 0.0451,  4.1932,  0.4625,  ..., -3.1532,  1.4910,  0.9177],
         [-0.0479,  1.9490, -0.0134,  ..., -5.0624,  0.6960,  0.3750],
         [-0.0362,  0.8222, -0.0868,  ..., -4.3279,  1.3172, -0.6573],
         ...,
         [-0.0592,  4.9259, -0.7697,  ..., -1.2226,  0.5350, -0.6174],
         [ 0.0820,  3.1600,  0.0309,  ..., -0.8367, -0.1340,  0.2382],
         [ 0.0425, 12.7997,  0.0543,  ..., -1.4786,  0.9543, -0.1075]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(9.1521, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(52.0461, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  6
6
{'input_ids': tensor([[[ 3788,   110,   108,   258,   111,  1183,   109,  4193,   120,  5887,
            119,  3788, 22017,  3249,  2309,   114,  4340,   132,  2162,   110,
            108,   132,   989,  6502,   115,   150,   545,  1771,  6180,   114,
           4340,   493,   126,   586,   112,   258,   165,   241,   111,   173,
            157,   133,  6502,   110,   107, 14845,   127,  1661,   115,   156,
            295,   110,   108,  2063,   119,  1235,   111,   400,   489,   110,
            107, 21556,   116,  4170, 16036,   762, 14567,  1736,   651,  1265,
           1185,  2652,   110,   108,  3013,   111,  9792,  5297,  5299,  2346,
           3601,  2567,  7499,   114,  1736,   266,  1678,   115,   109, 11319,
            124,   109, 16036,   762, 14567,   110,   107,  2346,  3601,  2567,
            898,  6949,   120,   142,  8635,   933,   113,  1647,   196,   506,
            174,  9889,   110,   108,   155,   701, 51098,   192,   129,   656,
            130,  6031,  1219,   115,  4190,  6500,  3381,   113, 43278,   110,
            107,   139,   657,   148, 18097,   112,   110,   105,   543,   109,
           2919,   113,   219,  6209,   702, 16837,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)

Everytime after generating next logits 38.4% (4226 out of 11019)
Commons debate on climate change affects oil debate .
iter = 0 reward = 1
final_logits =  tensor([[[ 1.2998e-01,  2.9846e+00,  1.2494e+00,  ..., -1.8864e-01,
           2.2760e+00,  5.1545e-01],
         [ 7.4554e-03,  7.0722e-01,  1.2337e-02,  ...,  4.2886e-02,
          -1.3682e+00, -1.2342e+00],
         [-5.1258e-02,  1.0114e+00, -4.6608e-01,  ...,  1.9889e-01,
          -8.8049e-02, -1.5300e+00],
         ...,
         [-9.7306e-02,  2.6764e+00, -7.0640e-01,  ..., -1.0392e+00,
          -1.4055e+00, -1.3556e+00],
         [ 6.5560e-02,  5.8326e+00,  4.0698e-01,  ..., -9.4152e-01,
          -3.3113e-01,  7.9968e-01],
         [ 5.9139e-02,  1.2214e+01,  3.2445e-01,  ...,  4.3794e-01,
           3.6572e-01, -8.1595e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-1.6485, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.2725, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)

Everytime after generating next logits 38.2% (4204 out of 11019)
Commons debate on climate politics allowing oil debate on June oil .
iter = 1 reward = 3
final_logits =  tensor([[[ 1.3409e-01,  2.9452e+00,  1.2902e+00,  ...,  6.9527e-02,
           2.2287e+00,  4.8275e-01],
         [ 9.4773e-03,  6.9374e-01,  1.3647e-02,  ...,  3.0183e-01,
          -1.2697e+00, -1.2643e+00],
         [-4.7565e-02,  1.0918e+00, -4.2157e-01,  ...,  4.8833e-01,
           1.0660e-01, -1.3446e+00],
         ...,
         [-1.1014e-01,  2.3813e+00, -4.2018e-01,  ..., -1.5557e+00,
          -8.9543e-01, -2.0064e+00],
         [ 8.8333e-02,  5.2282e+00,  2.5225e-01,  ..., -3.9659e-01,
          -3.6425e-01,  5.4054e-01],
         [ 7.8618e-02,  1.2874e+01,  4.8335e-01,  ...,  1.3683e+00,
           5.6405e-01, -4.6817e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.0441, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(4.3258, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)

Everytime after generating next logits 38.4% (4228 out of 11019)
Commons debate allowing oil debate on climate .
iter = 2 reward = 1
final_logits =  tensor([[[ 1.3845e-01,  2.9881e+00,  1.3188e+00,  ...,  1.0194e-01,
           2.1858e+00,  4.0924e-01],
         [ 1.0791e-02,  8.0402e-01, -9.0708e-03,  ...,  4.0975e-01,
          -1.0493e+00, -1.4692e+00],
         [-4.1831e-02,  1.1189e+00, -3.8305e-01,  ...,  7.6618e-01,
           2.4191e-01, -1.2522e+00],
         ...,
         [-1.0017e-01,  7.4577e-01, -3.1387e-01,  ...,  1.4437e-03,
          -4.6318e-01,  1.2180e+00],
         [ 6.6734e-02,  4.0800e+00,  7.0028e-01,  ..., -5.2617e-01,
           1.1844e-01,  2.2806e+00],
         [ 4.5792e-02,  1.1515e+01,  4.4090e-01,  ...,  1.5510e+00,
           1.1327e+00, -3.3152e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.6999, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(4.9165, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)
Commons debate allowing oil debate allowing oil debate on climate .
iter = 3 reward = 2
final_logits =  tensor([[[ 1.4643e-01,  3.1825e+00,  1.3590e+00,  ..., -2.7795e-01,
           1.9712e+00,  7.6816e-02],
         [ 1.2113e-02,  9.5043e-01, -3.2415e-02,  ...,  2.7999e-01,
          -1.0675e+00, -1.9693e+00],
         [-3.6253e-02,  1.3510e+00, -2.5775e-01,  ...,  8.1126e-01,
           3.1813e-01, -1.3058e+00],
         ...,
         [-9.6974e-02,  1.7442e+00, -4.2907e-01,  ..., -4.4723e-01,
          -7.2432e-01,  7.6515e-01],
         [ 8.8193e-02,  5.2078e+00,  3.9084e-01,  ..., -4.2512e-01,
          -4.3785e-01,  9.8002e-01],
         [ 8.3349e-02,  1.2340e+01,  5.8966e-01,  ...,  1.1715e+00,
           6.6312e-01, -4.9960e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.2483, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(8.9370, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)
Commons debate allowing oil debate allowing oil debate on climate .
iter = 4 reward = 2
final_logits =  tensor([[[ 1.4688e-01,  3.1605e+00,  1.3587e+00,  ..., -4.3301e-01,
           1.8389e+00, -2.5485e-01],
         [ 9.8403e-03,  9.4544e-01, -4.3416e-02,  ...,  3.9262e-02,
          -1.3488e+00, -2.3292e+00],
         [-3.6586e-02,  1.5187e+00, -2.6742e-01,  ...,  8.3080e-01,
           4.9530e-01, -1.6563e+00],
         ...,
         [-9.9972e-02,  1.7498e+00, -4.3003e-01,  ..., -5.9036e-01,
          -7.8634e-01,  6.1217e-01],
         [ 8.0298e-02,  5.1900e+00,  3.2877e-01,  ..., -7.0614e-01,
          -3.2856e-01,  1.0042e+00],
         [ 8.1323e-02,  1.2121e+01,  6.0675e-01,  ...,  1.1240e+00,
           7.2939e-01, -7.3841e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.7246, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(11.4487, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  2
2
{'input_ids': tensor([[[ 1428,  1307,  2652,  2882,  2033,   134, 84838,   726, 17403,  4596,
          16036,   148,  1939,   114,   177, 14464,  3889,   124,   109, 14154,
           7175,   113,  3064,   762,   210,   110,   108,   301,  2662,   416,
            110,   107,   168,   117,  8549,   109,   177,  3889,   138,   923,
            109,  8186,   111,  3155,   109,   762,   269,   154,   113,   126,
            137,  6218,   190,   109,  1917,   155,   126,   256,   129,   372,
            228,   390,   269, 16036,   235,   682,   109,   807,  2353,   117,
           1147,   110,   107,   139, 11335,   113,   109, 81065, 18308, 13773,
            115,   960,  3040,  1073,   200,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)

Everytime after generating next logits 38.2% (4210 out of 11019)
Deepwater Horizon at Mexico has more successful company at Gulf oil leak .
iter = 0 reward = 6
final_logits =  tensor([[[ 0.0597,  5.0193,  0.5955,  ..., -3.3216,  0.9586,  0.7193],
         [ 0.0174,  3.5087, -0.0318,  ..., -4.3790, -0.8659, -1.1099],
         [-0.0736,  2.7268, -0.4032,  ..., -3.9098, -1.3654, -1.7748],
         ...,
         [-0.1013,  1.8718, -0.6081,  ..., -2.6523,  0.1192, -1.1714],
         [ 0.0462,  4.4653, -0.0476,  ..., -0.6136,  0.3739,  0.1793],
         [ 0.0183, 12.4638,  0.1077,  ..., -1.8646, -0.3548,  0.0491]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(5.5252, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(75.0949, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)

Everytime after generating next logits 38.2% (4206 out of 11019)
Deepwater Horizon at Mexico has more successful company at Mexico latest latest sea capture .
iter = 1 reward = 4
final_logits =  tensor([[[ 0.0531,  4.9753,  0.5788,  ..., -3.2580,  0.8992,  0.7054],
         [ 0.0133,  3.4635, -0.0528,  ..., -4.4053, -0.9040, -1.1594],
         [-0.0766,  2.7534, -0.4066,  ..., -3.5491, -1.4928, -1.2977],
         ...,
         [-0.1426,  3.3391, -0.3866,  ..., -5.0832, -0.0740, -1.5968],
         [ 0.0559,  3.7223,  0.1469,  ..., -1.0038, -0.0688,  0.8992],
         [ 0.0191, 12.7096,  0.1221,  ..., -2.3983, -0.4373,  0.3209]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.6214, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(38.4802, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.5% (4244 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.6% (4256 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.9% (4286 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.0% (4302 out of 11019)

Everytime after generating next logits 39.2% (4320 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.5% (4358 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 39.7% (4378 out of 11019)

Everytime after generating next logits 40.1% (4422 out of 11019)

Everytime after generating next logits 40.5% (4466 out of 11019)

Everytime after generating next logits 40.5% (4466 out of 11019)

Everytime after generating next logits 40.5% (4466 out of 11019)

Everytime after generating next logits 40.5% (4466 out of 11019)

Everytime after generating next logits 40.5% (4466 out of 11019)

Everytime after generating next logits 41.0% (4514 out of 11019)

Everytime after generating next logits 41.4% (4562 out of 11019)

Everytime after generating next logits 41.4% (4562 out of 11019)

Everytime after generating next logits 41.4% (4562 out of 11019)

Everytime after generating next logits 41.4% (4562 out of 11019)

Everytime after generating next logits 41.9% (4614 out of 11019)

Everytime after generating next logits 42.3% (4666 out of 11019)

Everytime after generating next logits 42.3% (4666 out of 11019)

Everytime after generating next logits 42.3% (4666 out of 11019)

Everytime after generating next logits 42.3% (4666 out of 11019)

Everytime after generating next logits 42.9% (4722 out of 11019)

Everytime after generating next logits 43.4% (4778 out of 11019)

Everytime after generating next logits 43.4% (4778 out of 11019)

Everytime after generating next logits 43.4% (4778 out of 11019)

Everytime after generating next logits 43.4% (4778 out of 11019)

Everytime after generating next logits 43.4% (4778 out of 11019)

Everytime after generating next logits 43.9% (4838 out of 11019)

Everytime after generating next logits 44.5% (4898 out of 11019)

Everytime after generating next logits 44.5% (4898 out of 11019)

Everytime after generating next logits 44.5% (4898 out of 11019)

Everytime after generating next logits 44.5% (4898 out of 11019)

Everytime after generating next logits 45.0% (4962 out of 11019)

Everytime after generating next logits 45.6% (5026 out of 11019)

Everytime after generating next logits 45.6% (5026 out of 11019)

Everytime after generating next logits 45.6% (5026 out of 11019)

Everytime after generating next logits 45.6% (5026 out of 11019)

Everytime after generating next logits 45.6% (5026 out of 11019)

Everytime after generating next logits 46.2% (5094 out of 11019)

Everytime after generating next logits 46.8% (5162 out of 11019)

Everytime after generating next logits 46.8% (5162 out of 11019)

Everytime after generating next logits 46.8% (5162 out of 11019)

Everytime after generating next logits 46.8% (5162 out of 11019)

Everytime after generating next logits 47.5% (5234 out of 11019)

Everytime after generating next logits 48.2% (5306 out of 11019)

Everytime after generating next logits 48.2% (5306 out of 11019)

Everytime after generating next logits 48.2% (5306 out of 11019)

Everytime after generating next logits 48.2% (5306 out of 11019)

Everytime after generating next logits 48.2% (5306 out of 11019)

Everytime after generating next logits 48.8% (5382 out of 11019)

Everytime after generating next logits 49.5% (5458 out of 11019)

Everytime after generating next logits 49.5% (5458 out of 11019)

Everytime after generating next logits 49.5% (5458 out of 11019)

Everytime after generating next logits 49.5% (5458 out of 11019)

Everytime after generating next logits 50.3% (5538 out of 11019)

Everytime after generating next logits 51.0% (5618 out of 11019)

Everytime after generating next logits 51.0% (5618 out of 11019)

Everytime after generating next logits 51.0% (5618 out of 11019)

Everytime after generating next logits 51.0% (5618 out of 11019)

Everytime after generating next logits 51.0% (5618 out of 11019)

Everytime after generating next logits 51.7% (5702 out of 11019)

Everytime after generating next logits 52.5% (5786 out of 11019)

Everytime after generating next logits 52.5% (5786 out of 11019)

Everytime after generating next logits 52.5% (5786 out of 11019)

Everytime after generating next logits 52.5% (5786 out of 11019)

Everytime after generating next logits 53.3% (5874 out of 11019)

Everytime after generating next logits 54.1% (5962 out of 11019)

Everytime after generating next logits 54.1% (5962 out of 11019)

Everytime after generating next logits 54.1% (5962 out of 11019)

Everytime after generating next logits 54.1% (5962 out of 11019)

Everytime after generating next logits 54.1% (5962 out of 11019)

Everytime after generating next logits 54.9% (6054 out of 11019)

Everytime after generating next logits 55.8% (6146 out of 11019)

Everytime after generating next logits 55.8% (6146 out of 11019)

Everytime after generating next logits 55.8% (6146 out of 11019)

Everytime after generating next logits 55.8% (6146 out of 11019)

Everytime after generating next logits 56.6% (6242 out of 11019)

Everytime after generating next logits 57.5% (6338 out of 11019)

Everytime after generating next logits 57.5% (6338 out of 11019)

Everytime after generating next logits 57.5% (6338 out of 11019)

Everytime after generating next logits 57.5% (6338 out of 11019)

Everytime after generating next logits 58.4% (6438 out of 11019)

Everytime after generating next logits 59.3% (6538 out of 11019)

Everytime after generating next logits 59.3% (6538 out of 11019)

Everytime after generating next logits 59.3% (6538 out of 11019)

Everytime after generating next logits 59.3% (6538 out of 11019)

Everytime after generating next logits 59.3% (6538 out of 11019)

Everytime after generating next logits 60.3% (6642 out of 11019)

Everytime after generating next logits 61.2% (6746 out of 11019)

Everytime after generating next logits 61.2% (6746 out of 11019)

Everytime after generating next logits 61.2% (6746 out of 11019)

Everytime after generating next logits 61.2% (6746 out of 11019)

Everytime after generating next logits 62.2% (6854 out of 11019)

Everytime after generating next logits 63.2% (6962 out of 11019)

Everytime after generating next logits 63.2% (6962 out of 11019)

Everytime after generating next logits 63.2% (6962 out of 11019)

Everytime after generating next logits 63.2% (6962 out of 11019)

Everytime after generating next logits 63.2% (6962 out of 11019)

Everytime after generating next logits 64.2% (7074 out of 11019)

Everytime after generating next logits 65.2% (7186 out of 11019)

Everytime after generating next logits 65.2% (7186 out of 11019)

Everytime after generating next logits 65.2% (7186 out of 11019)

Everytime after generating next logits 65.2% (7186 out of 11019)

Everytime after generating next logits 66.3% (7302 out of 11019)

Everytime after generating next logits 67.3% (7418 out of 11019)

Everytime after generating next logits 67.3% (7418 out of 11019)

Everytime after generating next logits 67.3% (7418 out of 11019)

Everytime after generating next logits 67.3% (7418 out of 11019)

Everytime after generating next logits 67.3% (7418 out of 11019)

Everytime after generating next logits 68.4% (7538 out of 11019)

Everytime after generating next logits 69.5% (7658 out of 11019)

Everytime after generating next logits 69.5% (7658 out of 11019)

Everytime after generating next logits 69.5% (7658 out of 11019)

Everytime after generating next logits 69.5% (7658 out of 11019)

Everytime after generating next logits 70.6% (7782 out of 11019)

Everytime after generating next logits 71.7% (7906 out of 11019)

Everytime after generating next logits 71.7% (7906 out of 11019)

Everytime after generating next logits 71.7% (7906 out of 11019)

Everytime after generating next logits 71.7% (7906 out of 11019)

Everytime after generating next logits 71.7% (7906 out of 11019)

Everytime after generating next logits 72.9% (8034 out of 11019)

Everytime after generating next logits 74.1% (8162 out of 11019)

Everytime after generating next logits 74.1% (8162 out of 11019)

Everytime after generating next logits 74.1% (8162 out of 11019)

Everytime after generating next logits 74.1% (8162 out of 11019)

Everytime after generating next logits 75.3% (8294 out of 11019)

Everytime after generating next logits 76.5% (8426 out of 11019)

Everytime after generating next logits 76.5% (8426 out of 11019)

Everytime after generating next logits 76.5% (8426 out of 11019)

Everytime after generating next logits 76.5% (8426 out of 11019)

Everytime after generating next logits 76.5% (8426 out of 11019)

Everytime after generating next logits 77.7% (8562 out of 11019)

Everytime after generating next logits 78.9% (8698 out of 11019)

Everytime after generating next logits 78.9% (8698 out of 11019)

Everytime after generating next logits 78.9% (8698 out of 11019)

Everytime after generating next logits 78.9% (8698 out of 11019)

Everytime after generating next logits 80.2% (8838 out of 11019)

Everytime after generating next logits 81.5% (8978 out of 11019)

Everytime after generating next logits 81.5% (8978 out of 11019)

Everytime after generating next logits 81.5% (8978 out of 11019)

Everytime after generating next logits 81.5% (8978 out of 11019)

Everytime after generating next logits 82.8% (9122 out of 11019)

Everytime after generating next logits 84.1% (9266 out of 11019)

Everytime after generating next logits 84.1% (9266 out of 11019)

Everytime after generating next logits 84.1% (9266 out of 11019)

Everytime after generating next logits 84.1% (9266 out of 11019)

Everytime after generating next logits 84.1% (9266 out of 11019)

Everytime after generating next logits 85.4% (9414 out of 11019)

Everytime after generating next logits 86.8% (9562 out of 11019)

Everytime after generating next logits 86.8% (9562 out of 11019)

Everytime after generating next logits 86.8% (9562 out of 11019)

Everytime after generating next logits 86.8% (9562 out of 11019)

Everytime after generating next logits 88.2% (9714 out of 11019)

Everytime after generating next logits 89.5% (9866 out of 11019)

Everytime after generating next logits 89.5% (9866 out of 11019)

Everytime after generating next logits 89.5% (9866 out of 11019)

Everytime after generating next logits 89.5% (9866 out of 11019)

Everytime after generating next logits 89.5% (9866 out of 11019)

Everytime after generating next logits 91.0% (10022 out of 11019)

Everytime after generating next logits 92.4% (10178 out of 11019)

Everytime after generating next logits 92.4% (10178 out of 11019)

Everytime after generating next logits 92.4% (10178 out of 11019)

Everytime after generating next logits 92.4% (10178 out of 11019)

Everytime after generating next logits 93.8% (10338 out of 11019)

Everytime after generating next logits 95.3% (10498 out of 11019)

Everytime after generating next logits 95.3% (10498 out of 11019)

Everytime after generating next logits 95.3% (10498 out of 11019)

Everytime after generating next logits 95.3% (10498 out of 11019)

Everytime after generating next logits 95.3% (10498 out of 11019)

Everytime after generating next logits 96.8% (10662 out of 11019)

Everytime after generating next logits 98.2% (10826 out of 11019)

Everytime after generating next logits 98.2% (10826 out of 11019)

Everytime after generating next logits 98.2% (10826 out of 11019)

Everytime after generating next logits 98.2% (10826 out of 11019)

Everytime after generating next logits 99.8% (10994 out of 11019)

Everytime after generating next logits 41.2% (4536 out of 11019)

Everytime after generating next logits 41.2% (4536 out of 11019)

Everytime after generating next logits 41.2% (4536 out of 11019)

Everytime after generating next logits 41.2% (4536 out of 11019)

Everytime after generating next logits 41.2% (4536 out of 11019)

Everytime after generating next logits 42.7% (4708 out of 11019)

Everytime after generating next logits 44.3% (4880 out of 11019)

Everytime after generating next logits 44.3% (4880 out of 11019)

Everytime after generating next logits 44.3% (4880 out of 11019)

Everytime after generating next logits 44.3% (4880 out of 11019)

Everytime after generating next logits 45.9% (5056 out of 11019)

Everytime after generating next logits 47.5% (5232 out of 11019)

Everytime after generating next logits 47.5% (5232 out of 11019)

Everytime after generating next logits 47.5% (5232 out of 11019)

Everytime after generating next logits 47.5% (5232 out of 11019)

Everytime after generating next logits 47.5% (5232 out of 11019)

Everytime after generating next logits 49.1% (5412 out of 11019)

Everytime after generating next logits 50.7% (5592 out of 11019)

Everytime after generating next logits 50.7% (5592 out of 11019)

Everytime after generating next logits 50.7% (5592 out of 11019)

Everytime after generating next logits 50.7% (5592 out of 11019)

Everytime after generating next logits 52.4% (5776 out of 11019)

Everytime after generating next logits 54.1% (5960 out of 11019)

Everytime after generating next logits 54.1% (5960 out of 11019)

Everytime after generating next logits 54.1% (5960 out of 11019)

Everytime after generating next logits 54.1% (5960 out of 11019)

Everytime after generating next logits 55.8% (6148 out of 11019)

Everytime after generating next logits 57.5% (6336 out of 11019)

Everytime after generating next logits 57.5% (6336 out of 11019)

Everytime after generating next logits 57.5% (6336 out of 11019)

Everytime after generating next logits 57.5% (6336 out of 11019)

Everytime after generating next logits 57.5% (6336 out of 11019)

Everytime after generating next logits 59.2% (6528 out of 11019)

Everytime after generating next logits 61.0% (6720 out of 11019)

Everytime after generating next logits 61.0% (6720 out of 11019)

Everytime after generating next logits 61.0% (6720 out of 11019)

Everytime after generating next logits 61.0% (6720 out of 11019)

Everytime after generating next logits 62.8% (6916 out of 11019)

Everytime after generating next logits 64.5% (7112 out of 11019)

Everytime after generating next logits 64.5% (7112 out of 11019)

Everytime after generating next logits 64.5% (7112 out of 11019)

Everytime after generating next logits 64.5% (7112 out of 11019)

Everytime after generating next logits 64.5% (7112 out of 11019)

Everytime after generating next logits 66.4% (7312 out of 11019)

Everytime after generating next logits 68.2% (7512 out of 11019)

Everytime after generating next logits 68.2% (7512 out of 11019)

Everytime after generating next logits 68.2% (7512 out of 11019)

Everytime after generating next logits 68.2% (7512 out of 11019)

Everytime after generating next logits 70.0% (7716 out of 11019)

Everytime after generating next logits 71.9% (7920 out of 11019)

Everytime after generating next logits 71.9% (7920 out of 11019)

Everytime after generating next logits 71.9% (7920 out of 11019)

Everytime after generating next logits 71.9% (7920 out of 11019)

Everytime after generating next logits 71.9% (7920 out of 11019)

Everytime after generating next logits 73.8% (8128 out of 11019)

Everytime after generating next logits 75.7% (8336 out of 11019)

Everytime after generating next logits 75.7% (8336 out of 11019)

Everytime after generating next logits 75.7% (8336 out of 11019)

Everytime after generating next logits 75.7% (8336 out of 11019)

Everytime after generating next logits 77.6% (8548 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 81.5% (8976 out of 11019)

Everytime after generating next logits 83.4% (9192 out of 11019)

Everytime after generating next logits 83.4% (9192 out of 11019)

Everytime after generating next logits 83.4% (9192 out of 11019)

Everytime after generating next logits 83.4% (9192 out of 11019)

Everytime after generating next logits 85.4% (9412 out of 11019)

Everytime after generating next logits 87.4% (9632 out of 11019)

Everytime after generating next logits 87.4% (9632 out of 11019)

Everytime after generating next logits 87.4% (9632 out of 11019)

Everytime after generating next logits 87.4% (9632 out of 11019)

Everytime after generating next logits 87.4% (9632 out of 11019)

Everytime after generating next logits 89.4% (9856 out of 11019)

Everytime after generating next logits 91.5% (10080 out of 11019)

Everytime after generating next logits 91.5% (10080 out of 11019)

Everytime after generating next logits 91.5% (10080 out of 11019)

Everytime after generating next logits 91.5% (10080 out of 11019)

Everytime after generating next logits 93.5% (10308 out of 11019)

Everytime after generating next logits 95.6% (10536 out of 11019)

Everytime after generating next logits 95.6% (10536 out of 11019)

Everytime after generating next logits 95.6% (10536 out of 11019)

Everytime after generating next logits 95.6% (10536 out of 11019)

Everytime after generating next logits 97.7% (10768 out of 11019)

Everytime after generating next logits 99.8% (11000 out of 11019)

Everytime after generating next logits 99.8% (11000 out of 11019)

Everytime after generating next logits 99.8% (11000 out of 11019)

Everytime after generating next logits 99.8% (11000 out of 11019)

Everytime after generating next logits 99.8% (11000 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 44.5% (4904 out of 11019)

Everytime after generating next logits 44.5% (4904 out of 11019)

Everytime after generating next logits 44.5% (4904 out of 11019)

Everytime after generating next logits 44.5% (4904 out of 11019)

Everytime after generating next logits 46.7% (5144 out of 11019)

Everytime after generating next logits 48.9% (5384 out of 11019)

Everytime after generating next logits 48.9% (5384 out of 11019)

Everytime after generating next logits 48.9% (5384 out of 11019)

Everytime after generating next logits 48.9% (5384 out of 11019)

Everytime after generating next logits 48.9% (5384 out of 11019)

Everytime after generating next logits 51.1% (5628 out of 11019)

Everytime after generating next logits 53.3% (5872 out of 11019)

Everytime after generating next logits 53.3% (5872 out of 11019)

Everytime after generating next logits 53.3% (5872 out of 11019)

Everytime after generating next logits 53.3% (5872 out of 11019)

Everytime after generating next logits 55.5% (6120 out of 11019)

Everytime after generating next logits 57.8% (6368 out of 11019)

Everytime after generating next logits 57.8% (6368 out of 11019)

Everytime after generating next logits 57.8% (6368 out of 11019)

Everytime after generating next logits 57.8% (6368 out of 11019)

Everytime after generating next logits 57.8% (6368 out of 11019)

Everytime after generating next logits 60.1% (6620 out of 11019)

Everytime after generating next logits 62.4% (6872 out of 11019)

Everytime after generating next logits 62.4% (6872 out of 11019)

Everytime after generating next logits 62.4% (6872 out of 11019)

Everytime after generating next logits 62.4% (6872 out of 11019)

Everytime after generating next logits 64.7% (7128 out of 11019)

Everytime after generating next logits 67.0% (7384 out of 11019)

Everytime after generating next logits 67.0% (7384 out of 11019)

Everytime after generating next logits 67.0% (7384 out of 11019)

Everytime after generating next logits 67.0% (7384 out of 11019)

Everytime after generating next logits 67.0% (7384 out of 11019)

Everytime after generating next logits 69.4% (7644 out of 11019)

Everytime after generating next logits 71.7% (7904 out of 11019)

Everytime after generating next logits 71.7% (7904 out of 11019)

Everytime after generating next logits 71.7% (7904 out of 11019)

Everytime after generating next logits 71.7% (7904 out of 11019)

Everytime after generating next logits 74.1% (8168 out of 11019)

Everytime after generating next logits 76.5% (8432 out of 11019)

Everytime after generating next logits 76.5% (8432 out of 11019)

Everytime after generating next logits 76.5% (8432 out of 11019)

Everytime after generating next logits 76.5% (8432 out of 11019)

Everytime after generating next logits 76.5% (8432 out of 11019)

Everytime after generating next logits 79.0% (8700 out of 11019)

Everytime after generating next logits 81.4% (8968 out of 11019)

Everytime after generating next logits 81.4% (8968 out of 11019)

Everytime after generating next logits 81.4% (8968 out of 11019)

Everytime after generating next logits 81.4% (8968 out of 11019)

Everytime after generating next logits 83.9% (9240 out of 11019)

Everytime after generating next logits 86.3% (9512 out of 11019)

Everytime after generating next logits 86.3% (9512 out of 11019)

Everytime after generating next logits 86.3% (9512 out of 11019)

Everytime after generating next logits 86.3% (9512 out of 11019)

Everytime after generating next logits 88.8% (9788 out of 11019)

Everytime after generating next logits 91.3% (10064 out of 11019)

Everytime after generating next logits 91.3% (10064 out of 11019)

Everytime after generating next logits 91.3% (10064 out of 11019)

Everytime after generating next logits 91.3% (10064 out of 11019)

Everytime after generating next logits 91.3% (10064 out of 11019)

Everytime after generating next logits 93.9% (10344 out of 11019)

Everytime after generating next logits 96.4% (10624 out of 11019)

Everytime after generating next logits 96.4% (10624 out of 11019)

Everytime after generating next logits 96.4% (10624 out of 11019)

Everytime after generating next logits 96.4% (10624 out of 11019)

Everytime after generating next logits 99.0% (10908 out of 11019)

Everytime after generating next logits 43.3% (4768 out of 11019)

Everytime after generating next logits 43.3% (4768 out of 11019)

Everytime after generating next logits 43.3% (4768 out of 11019)

Everytime after generating next logits 43.3% (4768 out of 11019)

Everytime after generating next logits 43.3% (4768 out of 11019)

Everytime after generating next logits 45.9% (5056 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 51.1% (5636 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 53.8% (5928 out of 11019)

Everytime after generating next logits 56.5% (6224 out of 11019)

Everytime after generating next logits 59.2% (6520 out of 11019)

Everytime after generating next logits 59.2% (6520 out of 11019)

Everytime after generating next logits 59.2% (6520 out of 11019)

Everytime after generating next logits 59.2% (6520 out of 11019)

Everytime after generating next logits 61.9% (6820 out of 11019)

Everytime after generating next logits 64.6% (7120 out of 11019)

Everytime after generating next logits 64.6% (7120 out of 11019)

Everytime after generating next logits 64.6% (7120 out of 11019)

Everytime after generating next logits 64.6% (7120 out of 11019)

Everytime after generating next logits 64.6% (7120 out of 11019)

Everytime after generating next logits 67.4% (7424 out of 11019)

Everytime after generating next logits 70.1% (7728 out of 11019)

Everytime after generating next logits 70.1% (7728 out of 11019)

Everytime after generating next logits 70.1% (7728 out of 11019)

Everytime after generating next logits 70.1% (7728 out of 11019)

Everytime after generating next logits 72.9% (8036 out of 11019)

Everytime after generating next logits 75.7% (8344 out of 11019)

Everytime after generating next logits 75.7% (8344 out of 11019)

Everytime after generating next logits 75.7% (8344 out of 11019)

Everytime after generating next logits 75.7% (8344 out of 11019)

Everytime after generating next logits 75.7% (8344 out of 11019)

Everytime after generating next logits 78.6% (8656 out of 11019)

Everytime after generating next logits 81.4% (8968 out of 11019)

Everytime after generating next logits 81.4% (8968 out of 11019)

Everytime after generating next logits 81.4% (8968 out of 11019)

Everytime after generating next logits 81.4% (8968 out of 11019)

Everytime after generating next logits 84.3% (9284 out of 11019)

Everytime after generating next logits 87.1% (9600 out of 11019)

Everytime after generating next logits 87.1% (9600 out of 11019)

Everytime after generating next logits 87.1% (9600 out of 11019)

Everytime after generating next logits 87.1% (9600 out of 11019)

Everytime after generating next logits 90.0% (9920 out of 11019)

Everytime after generating next logits 92.9% (10240 out of 11019)

Everytime after generating next logits 92.9% (10240 out of 11019)

Everytime after generating next logits 92.9% (10240 out of 11019)

Everytime after generating next logits 92.9% (10240 out of 11019)

Everytime after generating next logits 92.9% (10240 out of 11019)

Everytime after generating next logits 95.9% (10564 out of 11019)

Everytime after generating next logits 98.8% (10888 out of 11019)

Everytime after generating next logits 98.8% (10888 out of 11019)

Everytime after generating next logits 98.8% (10888 out of 11019)

Everytime after generating next logits 98.8% (10888 out of 11019)

Everytime after generating next logits 45.3% (4994 out of 11019)

Everytime after generating next logits 48.3% (5322 out of 11019)

Everytime after generating next logits 48.3% (5322 out of 11019)

Everytime after generating next logits 48.3% (5322 out of 11019)

Everytime after generating next logits 48.3% (5322 out of 11019)

Everytime after generating next logits 48.3% (5322 out of 11019)

Everytime after generating next logits 51.3% (5654 out of 11019)

Everytime after generating next logits 54.3% (5986 out of 11019)

Everytime after generating next logits 54.3% (5986 out of 11019)

Everytime after generating next logits 54.3% (5986 out of 11019)

Everytime after generating next logits 54.3% (5986 out of 11019)

Everytime after generating next logits 57.4% (6322 out of 11019)

Everytime after generating next logits 60.4% (6658 out of 11019)

Everytime after generating next logits 60.4% (6658 out of 11019)

Everytime after generating next logits 60.4% (6658 out of 11019)

Everytime after generating next logits 60.4% (6658 out of 11019)

Everytime after generating next logits 60.4% (6658 out of 11019)

Everytime after generating next logits 63.5% (6998 out of 11019)

Everytime after generating next logits 66.6% (7338 out of 11019)

Everytime after generating next logits 66.6% (7338 out of 11019)

Everytime after generating next logits 66.6% (7338 out of 11019)

Everytime after generating next logits 66.6% (7338 out of 11019)

Everytime after generating next logits 69.7% (7682 out of 11019)

Everytime after generating next logits 72.8% (8026 out of 11019)

Everytime after generating next logits 72.8% (8026 out of 11019)

Everytime after generating next logits 72.8% (8026 out of 11019)

Everytime after generating next logits 72.8% (8026 out of 11019)

Everytime after generating next logits 72.8% (8026 out of 11019)

Everytime after generating next logits 76.0% (8374 out of 11019)

Everytime after generating next logits 79.2% (8722 out of 11019)

Everytime after generating next logits 79.2% (8722 out of 11019)

Everytime after generating next logits 79.2% (8722 out of 11019)

Everytime after generating next logits 79.2% (8722 out of 11019)

Everytime after generating next logits 82.3% (9074 out of 11019)

Everytime after generating next logits 85.5% (9426 out of 11019)

Everytime after generating next logits 85.5% (9426 out of 11019)

Everytime after generating next logits 85.5% (9426 out of 11019)

Everytime after generating next logits 85.5% (9426 out of 11019)

Everytime after generating next logits 85.5% (9426 out of 11019)

Everytime after generating next logits 88.8% (9782 out of 11019)

Everytime after generating next logits 92.0% (10138 out of 11019)

Everytime after generating next logits 92.0% (10138 out of 11019)

Everytime after generating next logits 92.0% (10138 out of 11019)

Everytime after generating next logits 92.0% (10138 out of 11019)

Everytime after generating next logits 95.3% (10498 out of 11019)

Everytime after generating next logits 98.5% (10858 out of 11019)

Everytime after generating next logits 98.5% (10858 out of 11019)

Everytime after generating next logits 98.5% (10858 out of 11019)

Everytime after generating next logits 98.5% (10858 out of 11019)

Everytime after generating next logits 46.0% (5066 out of 11019)

Everytime after generating next logits 49.3% (5430 out of 11019)

Everytime after generating next logits 49.3% (5430 out of 11019)

Everytime after generating next logits 49.3% (5430 out of 11019)

Everytime after generating next logits 49.3% (5430 out of 11019)

Everytime after generating next logits 49.3% (5430 out of 11019)

Everytime after generating next logits 52.6% (5798 out of 11019)

Everytime after generating next logits 56.0% (6166 out of 11019)

Everytime after generating next logits 56.0% (6166 out of 11019)

Everytime after generating next logits 56.0% (6166 out of 11019)

Everytime after generating next logits 56.0% (6166 out of 11019)

Everytime after generating next logits 59.3% (6538 out of 11019)

Everytime after generating next logits 62.7% (6910 out of 11019)

Everytime after generating next logits 62.7% (6910 out of 11019)

Everytime after generating next logits 62.7% (6910 out of 11019)

Everytime after generating next logits 62.7% (6910 out of 11019)

Everytime after generating next logits 62.7% (6910 out of 11019)

Everytime after generating next logits 66.1% (7286 out of 11019)

Everytime after generating next logits 69.5% (7662 out of 11019)

Everytime after generating next logits 69.5% (7662 out of 11019)

Everytime after generating next logits 69.5% (7662 out of 11019)

Everytime after generating next logits 69.5% (7662 out of 11019)

Everytime after generating next logits 73.0% (8042 out of 11019)

Everytime after generating next logits 76.4% (8422 out of 11019)

Everytime after generating next logits 76.4% (8422 out of 11019)

Everytime after generating next logits 76.4% (8422 out of 11019)

Everytime after generating next logits 76.4% (8422 out of 11019)

Everytime after generating next logits 76.4% (8422 out of 11019)

Everytime after generating next logits 79.9% (8806 out of 11019)

Everytime after generating next logits 83.4% (9190 out of 11019)

Everytime after generating next logits 83.4% (9190 out of 11019)

Everytime after generating next logits 83.4% (9190 out of 11019)

Everytime after generating next logits 83.4% (9190 out of 11019)

Everytime after generating next logits 86.9% (9578 out of 11019)

Everytime after generating next logits 90.4% (9966 out of 11019)

Everytime after generating next logits 90.4% (9966 out of 11019)

Everytime after generating next logits 90.4% (9966 out of 11019)

Everytime after generating next logits 90.4% (9966 out of 11019)

Everytime after generating next logits 90.4% (9966 out of 11019)

Everytime after generating next logits 94.0% (10358 out of 11019)

Everytime after generating next logits 97.6% (10750 out of 11019)

Everytime after generating next logits 97.6% (10750 out of 11019)

Everytime after generating next logits 97.6% (10750 out of 11019)

Everytime after generating next logits 97.6% (10750 out of 11019)

Everytime after generating next logits 46.6% (5130 out of 11019)

Everytime after generating next logits 50.1% (5526 out of 11019)

Everytime after generating next logits 50.1% (5526 out of 11019)

Everytime after generating next logits 50.1% (5526 out of 11019)

Everytime after generating next logits 50.1% (5526 out of 11019)

Everytime after generating next logits 50.1% (5526 out of 11019)

Everytime after generating next logits 53.8% (5926 out of 11019)

Everytime after generating next logits 57.4% (6326 out of 11019)

Everytime after generating next logits 57.4% (6326 out of 11019)

Everytime after generating next logits 57.4% (6326 out of 11019)

Everytime after generating next logits 57.4% (6326 out of 11019)

Everytime after generating next logits 61.1% (6730 out of 11019)

Everytime after generating next logits 64.7% (7134 out of 11019)

Everytime after generating next logits 64.7% (7134 out of 11019)

Everytime after generating next logits 64.7% (7134 out of 11019)

Everytime after generating next logits 64.7% (7134 out of 11019)

Everytime after generating next logits 64.7% (7134 out of 11019)

Everytime after generating next logits 68.4% (7542 out of 11019)

Everytime after generating next logits 72.1% (7950 out of 11019)

Everytime after generating next logits 72.1% (7950 out of 11019)

Everytime after generating next logits 72.1% (7950 out of 11019)

Everytime after generating next logits 72.1% (7950 out of 11019)

Everytime after generating next logits 75.9% (8362 out of 11019)

Everytime after generating next logits 79.6% (8774 out of 11019)

Everytime after generating next logits 79.6% (8774 out of 11019)

Everytime after generating next logits 79.6% (8774 out of 11019)

Everytime after generating next logits 79.6% (8774 out of 11019)

Everytime after generating next logits 83.4% (9190 out of 11019)

Everytime after generating next logits 87.2% (9606 out of 11019)

Everytime after generating next logits 87.2% (9606 out of 11019)

Everytime after generating next logits 87.2% (9606 out of 11019)

Everytime after generating next logits 87.2% (9606 out of 11019)

Everytime after generating next logits 87.2% (9606 out of 11019)

Everytime after generating next logits 91.0% (10026 out of 11019)

Everytime after generating next logits 94.8% (10446 out of 11019)

Everytime after generating next logits 94.8% (10446 out of 11019)

Everytime after generating next logits 94.8% (10446 out of 11019)

Everytime after generating next logits 94.8% (10446 out of 11019)

Everytime after generating next logits 98.6% (10870 out of 11019)

Everytime after generating next logits 47.1% (5190 out of 11019)

Everytime after generating next logits 47.1% (5190 out of 11019)

Everytime after generating next logits 47.1% (5190 out of 11019)

Everytime after generating next logits 47.1% (5190 out of 11019)

Everytime after generating next logits 47.1% (5190 out of 11019)

Everytime after generating next logits 51.0% (5618 out of 11019)

Everytime after generating next logits 54.9% (6046 out of 11019)

Everytime after generating next logits 54.9% (6046 out of 11019)

Everytime after generating next logits 54.9% (6046 out of 11019)

Everytime after generating next logits 54.9% (6046 out of 11019)

Everytime after generating next logits 58.8% (6478 out of 11019)

Everytime after generating next logits 62.7% (6910 out of 11019)

Everytime after generating next logits 62.7% (6910 out of 11019)

Everytime after generating next logits 62.7% (6910 out of 11019)

Everytime after generating next logits 62.7% (6910 out of 11019)

Everytime after generating next logits 62.7% (6910 out of 11019)

Everytime after generating next logits 66.7% (7346 out of 11019)

Everytime after generating next logits 70.6% (7782 out of 11019)

Everytime after generating next logits 70.6% (7782 out of 11019)

Everytime after generating next logits 70.6% (7782 out of 11019)

Everytime after generating next logits 70.6% (7782 out of 11019)

Everytime after generating next logits 74.6% (8222 out of 11019)

Everytime after generating next logits 78.6% (8662 out of 11019)

Everytime after generating next logits 78.6% (8662 out of 11019)

Everytime after generating next logits 78.6% (8662 out of 11019)

Everytime after generating next logits 78.6% (8662 out of 11019)

Everytime after generating next logits 78.6% (8662 out of 11019)

Everytime after generating next logits 82.6% (9106 out of 11019)

Everytime after generating next logits 86.7% (9550 out of 11019)

Everytime after generating next logits 86.7% (9550 out of 11019)

Everytime after generating next logits 86.7% (9550 out of 11019)

Everytime after generating next logits 86.7% (9550 out of 11019)

Everytime after generating next logits 90.7% (9998 out of 11019)

Everytime after generating next logits 94.8% (10446 out of 11019)

Everytime after generating next logits 94.8% (10446 out of 11019)

Everytime after generating next logits 94.8% (10446 out of 11019)

Everytime after generating next logits 94.8% (10446 out of 11019)

Everytime after generating next logits 94.8% (10446 out of 11019)

Everytime after generating next logits 98.9% (10898 out of 11019)

Everytime after generating next logits 47.6% (5246 out of 11019)

Everytime after generating next logits 47.6% (5246 out of 11019)

Everytime after generating next logits 47.6% (5246 out of 11019)

Everytime after generating next logits 47.6% (5246 out of 11019)

Everytime after generating next logits 51.7% (5702 out of 11019)

Everytime after generating next logits 55.9% (6158 out of 11019)

Everytime after generating next logits 55.9% (6158 out of 11019)

Everytime after generating next logits 55.9% (6158 out of 11019)

Everytime after generating next logits 55.9% (6158 out of 11019)

Everytime after generating next logits 60.1% (6618 out of 11019)

Everytime after generating next logits 64.2% (7078 out of 11019)

Everytime after generating next logits 64.2% (7078 out of 11019)

Everytime after generating next logits 64.2% (7078 out of 11019)

Everytime after generating next logits 64.2% (7078 out of 11019)

Everytime after generating next logits 64.2% (7078 out of 11019)

Everytime after generating next logits 68.4% (7542 out of 11019)

Everytime after generating next logits 72.7% (8006 out of 11019)

Everytime after generating next logits 72.7% (8006 out of 11019)

Everytime after generating next logits 72.7% (8006 out of 11019)

Everytime after generating next logits 72.7% (8006 out of 11019)

Everytime after generating next logits 76.9% (8474 out of 11019)

Everytime after generating next logits 81.2% (8942 out of 11019)

Everytime after generating next logits 81.2% (8942 out of 11019)

Everytime after generating next logits 81.2% (8942 out of 11019)

Everytime after generating next logits 81.2% (8942 out of 11019)

Everytime after generating next logits 81.2% (8942 out of 11019)

Everytime after generating next logits 85.4% (9414 out of 11019)

Everytime after generating next logits 89.7% (9886 out of 11019)

Everytime after generating next logits 89.7% (9886 out of 11019)

Everytime after generating next logits 89.7% (9886 out of 11019)

Everytime after generating next logits 89.7% (9886 out of 11019)

Everytime after generating next logits 94.0% (10362 out of 11019)

Everytime after generating next logits 98.4% (10838 out of 11019)

Everytime after generating next logits 98.4% (10838 out of 11019)

Everytime after generating next logits 98.4% (10838 out of 11019)

Everytime after generating next logits 98.4% (10838 out of 11019)

Everytime after generating next logits 98.4% (10838 out of 11019)

Everytime after generating next logits 48.1% (5298 out of 11019)

Everytime after generating next logits 52.4% (5778 out of 11019)

Everytime after generating next logits 52.4% (5778 out of 11019)

Everytime after generating next logits 52.4% (5778 out of 11019)

Everytime after generating next logits 52.4% (5778 out of 11019)

Everytime after generating next logits 56.8% (6262 out of 11019)

Everytime after generating next logits 61.2% (6746 out of 11019)

Everytime after generating next logits 61.2% (6746 out of 11019)

Everytime after generating next logits 61.2% (6746 out of 11019)

Everytime after generating next logits 61.2% (6746 out of 11019)

Everytime after generating next logits 61.2% (6746 out of 11019)

Everytime after generating next logits 65.7% (7234 out of 11019)

Everytime after generating next logits 70.1% (7722 out of 11019)

Everytime after generating next logits 70.1% (7722 out of 11019)

Everytime after generating next logits 70.1% (7722 out of 11019)

Everytime after generating next logits 70.1% (7722 out of 11019)

Everytime after generating next logits 74.5% (8214 out of 11019)

Everytime after generating next logits 79.0% (8706 out of 11019)

Everytime after generating next logits 79.0% (8706 out of 11019)

Everytime after generating next logits 79.0% (8706 out of 11019)

Everytime after generating next logits 79.0% (8706 out of 11019)

Everytime after generating next logits 79.0% (8706 out of 11019)

Everytime after generating next logits 83.5% (9202 out of 11019)

Everytime after generating next logits 88.0% (9698 out of 11019)

Everytime after generating next logits 88.0% (9698 out of 11019)

Everytime after generating next logits 88.0% (9698 out of 11019)

Everytime after generating next logits 88.0% (9698 out of 11019)

Everytime after generating next logits 92.5% (10198 out of 11019)

Everytime after generating next logits 97.1% (10698 out of 11019)

Everytime after generating next logits 97.1% (10698 out of 11019)

Everytime after generating next logits 97.1% (10698 out of 11019)

Everytime after generating next logits 97.1% (10698 out of 11019)

Everytime after generating next logits 48.5% (5346 out of 11019)

Everytime after generating next logits 53.1% (5850 out of 11019)

Everytime after generating next logits 53.1% (5850 out of 11019)

Everytime after generating next logits 53.1% (5850 out of 11019)

Everytime after generating next logits 53.1% (5850 out of 11019)

Everytime after generating next logits 53.1% (5850 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 62.3% (6866 out of 11019)

Everytime after generating next logits 62.3% (6866 out of 11019)

Everytime after generating next logits 62.3% (6866 out of 11019)

Everytime after generating next logits 62.3% (6866 out of 11019)

Everytime after generating next logits 67.0% (7378 out of 11019)

Everytime after generating next logits 71.6% (7890 out of 11019)

Everytime after generating next logits 71.6% (7890 out of 11019)

Everytime after generating next logits 71.6% (7890 out of 11019)

Everytime after generating next logits 71.6% (7890 out of 11019)

Everytime after generating next logits 71.6% (7890 out of 11019)

Everytime after generating next logits 76.3% (8406 out of 11019)

Everytime after generating next logits 81.0% (8922 out of 11019)

Everytime after generating next logits 81.0% (8922 out of 11019)

Everytime after generating next logits 81.0% (8922 out of 11019)

Everytime after generating next logits 81.0% (8922 out of 11019)

Everytime after generating next logits 85.7% (9442 out of 11019)

Everytime after generating next logits 90.4% (9962 out of 11019)

Everytime after generating next logits 90.4% (9962 out of 11019)

Everytime after generating next logits 90.4% (9962 out of 11019)

Everytime after generating next logits 90.4% (9962 out of 11019)

Everytime after generating next logits 90.4% (9962 out of 11019)

Everytime after generating next logits 95.2% (10486 out of 11019)

Everytime after generating next logits 99.9% (11010 out of 11019)

Everytime after generating next logits 99.9% (11010 out of 11019)

Everytime after generating next logits 99.9% (11010 out of 11019)

Everytime after generating next logits 99.9% (11010 out of 11019)

Everytime after generating next logits 49.0% (5394 out of 11019)

Everytime after generating next logits 53.7% (5922 out of 11019)

Everytime after generating next logits 53.7% (5922 out of 11019)

Everytime after generating next logits 53.7% (5922 out of 11019)

Everytime after generating next logits 53.7% (5922 out of 11019)

Everytime after generating next logits 53.7% (5922 out of 11019)

Everytime after generating next logits 58.6% (6454 out of 11019)

Everytime after generating next logits 63.4% (6986 out of 11019)

Everytime after generating next logits 63.4% (6986 out of 11019)

Everytime after generating next logits 63.4% (6986 out of 11019)

Everytime after generating next logits 63.4% (6986 out of 11019)

Everytime after generating next logits 68.3% (7522 out of 11019)

Everytime after generating next logits 73.1% (8058 out of 11019)

Everytime after generating next logits 73.1% (8058 out of 11019)

Everytime after generating next logits 73.1% (8058 out of 11019)

Everytime after generating next logits 73.1% (8058 out of 11019)

Everytime after generating next logits 73.1% (8058 out of 11019)

Everytime after generating next logits 78.0% (8598 out of 11019)

Everytime after generating next logits 82.9% (9138 out of 11019)

Everytime after generating next logits 82.9% (9138 out of 11019)

Everytime after generating next logits 82.9% (9138 out of 11019)

Everytime after generating next logits 82.9% (9138 out of 11019)

Everytime after generating next logits 87.9% (9682 out of 11019)

Everytime after generating next logits 92.8% (10226 out of 11019)

Everytime after generating next logits 92.8% (10226 out of 11019)

Everytime after generating next logits 92.8% (10226 out of 11019)

Everytime after generating next logits 92.8% (10226 out of 11019)

Everytime after generating next logits 97.8% (10774 out of 11019)

Everytime after generating next logits 49.4% (5438 out of 11019)

Everytime after generating next logits 49.4% (5438 out of 11019)

Everytime after generating next logits 49.4% (5438 out of 11019)

Everytime after generating next logits 49.4% (5438 out of 11019)

Everytime after generating next logits 49.4% (5438 out of 11019)

Everytime after generating next logits 54.4% (5990 out of 11019)

Everytime after generating next logits 59.4% (6542 out of 11019)

Everytime after generating next logits 59.4% (6542 out of 11019)

Everytime after generating next logits 59.4% (6542 out of 11019)

Everytime after generating next logits 59.4% (6542 out of 11019)

Everytime after generating next logits 64.4% (7098 out of 11019)

Everytime after generating next logits 69.5% (7654 out of 11019)

Everytime after generating next logits 69.5% (7654 out of 11019)

Everytime after generating next logits 69.5% (7654 out of 11019)

Everytime after generating next logits 69.5% (7654 out of 11019)

Everytime after generating next logits 69.5% (7654 out of 11019)

Everytime after generating next logits 74.5% (8214 out of 11019)

Everytime after generating next logits 79.6% (8774 out of 11019)

Everytime after generating next logits 79.6% (8774 out of 11019)

Everytime after generating next logits 79.6% (8774 out of 11019)

Everytime after generating next logits 79.6% (8774 out of 11019)

Everytime after generating next logits 84.7% (9338 out of 11019)

Everytime after generating next logits 89.9% (9902 out of 11019)

Everytime after generating next logits 89.9% (9902 out of 11019)

Everytime after generating next logits 89.9% (9902 out of 11019)

Everytime after generating next logits 89.9% (9902 out of 11019)

Everytime after generating next logits 89.9% (9902 out of 11019)

Everytime after generating next logits 95.0% (10470 out of 11019)

Everytime after generating next logits 49.7% (5478 out of 11019)

Everytime after generating next logits 49.7% (5478 out of 11019)

Everytime after generating next logits 49.7% (5478 out of 11019)

Everytime after generating next logits 49.7% (5478 out of 11019)

Everytime after generating next logits 54.9% (6050 out of 11019)

Everytime after generating next logits 60.1% (6622 out of 11019)

Everytime after generating next logits 60.1% (6622 out of 11019)

Everytime after generating next logits 60.1% (6622 out of 11019)

Everytime after generating next logits 60.1% (6622 out of 11019)

Everytime after generating next logits 60.1% (6622 out of 11019)

Everytime after generating next logits 65.3% (7198 out of 11019)

Everytime after generating next logits 70.6% (7774 out of 11019)

Everytime after generating next logits 70.6% (7774 out of 11019)

Everytime after generating next logits 70.6% (7774 out of 11019)

Everytime after generating next logits 70.6% (7774 out of 11019)

Everytime after generating next logits 75.8% (8354 out of 11019)

Everytime after generating next logits 81.1% (8934 out of 11019)

Everytime after generating next logits 81.1% (8934 out of 11019)

Everytime after generating next logits 81.1% (8934 out of 11019)

Everytime after generating next logits 81.1% (8934 out of 11019)

Everytime after generating next logits 81.1% (8934 out of 11019)

Everytime after generating next logits 86.4% (9518 out of 11019)

Everytime after generating next logits 91.7% (10102 out of 11019)

Everytime after generating next logits 91.7% (10102 out of 11019)

Everytime after generating next logits 91.7% (10102 out of 11019)

Everytime after generating next logits 91.7% (10102 out of 11019)

Everytime after generating next logits 97.0% (10690 out of 11019)

Everytime after generating next logits 50.1% (5518 out of 11019)

Everytime after generating next logits 50.1% (5518 out of 11019)

Everytime after generating next logits 50.1% (5518 out of 11019)

Everytime after generating next logits 50.1% (5518 out of 11019)

Everytime after generating next logits 55.4% (6110 out of 11019)

Everytime after generating next logits 60.8% (6702 out of 11019)

Everytime after generating next logits 60.8% (6702 out of 11019)

Everytime after generating next logits 60.8% (6702 out of 11019)

Everytime after generating next logits 60.8% (6702 out of 11019)

Everytime after generating next logits 60.8% (6702 out of 11019)

Everytime after generating next logits 66.2% (7298 out of 11019)

Everytime after generating next logits 71.6% (7894 out of 11019)

Everytime after generating next logits 71.6% (7894 out of 11019)

Everytime after generating next logits 71.6% (7894 out of 11019)

Everytime after generating next logits 71.6% (7894 out of 11019)

Everytime after generating next logits 77.1% (8494 out of 11019)

Everytime after generating next logits 82.5% (9094 out of 11019)
Deepwater Horizon at Mexico has more successful company at Mexico latest latest sea capture latest latest latest latest sea capture latest latest latest latest latest latest latest latest sea capture latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest sea capture latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest sea capture latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest sea capture latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest sea capture latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest sea capture latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest sea capture latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest . latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest . latest latest latest latest latest latest latest latest latest latest latest latest latest latest . latest sea capture latest latest latest latest latest latest latest latest latest . latest latest latest latest latest latest . latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest . latest sea capture latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest sea capture latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest . latest latest latest latest . latest . latest latest latest latest latest latest latest latest . latest latest latest latest latest . latest sea capture latest latest latest latest latest latest latest latest latest latest latest latest latest latest latest . latest sea capture latest latest latest latest latest latest . latest latest latest latest . latest . latest sea capture latest latest latest latest latest . latest latest latest latest . latest . latest latest . latest . latest . latest . latest . latest . latest . latest sea capture latest . latest latest latest latest latest . latest latest . latest . latest sea capture latest latest latest latest . latest latest . latest . latest . latest sea capture latest . latest latest latest latest . latest latest latest latest latest latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest sea capture latest . latest latest . latest . latest . latest . latest . latest . latest . latest . latest . latest sea capture latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest sea capture latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . latest . oil leak of officials installed latest . oil leak of officials installed latest . latest . latest .
iter = 2 reward = 30
final_logits =  tensor([[[ 0.0498,  5.0135,  0.5937,  ..., -3.0482,  0.9098,  0.5087],
         [ 0.0114,  3.5393, -0.0502,  ..., -4.4207, -0.9012, -1.2905],
         [-0.0700,  2.8368, -0.4276,  ..., -3.5520, -1.3607, -1.2819],
         ...,
         [-0.0398,  6.3513, -0.1239,  ..., -4.5784, -0.3180, -0.9766],
         [ 0.0340,  9.0149,  0.2993,  ..., -1.1611, -0.1865, -1.2749],
         [-0.0381, 10.9760,  0.2631,  ..., -1.4379, -0.8186, -0.8474]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(15.3826, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(443.6322, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)

Everytime after generating next logits 57.7% (6358 out of 11019)
Deepwater Horizon at Mexico has more successful company at Mexico .
iter = 3 reward = 3
final_logits =  tensor([[[ 3.4721e-02,  5.1714e+00,  6.3884e-01,  ..., -2.7672e+00,
           6.1775e-01,  1.9152e-01],
         [ 5.5568e-03,  3.4804e+00, -6.5244e-02,  ..., -4.1743e+00,
          -9.6652e-01, -1.4602e+00],
         [-7.0272e-02,  2.8632e+00, -3.8712e-01,  ..., -3.1639e+00,
          -1.1376e+00, -9.8501e-01],
         ...,
         [-3.4216e-02,  3.8132e+00, -4.8792e-01,  ..., -3.6898e+00,
          -7.3825e-01, -1.2253e+00],
         [ 6.7471e-02,  3.4174e+00,  3.1948e-01,  ..., -1.5739e+00,
          -5.5161e-02,  7.5508e-01],
         [ 2.8901e-02,  1.2422e+01,  3.6873e-01,  ..., -2.1959e+00,
           7.1833e-02,  5.2831e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.4440, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.9702, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)
Deepwater Horizon at Mexico has more successful company at Mexico .
iter = 4 reward = 3
final_logits =  tensor([[[ 0.0185,  4.8996,  0.5718,  ..., -1.5147,  0.2871, -0.1197],
         [-0.0138,  3.5122, -0.1123,  ..., -4.0105, -0.7768, -1.5993],
         [-0.0834,  2.9396, -0.4582,  ..., -3.0487, -0.6820, -1.3240],
         ...,
         [-0.0561,  4.1119, -0.5010,  ..., -3.9809, -0.4017, -0.8925],
         [ 0.0432,  4.5188,  0.3664,  ..., -1.8899, -0.1870,  0.9211],
         [ 0.0154, 12.7549,  0.4166,  ..., -1.8749,  0.3184,  0.1622]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.0938, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(33.5193, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  3
3
{'input_ids': tensor([[[16036,   762, 14567,   114,   711,   113,   110,   105,  2111,  1057,
            395,  1034,   139, 21549, 16036,   762, 14567,   115,   109,  7175,
            113,  3064,   148, 50233,   109,  4170,   160,  3079,   644,  2175,
            110,   108,   155,   109,  1319,  1977,   113, 11461,   649,   109,
          14567,   140,   109,   711,   113,  1025, 46366,   110,   107,  1084,
          44907, 41534,   140,  1977,   113, 11461,  1034,   116,   655,   260,
            135,  5109,   112,  3390,   111,   148,   243,   223,   644,   524,
            632,   112,   823,   114,   110,   105, 30493,   824,   113,   109,
           2379,   110,   107, 16837,   125,   116,  1086,   734,   112,   145,
           1321,  1110,   299,   114,   300,   121,  1704,  6030,   112, 11881,
          13922,   110,   152,   325,   117,   461,   762,   297,   113,   109,
            951,   110,   108,   132,   109,   575,   110,   152,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1]]])}

Before Sampling 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)
Gulf-Mexico oil fuels of 2005 disaster result of 2005 . American debate has sharpened judgement on Gulf-John of Mexico .
iter = 0 reward = 2
final_logits =  tensor([[[ 0.0618,  4.1956,  0.6742,  ..., -0.3712,  0.3315, -1.9367],
         [-0.0519,  3.2229, -0.3463,  ..., -1.9207, -0.7712, -1.1114],
         [ 0.0422,  4.3640,  1.2448,  ...,  1.1657, -0.7333,  0.6282],
         ...,
         [-0.0755,  3.5733, -0.5836,  ..., -2.2527, -0.3559, -3.7246],
         [ 0.0377,  4.6998, -0.1457,  ..., -0.8844, -0.6184, -0.5723],
         [ 0.0184, 12.3713, -0.2391,  ..., -0.5593, -0.1492, -1.7213]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-3.5538, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(18.4533, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)
Gulf-John of Mexico's oil fuels of 2005 has disastrous result of poor fuel of 2005 . American debate has sharpened sharpened up on issue of oil-term solution .
iter = 1 reward = 1
final_logits =  tensor([[[ 4.9368e-02,  4.3274e+00,  5.5313e-01,  ..., -1.4051e+00,
           1.7904e-01, -1.0574e+00],
         [-6.4182e-02,  2.9864e+00, -3.1302e-01,  ..., -1.8762e+00,
          -1.0158e+00, -1.0105e+00],
         [ 5.0622e-02,  4.2350e+00,  1.3419e+00,  ...,  1.1658e+00,
          -6.2700e-01,  9.8624e-01],
         ...,
         [-5.9019e-02,  4.4928e+00, -6.1226e-01,  ..., -1.5469e+00,
          -4.3693e-01, -1.3631e+00],
         [ 7.4989e-02,  5.0671e+00,  6.6182e-01,  ..., -9.4710e-01,
           1.2092e+00,  1.5178e+00],
         [ 2.9887e-03,  1.2788e+01, -3.4788e-02,  ..., -2.0166e-01,
           3.6370e-01, -4.2788e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-7.7366, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(43.8153, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)
BP spill is a sustainable business, Shellmeister says, Shell says .
iter = 2 reward = 3
final_logits =  tensor([[[ 5.4015e-02,  4.8000e+00,  3.5176e-01,  ..., -1.2788e+00,
          -7.2283e-01, -1.7617e+00],
         [-5.0185e-02,  3.7314e+00, -3.4913e-01,  ..., -3.4501e+00,
          -1.3809e+00, -2.5592e+00],
         [-1.3977e-01,  2.5499e+00, -4.6217e-01,  ..., -1.7864e+00,
          -5.0960e-01, -1.3089e+00],
         ...,
         [-8.7037e-03,  5.6930e+00, -2.3198e-01,  ..., -1.7341e+00,
          -1.1049e-01, -4.6268e-01],
         [ 8.5435e-02,  4.2947e+00,  2.6233e-01,  ..., -1.9794e+00,
          -5.2166e-01,  7.3958e-01],
         [-1.4219e-02,  1.0714e+01, -2.3811e-01,  ..., -2.6782e+00,
          -1.1524e+00, -1.1767e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-8.2701, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(88.6501, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)

Everytime after generating next logits 40.7% (4482 out of 11019)
Gulf-Mexico oil spill off off the coast of 2005 . American oil-term solution has guarded part of the disaster result of disaster . Shell says energy strategies and the bad business are ready .
iter = 3 reward = 6
final_logits =  tensor([[[ 6.6502e-02,  3.8599e+00,  6.4186e-01,  ..., -2.1171e+00,
          -1.6231e-01, -2.5240e+00],
         [-4.1624e-02,  2.5425e+00, -2.6093e-01,  ..., -2.5395e+00,
          -1.6956e+00, -1.4458e+00],
         [ 5.7169e-02,  2.6093e+00,  1.6676e+00,  ...,  9.9791e-01,
          -7.2035e-01,  8.4453e-01],
         ...,
         [-3.5696e-02,  5.4465e+00, -7.8299e-01,  ..., -1.9898e+00,
          -8.7365e-01, -1.3950e+00],
         [ 6.9229e-02,  3.3363e+00, -4.0277e-02,  ..., -1.1381e+00,
           6.8273e-03,  2.7980e-01],
         [ 3.4380e-02,  1.3225e+01, -4.3136e-01,  ..., -8.4477e-01,
          -9.6764e-01, -1.8389e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.2006, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(33.9340, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)
fossil-term solution of Mexico's oil spill off off the coast of Mexico . Shell says company is ready to make sustainable business .
iter = 4 reward = 6
final_logits =  tensor([[[ 6.5658e-02,  4.4900e+00,  5.7345e-01,  ..., -1.4196e+00,
          -7.3481e-01, -2.4850e+00],
         [-3.4823e-02,  3.9603e+00, -3.2909e-01,  ..., -1.5560e+00,
          -2.8736e-01, -1.7656e+00],
         [ 5.6278e-02,  5.0555e+00,  1.3892e+00,  ...,  3.3729e+00,
           5.2818e-01,  4.0435e-01],
         ...,
         [-7.1739e-02,  5.7040e+00, -6.3896e-01,  ..., -2.5851e+00,
           3.9562e-01, -1.9672e+00],
         [ 4.7903e-02,  3.6441e+00,  6.1998e-02,  ..., -2.0064e+00,
          -1.3265e-01,  8.7590e-01],
         [ 6.4016e-04,  1.3240e+01, -3.8596e-01,  ..., -4.2633e-01,
          -7.7264e-01, -1.3460e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-3.8122, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(45.7552, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  6
6
{'input_ids': tensor([[[ 1346,  6661,   118,   762, 14567,   945,   139,  2800,  1728,   112,
           2555,   110,   105, 41638, 16837,  1003,   121,   768,  1739,   120,
            133,   174,   263,   115,   109,  7175,   113,  3064,   139,  1346,
           6661,  2800,   110,   108,   229,   606,   118,  6957,   109,   808,
          70959,   503,   110,   108,   148,  2365,   114,  3662, 13602,   604,
            762,  1003,   121,   768,  1459,   110,   107,   139,  2800,   110,
            108,   162,  1653,   120,   203,  1962,  2560,   117,   110,   105,
            112,   650,   160,  8695, 33237,   118,   109,  1280,   113,  7633,
          16837,  1487,   203,   807,  4086,   134,   114,  1833,  1792,   115,
           1741,  3710,   110,   107,   182,   117,   203,  6932,   110,   105,
            698,  9501,  1702, 16837,   110,   107,  1810,  1518,   137,  2337,
            118,   109,  1702,   430,   960,  2651,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1]]])}

Before Sampling 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)
DC oil prize oil solution foundation announced April 2011 prize breakthroughs at DC oil solution .
iter = 0 reward = 3
final_logits =  tensor([[[ 0.0636,  3.2452,  0.5626,  ..., -2.6181,  1.1730, -1.7807],
         [ 0.0549,  4.0974,  0.0189,  ..., -2.2915, -1.5116, -2.1670],
         [-0.0512,  2.5296, -0.0484,  ..., -2.7568, -1.2613, -0.7003],
         ...,
         [-0.0140,  3.5964, -0.4953,  ..., -4.5322, -1.5283, -2.9641],
         [ 0.0813,  3.3447, -0.0176,  ..., -0.6624, -0.1238,  0.2201],
         [ 0.0533, 11.8960,  0.4100,  ..., -1.3473, -0.3056, -1.9925]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.3958, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(13.5833, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)
Gulf Prize oil prize prize prize winner at DC Foundation announced latest breakthroughs .
iter = 1 reward = 2
final_logits =  tensor([[[ 0.0781,  3.1011,  0.5265,  ..., -3.5336,  0.4453, -1.8634],
         [-0.0161,  3.1961, -0.2448,  ..., -4.2271, -1.4820, -1.0119],
         [-0.0419,  2.1873, -0.3641,  ..., -4.2105, -1.0069, -0.2555],
         ...,
         [-0.1128,  3.6170, -0.4033,  ..., -4.1649, -0.9737,  0.0391],
         [ 0.0639,  2.3875,  0.0829,  ..., -0.4960,  0.3934,  0.6164],
         [ 0.0592, 11.9054,  0.2849,  ..., -1.6605, -0.8662, -1.0169]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.9263, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(697.6136, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)
DC oil solution foundation announced latest breakthroughs of oil solutions at Gulf Prize oil solutions .
iter = 2 reward = 4
final_logits =  tensor([[[ 0.0892,  3.4957,  0.4848,  ..., -3.2436,  0.1846, -1.9846],
         [ 0.0713,  4.1390, -0.0823,  ..., -2.6019, -1.2007, -2.5457],
         [-0.0126,  2.6643,  0.0263,  ..., -2.4083, -1.1035, -0.3638],
         ...,
         [-0.0323,  2.3568, -0.6024,  ..., -4.6607, -0.8331, -2.0351],
         [ 0.0741,  2.1056,  0.0717,  ..., -0.2718, -0.1997,  0.5023],
         [ 0.0631, 11.8151,  0.1682,  ..., -1.6024, -0.8667, -1.5491]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(7.0725, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(490.9143, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)
Gulf Prize oil solutions foundation announced latest breakthroughs in 2011 conference .
iter = 3 reward = 2
final_logits =  tensor([[[ 0.0751,  2.8719,  0.5431,  ..., -3.4974,  0.3059, -1.9573],
         [-0.0203,  3.0175, -0.2259,  ..., -3.9343, -1.4072, -1.2535],
         [-0.0447,  1.9342, -0.2916,  ..., -4.0976, -0.9867, -0.3022],
         ...,
         [-0.1225,  3.7443, -0.0341,  ..., -4.5496, -2.3836,  1.2089],
         [ 0.0610,  1.6666,  0.1363,  ..., -0.3362,  0.2667,  0.6691],
         [ 0.0578, 11.7494,  0.2122,  ..., -1.8949, -0.6579, -1.1120]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.3813, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(56.3132, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)
Gulf Prize oil solutions foundation announced latest breakthroughs in 2011 conference .
iter = 4 reward = 2
final_logits =  tensor([[[ 0.0680,  2.9151,  0.5514,  ..., -3.2792,  0.2362, -1.6826],
         [-0.0240,  3.0350, -0.2386,  ..., -3.8391, -1.4373, -1.1860],
         [-0.0506,  1.9327, -0.2903,  ..., -4.0790, -1.1098, -0.2317],
         ...,
         [-0.1239,  3.6532, -0.0543,  ..., -4.4291, -2.3836,  1.4994],
         [ 0.0588,  1.6469,  0.1317,  ..., -0.2996,  0.2909,  0.7675],
         [ 0.0504, 11.7540,  0.1780,  ..., -1.8231, -0.6463, -1.0175]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.3556, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(239.6293, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  2
2
{'input_ids': tensor([[[ 3211,  1307,  2652,  2882,  2033,   134,   280, 85536, 17403,  4596,
            398, 16036,  1034,   116, 11599,  2664,   109,   519,   113,   114,
           6033,   124,  9134,  9600,   110,   108,   109, 12220,   113, 14645,
           9727,   135,   109,   762, 14567,   115,   109,  7175,   113,  3064,
            148,  9380,   164,   115,   114,  2043, 22369,   115, 10792,   110,
            107,   353,  1761,  7690,   355,  1854,   199,   109, 18191,   113,
          14645,   246,   129,  8626,   122,   110,   107,  9903, 90966,  1574,
            135,   351,   859,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)

Everytime after generating next logits 40.7% (4480 out of 11019)
Seven oil rivals at Mexico have been sued over oil rivals at the spill .
iter = 0 reward = 4
final_logits =  tensor([[[-1.0127e-04,  4.1337e+00,  6.8920e-01,  ..., -3.4051e-01,
          -1.3536e+00,  5.5842e-01],
         [-1.4827e-01,  2.5773e+00,  1.3368e-01,  ..., -9.3204e-01,
           8.5526e-01, -1.7718e+00],
         [-1.3652e-01,  3.6211e+00,  1.6256e-02,  ..., -2.4445e+00,
          -2.0495e-01, -1.0296e+00],
         ...,
         [-2.0297e-01,  2.0336e+00, -9.4026e-01,  ..., -2.5522e+00,
           8.6346e-02, -2.0462e+00],
         [ 2.5966e-02,  1.8887e+00,  2.6394e-01,  ..., -5.8494e-02,
           3.3306e-01,  3.9201e-01],
         [-2.0276e-02,  1.1706e+01,  3.9058e-01,  ...,  1.4424e+00,
          -7.2261e-01, -1.4128e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-6.9631, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(34.8659, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)

Everytime after generating next logits 40.6% (4470 out of 11019)
Seven oil rivals at Mexico have been sued by seven oil rivals .
iter = 1 reward = 4
final_logits =  tensor([[[-1.8600e-03,  4.3930e+00,  6.8336e-01,  ..., -3.2311e-01,
          -1.3594e+00,  5.3602e-01],
         [-1.4917e-01,  2.5719e+00,  1.2824e-01,  ..., -9.4536e-01,
           8.6409e-01, -1.8528e+00],
         [-1.3771e-01,  3.5255e+00,  1.8724e-02,  ..., -2.4954e+00,
          -1.6838e-01, -1.0565e+00],
         ...,
         [-1.3235e-01,  3.9352e+00, -5.4332e-01,  ..., -2.1976e+00,
           6.6987e-01, -2.0885e+00],
         [-5.3780e-03,  3.7667e+00,  6.4629e-01,  ..., -2.3295e-01,
           1.1478e+00,  1.3063e+00],
         [-6.8949e-02,  1.1539e+01,  4.3017e-01,  ...,  7.9495e-01,
          -1.3456e-01, -1.7477e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-6.3372, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(31.2627, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)

Everytime after generating next logits 40.6% (4476 out of 11019)
Lawsuits in Idaho have washed federal reports of seven oil rivals at Mexico .
iter = 2 reward = 4
final_logits =  tensor([[[ 1.4325e-03,  4.6980e+00,  6.5787e-01,  ..., -3.3159e-01,
          -1.3698e+00,  4.9034e-01],
         [-1.3955e-01,  2.6644e+00, -2.4091e-01,  ..., -3.2248e+00,
           2.2655e+00,  5.3717e-01],
         [-9.7548e-02,  3.8303e+00, -4.2310e-01,  ..., -1.7631e+00,
           8.4028e-01, -2.8726e+00],
         ...,
         [-1.2139e-01,  3.9564e+00, -5.8339e-01,  ..., -2.2296e+00,
          -7.7260e-01, -2.7868e+00],
         [ 3.3711e-02,  2.7137e+00,  4.3628e-01,  ..., -4.3852e-01,
           7.7741e-01,  5.1760e-01],
         [-2.4539e-02,  1.2048e+01,  2.4834e-01,  ...,  8.8715e-01,
          -4.4777e-01, -2.3365e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.7178, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(21.3632, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)

Everytime after generating next logits 40.6% (4478 out of 11019)
On July 7 oil rivals at federal tide of Gulf oil rivals at Mexico . Seven lawsuits arising from oil rivals at federal tide of Mexico wash .
iter = 3 reward = 7
final_logits =  tensor([[[ 7.5548e-03,  4.8536e+00,  6.1284e-01,  ..., -4.1025e-01,
          -1.4498e+00,  3.0713e-01],
         [-5.5168e-02,  2.0028e+00,  2.1350e-01,  ...,  3.9507e-01,
          -1.1375e+00, -3.1753e-01],
         [ 2.9008e-03,  1.7936e+00,  1.3730e-02,  ..., -6.8477e-01,
          -1.6371e+00, -1.3076e+00],
         ...,
         [-1.3567e-01,  3.9314e+00, -3.2377e-01,  ...,  4.6119e-01,
          -1.8459e-01, -2.3641e+00],
         [ 3.3796e-03,  3.6390e+00,  3.7595e-01,  ..., -5.1314e-01,
          -1.7260e-01,  1.3556e-01],
         [ 4.0829e-03,  1.2302e+01,  4.3398e-01,  ...,  1.7453e+00,
          -8.3774e-01, -2.5970e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.7448, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.4825, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)

Everytime after generating next logits 40.9% (4508 out of 11019)
On July 7 oil rivals at federal tide of Gulf oil rivals at tide of tide of tide of Mexico . Seven lawsuits arising from oil rivals arising from oil rivals at federal tide of Mexico .
iter = 4 reward = 8
final_logits =  tensor([[[ 1.0159e-02,  4.8772e+00,  5.9578e-01,  ..., -3.6202e-01,
          -1.4543e+00,  2.4359e-01],
         [-5.5251e-02,  1.9855e+00,  2.3138e-01,  ...,  4.5778e-01,
          -1.0957e+00, -4.4758e-01],
         [ 6.8352e-03,  1.8496e+00,  2.2672e-02,  ..., -6.6336e-01,
          -1.6901e+00, -1.3360e+00],
         ...,
         [-7.3408e-02,  3.8224e+00, -6.0176e-01,  ..., -1.8203e+00,
          -9.4908e-01, -2.6880e+00],
         [ 3.5573e-02,  3.2457e+00,  2.5625e-01,  ..., -5.0263e-01,
           4.7483e-01,  4.8622e-01],
         [ 7.4535e-03,  1.2241e+01,  3.7629e-01,  ...,  1.7902e+00,
          -8.1912e-01, -3.0193e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.8099, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(13.5461, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  8
8
{'input_ids': tensor([[[ 2571,  1508,  2652,  2882,  2033,   134,   305, 49218, 17403,  4596,
            222,  6358,   109, 40522, 34524,  4820,   164,   299,  4027,  1034,
            116, 62223,   454,  3682,  3793,   156,   113,   109,   278,  1034,
            116,  3741,   762, 12802,   110,   107,  1478,   115,   109,  5569,
            110,   108,  3070,   111,   176,  3217, 17659,   109,  1322,   192,
            394,  5097,   110,   107,   343, 62223,   148,   174, 71731,   115,
            109,  1965, 43983,   231,   110,   108,   112,   253,   142,  4156,
            120,   126,   117,   744,   130,   175,   109,  2648,   394,  2032,
            110,   107,   168,   117,  8549, 62223,  1034,   116,   584,   256,
            319,  2520,   118,   274,   115,   109,  7175,   113,  3064,   170,
            127, 48322,   120,   157,   138,   521,  5097,   135,   109,  2926,
          16036,   762, 14567,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1]]])}

Before Sampling 41.1% (4534 out of 11019)

Everytime after generating next logits 41.1% (4534 out of 11019)

Everytime after generating next logits 41.1% (4534 out of 11019)

Everytime after generating next logits 41.1% (4534 out of 11019)

Everytime after generating next logits 41.1% (4534 out of 11019)

Everytime after generating next logits 41.1% (4534 out of 11019)

Everytime after generating next logits 41.1% (4534 out of 11019)

Everytime after generating next logits 41.1% (4534 out of 11019)

Everytime after generating next logits 41.1% (4534 out of 11019)

Everytime after generating next logits 41.1% (4534 out of 11019)

Everytime after generating next logits 41.1% (4534 out of 11019)
Gulf of Mexico oil industry broke industry .
iter = 0 reward = 2
final_logits =  tensor([[[ 1.3347e-02,  6.5809e+00,  4.4128e-01,  ..., -2.9695e+00,
          -7.7000e-01, -2.1926e-01],
         [-9.4192e-02,  4.0943e+00, -1.7368e-01,  ..., -6.4645e+00,
          -2.4473e+00, -1.9149e+00],
         [-3.2880e-03,  3.1297e+00,  2.1536e-01,  ..., -1.9245e+00,
          -1.9501e+00, -8.9169e-01],
         ...,
         [-1.3031e-01,  3.9528e+00, -6.1409e-01,  ..., -5.0344e+00,
          -4.1080e-01, -8.8014e-01],
         [ 3.8061e-02,  6.7738e+00,  1.1939e-01,  ..., -1.7080e+00,
          -4.7475e-01, -6.8138e-01],
         [ 6.4965e-03,  1.2034e+01,  3.5724e-01,  ..., -1.4585e+00,
          -8.9896e-01, -9.1221e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(7.0460, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(32.4456, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)

Everytime after generating next logits 40.5% (4460 out of 11019)
Many extent pessimistic Spain if oil broke industries broke industries from Gulf of Mexico .
iter = 1 reward = 2
final_logits =  tensor([[[ 0.0644,  5.1327,  0.3140,  ..., -5.9611, -1.2425, -1.3763],
         [-0.0505,  1.5513, -0.1409,  ..., -4.8932, -3.1567, -2.9318],
         [-0.0474,  3.5676, -0.0147,  ..., -3.7385, -1.6325, -6.2328],
         ...,
         [-0.0715,  3.2947, -0.7050,  ..., -3.0576, -0.6314, -1.7451],
         [ 0.0596,  3.8178,  0.1070,  ..., -1.0070, -0.4717, -0.3759],
         [ 0.0347, 12.7560,  0.1890,  ..., -2.7236, -1.9141, -2.4495]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(5.4233, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(40.1252, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)

Everytime after generating next logits 40.6% (4474 out of 11019)
Many extent pessimistic Spain if oil broke broke industries of Mexico . Gulf of Mexico oil region could have broken industries .
iter = 2 reward = 5
final_logits =  tensor([[[ 5.3991e-02,  5.2037e+00,  3.2604e-01,  ..., -6.0517e+00,
          -1.1577e+00, -1.3678e+00],
         [-5.7820e-02,  1.7091e+00, -1.0040e-01,  ..., -4.5186e+00,
          -3.0287e+00, -3.3871e+00],
         [-4.7473e-02,  3.7629e+00,  1.5322e-01,  ..., -3.5800e+00,
          -1.7517e+00, -7.7944e+00],
         ...,
         [-1.1299e-01,  3.3294e+00, -8.1110e-01,  ..., -2.6375e+00,
          -1.1448e+00, -1.1902e+00],
         [ 4.5960e-02,  3.2876e+00,  5.5940e-02,  ..., -6.4058e-01,
          -3.1730e-01, -1.2576e-01],
         [ 6.6580e-03,  1.3045e+01, -9.5350e-03,  ..., -2.5165e+00,
          -1.5093e+00, -3.2007e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(6.2362, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(47.6918, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)
Many extent pessimistic Spain if oil broke industries broke industries of Mexico . Gulf of Mexico oil region could have broken industries .
iter = 3 reward = 5
final_logits =  tensor([[[ 3.4596e-02,  5.4510e+00,  2.9175e-01,  ..., -6.0546e+00,
          -1.0488e+00, -1.1343e+00],
         [-6.9933e-02,  2.1977e+00, -1.0060e-01,  ..., -4.5698e+00,
          -3.0123e+00, -3.3850e+00],
         [-4.7124e-02,  4.2852e+00,  2.2137e-01,  ..., -3.6475e+00,
          -1.5889e+00, -8.1307e+00],
         ...,
         [-1.3187e-01,  3.9916e+00, -7.6667e-01,  ..., -2.5768e+00,
          -1.5313e+00, -1.1327e+00],
         [ 3.8570e-02,  3.5854e+00,  8.6160e-02,  ..., -4.9777e-01,
          -2.5789e-01, -2.4900e-01],
         [ 6.1544e-04,  1.3089e+01,  6.4566e-02,  ..., -2.4352e+00,
          -1.2990e+00, -2.6184e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.9136, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(25.6002, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)

Everytime after generating next logits 40.8% (4494 out of 11019)
Many extent pessimistic Spain if oil broke industries broke industries . Gulf of Mexico oil region broke industries .
iter = 4 reward = 4
final_logits =  tensor([[[ 1.9701e-02,  5.4603e+00,  2.5443e-01,  ..., -6.1788e+00,
          -1.0776e+00, -9.5703e-01],
         [-8.8451e-02,  2.2121e+00, -1.5613e-01,  ..., -4.8646e+00,
          -2.9595e+00, -3.5087e+00],
         [-5.8138e-02,  4.2853e+00,  1.8590e-01,  ..., -3.7429e+00,
          -1.3971e+00, -8.2507e+00],
         ...,
         [-1.3871e-01,  3.8079e+00, -6.2839e-01,  ..., -2.9549e+00,
          -1.3409e+00, -1.8480e+00],
         [ 1.2950e-02,  4.5427e+00,  1.9849e-01,  ..., -1.0472e+00,
          -3.7378e-01, -9.8598e-01],
         [-1.7112e-02,  1.2499e+01,  1.1124e-02,  ..., -2.6322e+00,
          -1.9716e+00, -2.2523e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.2352, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(4.3055, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  4
4
{'input_ids': tensor([[[  787, 17984,   116, 16036,   204,  7175,   113,  3064,   762,  5135,
           1195,  1408,  2652,  2882,  2033,   134, 79386,   914, 17403,  4596,
            139,   706,  1013,   117,   112, 17984, 16036,   111,  1965,   176,
            524,   204,   114,  1124,   762, 14567,   115,   109,  7175,   113,
           3064,   110,   107,   139,   657,   243,   126,   192,  1137,   109,
           3358,  1069,  9873,   118,   109, 13353,   113,  2729,  1363,   124,
           1496,   164,   109,  3741,  2249,  5135,   115,   787,   689,   110,
            107, 36347,  1024,  2342,   115,   109, 11335,   124,   109, 81065,
          18308,  9600, 13773,   115,   960,   110,   107,   139,  6442,  1034,
            116, 55065, 66707,  1574,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1]]])}

Before Sampling 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)

Everytime after generating next logits 40.8% (4492 out of 11019)
Over the Gulf States Gulf Horizon on December 16 11 11th oil reported at 11th . Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater was the worst disaster held the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
iter = 0 reward = 33
final_logits =  tensor([[[ 7.8685e-02,  3.8747e+00,  7.3813e-01,  ..., -2.9007e+00,
           1.5239e+00,  1.5858e+00],
         [ 4.0078e-02,  2.9863e+00,  4.9518e-01,  ..., -3.3713e+00,
           1.4804e+00, -9.4333e-01],
         [ 8.0626e-03,  2.6827e+00,  2.7572e-01,  ..., -3.9847e+00,
          -6.0847e-01, -1.2484e+00],
         ...,
         [ 5.3592e-02,  1.2651e+01, -1.4430e-01,  ..., -4.9610e-01,
          -9.7285e-01, -5.7887e-01],
         [ 5.4628e-02,  1.2871e+01, -1.3805e-01,  ..., -4.9433e-01,
          -8.9614e-01, -6.0853e-01],
         [ 4.7359e-02,  1.3026e+01, -1.8323e-01,  ..., -6.1710e-01,
          -7.3961e-01, -1.1174e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(15.6664, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(750.9316, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.5% (4680 out of 11019)

Everytime after generating next logits 42.9% (4724 out of 11019)

Everytime after generating next logits 42.9% (4724 out of 11019)

Everytime after generating next logits 42.9% (4724 out of 11019)

Everytime after generating next logits 42.9% (4724 out of 11019)

Everytime after generating next logits 42.9% (4724 out of 11019)

Everytime after generating next logits 42.9% (4724 out of 11019)

Everytime after generating next logits 42.9% (4724 out of 11019)

Everytime after generating next logits 42.9% (4724 out of 11019)

Everytime after generating next logits 43.3% (4770 out of 11019)

Everytime after generating next logits 43.3% (4770 out of 11019)

Everytime after generating next logits 43.3% (4770 out of 11019)

Everytime after generating next logits 43.3% (4770 out of 11019)

Everytime after generating next logits 43.3% (4770 out of 11019)

Everytime after generating next logits 43.7% (4818 out of 11019)

Everytime after generating next logits 43.7% (4818 out of 11019)

Everytime after generating next logits 43.7% (4818 out of 11019)

Everytime after generating next logits 43.7% (4818 out of 11019)

Everytime after generating next logits 43.7% (4818 out of 11019)

Everytime after generating next logits 44.2% (4868 out of 11019)

Everytime after generating next logits 44.2% (4868 out of 11019)

Everytime after generating next logits 44.2% (4868 out of 11019)

Everytime after generating next logits 44.2% (4868 out of 11019)

Everytime after generating next logits 44.2% (4868 out of 11019)

Everytime after generating next logits 44.7% (4920 out of 11019)

Everytime after generating next logits 44.7% (4920 out of 11019)

Everytime after generating next logits 44.7% (4920 out of 11019)

Everytime after generating next logits 44.7% (4920 out of 11019)

Everytime after generating next logits 44.7% (4920 out of 11019)

Everytime after generating next logits 45.1% (4974 out of 11019)

Everytime after generating next logits 45.1% (4974 out of 11019)

Everytime after generating next logits 45.1% (4974 out of 11019)

Everytime after generating next logits 45.1% (4974 out of 11019)

Everytime after generating next logits 45.1% (4974 out of 11019)

Everytime after generating next logits 45.1% (4974 out of 11019)

Everytime after generating next logits 45.1% (4974 out of 11019)

Everytime after generating next logits 45.6% (5030 out of 11019)

Everytime after generating next logits 45.6% (5030 out of 11019)

Everytime after generating next logits 45.6% (5030 out of 11019)

Everytime after generating next logits 45.6% (5030 out of 11019)

Everytime after generating next logits 46.2% (5088 out of 11019)

Everytime after generating next logits 46.2% (5088 out of 11019)

Everytime after generating next logits 46.2% (5088 out of 11019)

Everytime after generating next logits 46.2% (5088 out of 11019)

Everytime after generating next logits 46.2% (5088 out of 11019)

Everytime after generating next logits 46.2% (5088 out of 11019)

Everytime after generating next logits 46.7% (5148 out of 11019)

Everytime after generating next logits 46.7% (5148 out of 11019)

Everytime after generating next logits 46.7% (5148 out of 11019)

Everytime after generating next logits 46.7% (5150 out of 11019)

Everytime after generating next logits 47.3% (5212 out of 11019)

Everytime after generating next logits 47.3% (5212 out of 11019)

Everytime after generating next logits 47.3% (5212 out of 11019)

Everytime after generating next logits 47.3% (5212 out of 11019)

Everytime after generating next logits 47.3% (5212 out of 11019)

Everytime after generating next logits 47.3% (5212 out of 11019)

Everytime after generating next logits 47.9% (5276 out of 11019)

Everytime after generating next logits 47.9% (5278 out of 11019)

Everytime after generating next logits 47.9% (5278 out of 11019)

Everytime after generating next logits 47.9% (5278 out of 11019)

Everytime after generating next logits 47.9% (5278 out of 11019)

Everytime after generating next logits 47.9% (5278 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 48.5% (5344 out of 11019)

Everytime after generating next logits 49.1% (5412 out of 11019)

Everytime after generating next logits 49.1% (5412 out of 11019)

Everytime after generating next logits 49.1% (5412 out of 11019)

Everytime after generating next logits 49.1% (5412 out of 11019)

Everytime after generating next logits 49.8% (5482 out of 11019)

Everytime after generating next logits 49.8% (5482 out of 11019)

Everytime after generating next logits 49.8% (5482 out of 11019)

Everytime after generating next logits 49.8% (5482 out of 11019)

Everytime after generating next logits 49.8% (5482 out of 11019)

Everytime after generating next logits 49.8% (5482 out of 11019)

Everytime after generating next logits 50.4% (5554 out of 11019)

Everytime after generating next logits 50.4% (5554 out of 11019)

Everytime after generating next logits 50.4% (5554 out of 11019)

Everytime after generating next logits 50.4% (5554 out of 11019)

Everytime after generating next logits 50.4% (5554 out of 11019)

Everytime after generating next logits 50.4% (5554 out of 11019)

Everytime after generating next logits 51.1% (5628 out of 11019)

Everytime after generating next logits 51.1% (5628 out of 11019)

Everytime after generating next logits 51.1% (5628 out of 11019)

Everytime after generating next logits 51.1% (5628 out of 11019)

Everytime after generating next logits 51.8% (5704 out of 11019)

Everytime after generating next logits 51.8% (5704 out of 11019)

Everytime after generating next logits 51.8% (5704 out of 11019)

Everytime after generating next logits 51.8% (5704 out of 11019)

Everytime after generating next logits 51.8% (5704 out of 11019)

Everytime after generating next logits 51.8% (5704 out of 11019)

Everytime after generating next logits 52.5% (5782 out of 11019)

Everytime after generating next logits 52.5% (5782 out of 11019)

Everytime after generating next logits 52.5% (5784 out of 11019)

Everytime after generating next logits 52.5% (5784 out of 11019)

Everytime after generating next logits 52.5% (5784 out of 11019)

Everytime after generating next logits 52.5% (5784 out of 11019)

Everytime after generating next logits 53.2% (5864 out of 11019)

Everytime after generating next logits 53.2% (5864 out of 11019)

Everytime after generating next logits 53.2% (5864 out of 11019)

Everytime after generating next logits 53.2% (5864 out of 11019)

Everytime after generating next logits 53.2% (5864 out of 11019)

Everytime after generating next logits 53.2% (5864 out of 11019)

Everytime after generating next logits 54.0% (5946 out of 11019)

Everytime after generating next logits 54.0% (5946 out of 11019)

Everytime after generating next logits 54.0% (5946 out of 11019)

Everytime after generating next logits 54.0% (5946 out of 11019)

Everytime after generating next logits 55.5% (6114 out of 11019)

Everytime after generating next logits 56.2% (6198 out of 11019)

Everytime after generating next logits 56.2% (6198 out of 11019)

Everytime after generating next logits 56.2% (6198 out of 11019)

Everytime after generating next logits 56.2% (6198 out of 11019)

Everytime after generating next logits 56.2% (6198 out of 11019)

Everytime after generating next logits 57.0% (6284 out of 11019)

Everytime after generating next logits 58.6% (6456 out of 11019)

Everytime after generating next logits 58.6% (6456 out of 11019)

Everytime after generating next logits 58.6% (6456 out of 11019)

Everytime after generating next logits 58.6% (6456 out of 11019)

Everytime after generating next logits 60.2% (6632 out of 11019)

Everytime after generating next logits 61.0% (6720 out of 11019)

Everytime after generating next logits 61.0% (6720 out of 11019)

Everytime after generating next logits 61.0% (6720 out of 11019)

Everytime after generating next logits 61.0% (6720 out of 11019)

Everytime after generating next logits 61.0% (6720 out of 11019)

Everytime after generating next logits 61.8% (6810 out of 11019)

Everytime after generating next logits 63.4% (6990 out of 11019)

Everytime after generating next logits 63.4% (6990 out of 11019)

Everytime after generating next logits 63.4% (6990 out of 11019)

Everytime after generating next logits 63.4% (6990 out of 11019)

Everytime after generating next logits 65.1% (7174 out of 11019)

Everytime after generating next logits 65.9% (7266 out of 11019)

Everytime after generating next logits 65.9% (7266 out of 11019)

Everytime after generating next logits 65.9% (7266 out of 11019)

Everytime after generating next logits 65.9% (7266 out of 11019)

Everytime after generating next logits 67.6% (7454 out of 11019)

Everytime after generating next logits 68.5% (7550 out of 11019)

Everytime after generating next logits 68.5% (7550 out of 11019)

Everytime after generating next logits 68.5% (7550 out of 11019)

Everytime after generating next logits 68.5% (7550 out of 11019)

Everytime after generating next logits 68.5% (7552 out of 11019)

Everytime after generating next logits 69.4% (7648 out of 11019)

Everytime after generating next logits 71.1% (7840 out of 11019)

Everytime after generating next logits 71.1% (7840 out of 11019)

Everytime after generating next logits 71.1% (7840 out of 11019)

Everytime after generating next logits 71.1% (7840 out of 11019)

Everytime after generating next logits 72.9% (8036 out of 11019)

Everytime after generating next logits 73.8% (8134 out of 11019)

Everytime after generating next logits 73.8% (8134 out of 11019)

Everytime after generating next logits 73.8% (8134 out of 11019)

Everytime after generating next logits 73.8% (8134 out of 11019)

Everytime after generating next logits 73.8% (8134 out of 11019)

Everytime after generating next logits 74.7% (8234 out of 11019)

Everytime after generating next logits 76.5% (8434 out of 11019)

Everytime after generating next logits 76.5% (8434 out of 11019)

Everytime after generating next logits 76.5% (8434 out of 11019)

Everytime after generating next logits 76.5% (8434 out of 11019)

Everytime after generating next logits 78.4% (8638 out of 11019)

Everytime after generating next logits 79.3% (8740 out of 11019)

Everytime after generating next logits 79.3% (8740 out of 11019)

Everytime after generating next logits 79.3% (8740 out of 11019)

Everytime after generating next logits 79.3% (8740 out of 11019)

Everytime after generating next logits 79.3% (8740 out of 11019)

Everytime after generating next logits 80.3% (8844 out of 11019)

Everytime after generating next logits 82.1% (9052 out of 11019)

Everytime after generating next logits 82.1% (9052 out of 11019)

Everytime after generating next logits 82.1% (9052 out of 11019)

Everytime after generating next logits 82.1% (9052 out of 11019)

Everytime after generating next logits 84.1% (9264 out of 11019)

Everytime after generating next logits 85.0% (9370 out of 11019)

Everytime after generating next logits 85.0% (9370 out of 11019)

Everytime after generating next logits 85.0% (9370 out of 11019)

Everytime after generating next logits 85.0% (9370 out of 11019)

Everytime after generating next logits 85.0% (9370 out of 11019)

Everytime after generating next logits 86.0% (9478 out of 11019)

Everytime after generating next logits 88.0% (9694 out of 11019)

Everytime after generating next logits 88.0% (9694 out of 11019)

Everytime after generating next logits 88.0% (9694 out of 11019)

Everytime after generating next logits 88.0% (9694 out of 11019)

Everytime after generating next logits 90.0% (9914 out of 11019)

Everytime after generating next logits 91.0% (10024 out of 11019)

Everytime after generating next logits 91.0% (10024 out of 11019)

Everytime after generating next logits 91.0% (10024 out of 11019)

Everytime after generating next logits 91.0% (10024 out of 11019)

Everytime after generating next logits 91.0% (10024 out of 11019)

Everytime after generating next logits 92.0% (10136 out of 11019)

Everytime after generating next logits 94.0% (10360 out of 11019)
Over the Gulf of Mexico oil at 11 billion reported on BBC . Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater was the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
iter = 1 reward = 19
final_logits =  tensor([[[ 6.5499e-02,  4.1236e+00,  6.9247e-01,  ..., -2.6311e+00,
           5.9839e-01,  1.3838e+00],
         [ 2.5659e-02,  3.2812e+00,  4.3669e-01,  ..., -3.5808e+00,
           1.3875e+00, -6.3068e-01],
         [-8.6252e-03,  2.6389e+00,  2.3833e-01,  ..., -4.4407e+00,
          -5.3016e-01, -1.3911e+00],
         ...,
         [ 1.0544e-02,  1.1251e+01, -1.3211e-01,  ..., -6.3026e-02,
          -1.2058e+00, -5.9345e-01],
         [ 1.1191e-02,  1.1974e+01, -2.0628e-01,  ..., -9.4792e-02,
          -1.2207e+00, -9.4635e-01],
         [ 2.8484e-03,  1.2772e+01, -3.1915e-01,  ..., -1.1179e-01,
          -9.0196e-01, -9.0920e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.3655, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(159.6426, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.0% (5176 out of 11019)

Everytime after generating next logits 47.7% (5256 out of 11019)

Everytime after generating next logits 47.7% (5256 out of 11019)

Everytime after generating next logits 47.7% (5256 out of 11019)

Everytime after generating next logits 47.7% (5256 out of 11019)

Everytime after generating next logits 47.7% (5256 out of 11019)

Everytime after generating next logits 47.7% (5256 out of 11019)

Everytime after generating next logits 47.7% (5256 out of 11019)

Everytime after generating next logits 48.4% (5338 out of 11019)

Everytime after generating next logits 48.4% (5338 out of 11019)

Everytime after generating next logits 48.4% (5338 out of 11019)

Everytime after generating next logits 48.4% (5338 out of 11019)

Everytime after generating next logits 48.4% (5338 out of 11019)

Everytime after generating next logits 49.2% (5422 out of 11019)

Everytime after generating next logits 49.2% (5422 out of 11019)

Everytime after generating next logits 49.2% (5422 out of 11019)

Everytime after generating next logits 49.2% (5422 out of 11019)

Everytime after generating next logits 49.2% (5422 out of 11019)

Everytime after generating next logits 49.2% (5424 out of 11019)

Everytime after generating next logits 49.2% (5424 out of 11019)

Everytime after generating next logits 50.0% (5510 out of 11019)

Everytime after generating next logits 50.0% (5510 out of 11019)

Everytime after generating next logits 50.0% (5510 out of 11019)

Everytime after generating next logits 50.0% (5510 out of 11019)

Everytime after generating next logits 50.0% (5510 out of 11019)

Everytime after generating next logits 50.8% (5598 out of 11019)

Everytime after generating next logits 50.8% (5598 out of 11019)

Everytime after generating next logits 50.8% (5598 out of 11019)

Everytime after generating next logits 50.8% (5598 out of 11019)

Everytime after generating next logits 50.8% (5598 out of 11019)

Everytime after generating next logits 51.6% (5688 out of 11019)

Everytime after generating next logits 51.6% (5688 out of 11019)

Everytime after generating next logits 51.6% (5688 out of 11019)

Everytime after generating next logits 51.6% (5688 out of 11019)

Everytime after generating next logits 51.6% (5688 out of 11019)

Everytime after generating next logits 52.5% (5780 out of 11019)

Everytime after generating next logits 52.5% (5780 out of 11019)

Everytime after generating next logits 52.5% (5780 out of 11019)

Everytime after generating next logits 52.5% (5780 out of 11019)

Everytime after generating next logits 52.5% (5780 out of 11019)

Everytime after generating next logits 53.3% (5874 out of 11019)

Everytime after generating next logits 53.3% (5874 out of 11019)

Everytime after generating next logits 53.3% (5876 out of 11019)

Everytime after generating next logits 53.3% (5878 out of 11019)

Everytime after generating next logits 53.3% (5878 out of 11019)

Everytime after generating next logits 53.3% (5878 out of 11019)

Everytime after generating next logits 53.3% (5878 out of 11019)

Everytime after generating next logits 54.2% (5974 out of 11019)

Everytime after generating next logits 54.2% (5974 out of 11019)

Everytime after generating next logits 54.2% (5974 out of 11019)

Everytime after generating next logits 55.1% (6072 out of 11019)

Everytime after generating next logits 55.1% (6072 out of 11019)

Everytime after generating next logits 55.1% (6072 out of 11019)

Everytime after generating next logits 55.1% (6072 out of 11019)

Everytime after generating next logits 55.1% (6072 out of 11019)

Everytime after generating next logits 56.0% (6172 out of 11019)

Everytime after generating next logits 56.0% (6172 out of 11019)

Everytime after generating next logits 56.0% (6172 out of 11019)

Everytime after generating next logits 56.0% (6172 out of 11019)

Everytime after generating next logits 56.0% (6172 out of 11019)

Everytime after generating next logits 56.9% (6274 out of 11019)

Everytime after generating next logits 56.9% (6274 out of 11019)

Everytime after generating next logits 56.9% (6274 out of 11019)

Everytime after generating next logits 56.9% (6274 out of 11019)

Everytime after generating next logits 56.9% (6274 out of 11019)

Everytime after generating next logits 56.9% (6274 out of 11019)

Everytime after generating next logits 56.9% (6274 out of 11019)

Everytime after generating next logits 57.9% (6378 out of 11019)

Everytime after generating next logits 57.9% (6378 out of 11019)

Everytime after generating next logits 57.9% (6378 out of 11019)

Everytime after generating next logits 57.9% (6378 out of 11019)

Everytime after generating next logits 57.9% (6378 out of 11019)

Everytime after generating next logits 58.8% (6484 out of 11019)

Everytime after generating next logits 58.8% (6484 out of 11019)

Everytime after generating next logits 58.8% (6484 out of 11019)

Everytime after generating next logits 58.8% (6484 out of 11019)

Everytime after generating next logits 58.8% (6484 out of 11019)

Everytime after generating next logits 59.8% (6592 out of 11019)

Everytime after generating next logits 59.8% (6592 out of 11019)

Everytime after generating next logits 59.8% (6592 out of 11019)

Everytime after generating next logits 59.8% (6592 out of 11019)

Everytime after generating next logits 59.8% (6592 out of 11019)

Everytime after generating next logits 60.8% (6702 out of 11019)

Everytime after generating next logits 60.8% (6702 out of 11019)

Everytime after generating next logits 60.8% (6702 out of 11019)

Everytime after generating next logits 60.8% (6702 out of 11019)

Everytime after generating next logits 60.8% (6702 out of 11019)

Everytime after generating next logits 60.8% (6702 out of 11019)

Everytime after generating next logits 60.8% (6702 out of 11019)

Everytime after generating next logits 61.8% (6814 out of 11019)
Over the Gulf of Mexico oil at 11 billion reported on BBC . Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater was the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
iter = 2 reward = 19
final_logits =  tensor([[[ 4.7573e-02,  4.0846e+00,  6.7223e-01,  ..., -2.5286e+00,
           5.4276e-01,  1.2239e+00],
         [ 9.0019e-03,  3.4336e+00,  4.4322e-01,  ..., -3.4979e+00,
           1.5477e+00, -6.4482e-01],
         [-2.6896e-02,  2.6756e+00,  2.3209e-01,  ..., -4.3312e+00,
          -4.2263e-01, -1.3915e+00],
         ...,
         [-2.1182e-02,  1.1057e+01, -1.5034e-01,  ...,  1.3912e-01,
          -1.4847e+00, -2.4688e-01],
         [-1.6443e-02,  1.1876e+01, -2.1469e-01,  ...,  1.6380e-01,
          -1.4737e+00, -5.7929e-01],
         [-2.1513e-02,  1.2681e+01, -3.2610e-01,  ...,  1.4673e-01,
          -1.0562e+00, -9.1878e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.5111, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(83.1423, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.1% (5186 out of 11019)

Everytime after generating next logits 47.8% (5266 out of 11019)

Everytime after generating next logits 47.8% (5266 out of 11019)

Everytime after generating next logits 47.8% (5266 out of 11019)

Everytime after generating next logits 47.8% (5266 out of 11019)

Everytime after generating next logits 47.8% (5266 out of 11019)

Everytime after generating next logits 48.5% (5348 out of 11019)

Everytime after generating next logits 48.5% (5348 out of 11019)

Everytime after generating next logits 48.5% (5348 out of 11019)

Everytime after generating next logits 48.5% (5348 out of 11019)

Everytime after generating next logits 48.5% (5348 out of 11019)

Everytime after generating next logits 49.3% (5432 out of 11019)

Everytime after generating next logits 49.3% (5432 out of 11019)

Everytime after generating next logits 49.3% (5432 out of 11019)

Everytime after generating next logits 49.3% (5432 out of 11019)

Everytime after generating next logits 49.3% (5432 out of 11019)

Everytime after generating next logits 50.1% (5518 out of 11019)

Everytime after generating next logits 50.1% (5518 out of 11019)

Everytime after generating next logits 50.1% (5518 out of 11019)

Everytime after generating next logits 50.1% (5518 out of 11019)

Everytime after generating next logits 50.1% (5518 out of 11019)

Everytime after generating next logits 50.9% (5606 out of 11019)

Everytime after generating next logits 50.9% (5606 out of 11019)

Everytime after generating next logits 50.9% (5606 out of 11019)

Everytime after generating next logits 50.9% (5606 out of 11019)

Everytime after generating next logits 50.9% (5606 out of 11019)

Everytime after generating next logits 50.9% (5606 out of 11019)

Everytime after generating next logits 50.9% (5606 out of 11019)

Everytime after generating next logits 51.7% (5696 out of 11019)

Everytime after generating next logits 51.7% (5696 out of 11019)

Everytime after generating next logits 51.7% (5696 out of 11019)

Everytime after generating next logits 51.7% (5696 out of 11019)

Everytime after generating next logits 51.7% (5696 out of 11019)

Everytime after generating next logits 52.5% (5788 out of 11019)

Everytime after generating next logits 52.5% (5788 out of 11019)

Everytime after generating next logits 52.5% (5788 out of 11019)

Everytime after generating next logits 52.5% (5788 out of 11019)

Everytime after generating next logits 52.5% (5788 out of 11019)

Everytime after generating next logits 53.4% (5882 out of 11019)

Everytime after generating next logits 53.4% (5882 out of 11019)

Everytime after generating next logits 53.4% (5882 out of 11019)

Everytime after generating next logits 53.4% (5882 out of 11019)

Everytime after generating next logits 53.4% (5882 out of 11019)

Everytime after generating next logits 54.3% (5978 out of 11019)

Everytime after generating next logits 54.3% (5978 out of 11019)

Everytime after generating next logits 54.3% (5978 out of 11019)

Everytime after generating next logits 54.3% (5978 out of 11019)

Everytime after generating next logits 54.3% (5978 out of 11019)

Everytime after generating next logits 55.1% (6076 out of 11019)

Everytime after generating next logits 55.1% (6076 out of 11019)

Everytime after generating next logits 55.1% (6076 out of 11019)

Everytime after generating next logits 55.1% (6076 out of 11019)

Everytime after generating next logits 55.1% (6076 out of 11019)

Everytime after generating next logits 55.1% (6076 out of 11019)

Everytime after generating next logits 55.1% (6076 out of 11019)

Everytime after generating next logits 56.0% (6176 out of 11019)

Everytime after generating next logits 56.0% (6176 out of 11019)

Everytime after generating next logits 56.0% (6176 out of 11019)

Everytime after generating next logits 56.0% (6176 out of 11019)

Everytime after generating next logits 56.0% (6176 out of 11019)

Everytime after generating next logits 57.0% (6278 out of 11019)

Everytime after generating next logits 57.0% (6278 out of 11019)

Everytime after generating next logits 57.0% (6278 out of 11019)

Everytime after generating next logits 57.0% (6278 out of 11019)

Everytime after generating next logits 57.0% (6278 out of 11019)

Everytime after generating next logits 57.9% (6382 out of 11019)

Everytime after generating next logits 57.9% (6382 out of 11019)

Everytime after generating next logits 57.9% (6382 out of 11019)

Everytime after generating next logits 57.9% (6382 out of 11019)

Everytime after generating next logits 57.9% (6382 out of 11019)

Everytime after generating next logits 58.9% (6488 out of 11019)

Everytime after generating next logits 58.9% (6488 out of 11019)

Everytime after generating next logits 58.9% (6488 out of 11019)

Everytime after generating next logits 58.9% (6488 out of 11019)

Everytime after generating next logits 58.9% (6488 out of 11019)

Everytime after generating next logits 58.9% (6488 out of 11019)

Everytime after generating next logits 58.9% (6488 out of 11019)

Everytime after generating next logits 59.9% (6596 out of 11019)

Everytime after generating next logits 59.9% (6596 out of 11019)

Everytime after generating next logits 59.9% (6596 out of 11019)

Everytime after generating next logits 59.9% (6596 out of 11019)

Everytime after generating next logits 59.9% (6596 out of 11019)

Everytime after generating next logits 60.9% (6706 out of 11019)

Everytime after generating next logits 60.9% (6706 out of 11019)

Everytime after generating next logits 60.9% (6706 out of 11019)

Everytime after generating next logits 60.9% (6706 out of 11019)

Everytime after generating next logits 60.9% (6706 out of 11019)

Everytime after generating next logits 61.9% (6818 out of 11019)

Everytime after generating next logits 61.9% (6818 out of 11019)

Everytime after generating next logits 61.9% (6818 out of 11019)

Everytime after generating next logits 62.9% (6930 out of 11019)

Everytime after generating next logits 62.9% (6930 out of 11019)

Everytime after generating next logits 65.0% (7158 out of 11019)

Everytime after generating next logits 67.0% (7386 out of 11019)

Everytime after generating next logits 67.0% (7386 out of 11019)

Everytime after generating next logits 67.0% (7386 out of 11019)

Everytime after generating next logits 67.0% (7386 out of 11019)

Everytime after generating next logits 69.1% (7618 out of 11019)

Everytime after generating next logits 71.2% (7850 out of 11019)

Everytime after generating next logits 71.2% (7850 out of 11019)

Everytime after generating next logits 71.2% (7850 out of 11019)

Everytime after generating next logits 71.2% (7850 out of 11019)

Everytime after generating next logits 71.2% (7850 out of 11019)

Everytime after generating next logits 73.4% (8086 out of 11019)

Everytime after generating next logits 75.5% (8322 out of 11019)

Everytime after generating next logits 75.5% (8322 out of 11019)

Everytime after generating next logits 75.5% (8322 out of 11019)

Everytime after generating next logits 75.5% (8322 out of 11019)

Everytime after generating next logits 77.7% (8562 out of 11019)

Everytime after generating next logits 79.9% (8802 out of 11019)

Everytime after generating next logits 79.9% (8802 out of 11019)

Everytime after generating next logits 79.9% (8802 out of 11019)

Everytime after generating next logits 79.9% (8802 out of 11019)

Everytime after generating next logits 79.9% (8802 out of 11019)

Everytime after generating next logits 82.1% (9046 out of 11019)

Everytime after generating next logits 84.3% (9290 out of 11019)

Everytime after generating next logits 84.3% (9290 out of 11019)

Everytime after generating next logits 84.3% (9290 out of 11019)

Everytime after generating next logits 84.3% (9290 out of 11019)

Everytime after generating next logits 86.6% (9538 out of 11019)

Everytime after generating next logits 88.8% (9786 out of 11019)

Everytime after generating next logits 88.8% (9786 out of 11019)

Everytime after generating next logits 88.8% (9786 out of 11019)

Everytime after generating next logits 88.8% (9786 out of 11019)

Everytime after generating next logits 88.8% (9786 out of 11019)

Everytime after generating next logits 91.1% (10038 out of 11019)

Everytime after generating next logits 93.4% (10290 out of 11019)

Everytime after generating next logits 93.4% (10290 out of 11019)

Everytime after generating next logits 93.4% (10290 out of 11019)

Everytime after generating next logits 93.4% (10290 out of 11019)

Everytime after generating next logits 95.7% (10546 out of 11019)

Everytime after generating next logits 98.0% (10802 out of 11019)

Everytime after generating next logits 98.0% (10802 out of 11019)

Everytime after generating next logits 98.0% (10802 out of 11019)

Everytime after generating next logits 98.0% (10802 out of 11019)

Everytime after generating next logits 98.0% (10802 out of 11019)

Everytime after generating next logits 49.7% (5478 out of 11019)

Everytime after generating next logits 52.1% (5738 out of 11019)

Everytime after generating next logits 52.1% (5738 out of 11019)

Everytime after generating next logits 52.1% (5738 out of 11019)

Everytime after generating next logits 52.1% (5738 out of 11019)

Everytime after generating next logits 54.5% (6002 out of 11019)

Everytime after generating next logits 56.9% (6266 out of 11019)

Everytime after generating next logits 56.9% (6266 out of 11019)

Everytime after generating next logits 56.9% (6266 out of 11019)

Everytime after generating next logits 56.9% (6266 out of 11019)

Everytime after generating next logits 56.9% (6266 out of 11019)

Everytime after generating next logits 59.3% (6534 out of 11019)

Everytime after generating next logits 61.7% (6802 out of 11019)

Everytime after generating next logits 61.7% (6802 out of 11019)

Everytime after generating next logits 61.7% (6802 out of 11019)

Everytime after generating next logits 61.7% (6802 out of 11019)

Everytime after generating next logits 64.2% (7074 out of 11019)

Everytime after generating next logits 66.7% (7346 out of 11019)

Everytime after generating next logits 66.7% (7346 out of 11019)

Everytime after generating next logits 66.7% (7346 out of 11019)

Everytime after generating next logits 66.7% (7346 out of 11019)

Everytime after generating next logits 69.2% (7622 out of 11019)

Everytime after generating next logits 71.7% (7898 out of 11019)

Everytime after generating next logits 71.7% (7898 out of 11019)

Everytime after generating next logits 71.7% (7898 out of 11019)

Everytime after generating next logits 71.7% (7898 out of 11019)

Everytime after generating next logits 71.7% (7898 out of 11019)

Everytime after generating next logits 74.2% (8178 out of 11019)

Everytime after generating next logits 76.8% (8458 out of 11019)

Everytime after generating next logits 76.8% (8458 out of 11019)

Everytime after generating next logits 76.8% (8458 out of 11019)

Everytime after generating next logits 76.8% (8458 out of 11019)

Everytime after generating next logits 79.3% (8742 out of 11019)

Everytime after generating next logits 81.9% (9026 out of 11019)

Everytime after generating next logits 81.9% (9026 out of 11019)

Everytime after generating next logits 81.9% (9026 out of 11019)

Everytime after generating next logits 81.9% (9026 out of 11019)

Everytime after generating next logits 81.9% (9026 out of 11019)

Everytime after generating next logits 84.5% (9314 out of 11019)

Everytime after generating next logits 87.1% (9602 out of 11019)

Everytime after generating next logits 87.1% (9602 out of 11019)

Everytime after generating next logits 87.1% (9602 out of 11019)

Everytime after generating next logits 87.1% (9602 out of 11019)

Everytime after generating next logits 89.8% (9894 out of 11019)

Everytime after generating next logits 92.4% (10186 out of 11019)

Everytime after generating next logits 92.4% (10186 out of 11019)

Everytime after generating next logits 92.4% (10186 out of 11019)

Everytime after generating next logits 92.4% (10186 out of 11019)

Everytime after generating next logits 92.4% (10186 out of 11019)

Everytime after generating next logits 95.1% (10482 out of 11019)

Everytime after generating next logits 97.8% (10778 out of 11019)

Everytime after generating next logits 97.8% (10778 out of 11019)

Everytime after generating next logits 97.8% (10778 out of 11019)

Everytime after generating next logits 97.8% (10778 out of 11019)

Everytime after generating next logits 50.4% (5558 out of 11019)

Everytime after generating next logits 53.2% (5858 out of 11019)

Everytime after generating next logits 53.2% (5858 out of 11019)

Everytime after generating next logits 53.2% (5858 out of 11019)

Everytime after generating next logits 53.2% (5858 out of 11019)

Everytime after generating next logits 53.2% (5858 out of 11019)

Everytime after generating next logits 55.9% (6162 out of 11019)

Everytime after generating next logits 58.7% (6466 out of 11019)

Everytime after generating next logits 58.7% (6466 out of 11019)

Everytime after generating next logits 58.7% (6466 out of 11019)

Everytime after generating next logits 58.7% (6466 out of 11019)

Everytime after generating next logits 61.5% (6774 out of 11019)

Everytime after generating next logits 64.3% (7082 out of 11019)

Everytime after generating next logits 64.3% (7082 out of 11019)

Everytime after generating next logits 64.3% (7082 out of 11019)

Everytime after generating next logits 64.3% (7082 out of 11019)

Everytime after generating next logits 64.3% (7082 out of 11019)

Everytime after generating next logits 67.1% (7394 out of 11019)

Everytime after generating next logits 69.9% (7706 out of 11019)

Everytime after generating next logits 69.9% (7706 out of 11019)

Everytime after generating next logits 69.9% (7706 out of 11019)

Everytime after generating next logits 69.9% (7706 out of 11019)

Everytime after generating next logits 72.8% (8022 out of 11019)

Everytime after generating next logits 75.7% (8338 out of 11019)

Everytime after generating next logits 75.7% (8338 out of 11019)

Everytime after generating next logits 75.7% (8338 out of 11019)

Everytime after generating next logits 75.7% (8338 out of 11019)

Everytime after generating next logits 78.6% (8658 out of 11019)

Everytime after generating next logits 81.5% (8978 out of 11019)

Everytime after generating next logits 81.5% (8978 out of 11019)

Everytime after generating next logits 81.5% (8978 out of 11019)

Everytime after generating next logits 81.5% (8978 out of 11019)

Everytime after generating next logits 81.5% (8978 out of 11019)

Everytime after generating next logits 84.4% (9302 out of 11019)

Everytime after generating next logits 87.4% (9626 out of 11019)

Everytime after generating next logits 87.4% (9626 out of 11019)

Everytime after generating next logits 87.4% (9626 out of 11019)

Everytime after generating next logits 87.4% (9626 out of 11019)

Everytime after generating next logits 90.3% (9954 out of 11019)

Everytime after generating next logits 93.3% (10282 out of 11019)

Everytime after generating next logits 93.3% (10282 out of 11019)

Everytime after generating next logits 93.3% (10282 out of 11019)

Everytime after generating next logits 93.3% (10282 out of 11019)

Everytime after generating next logits 93.3% (10282 out of 11019)

Everytime after generating next logits 96.3% (10614 out of 11019)

Everytime after generating next logits 99.3% (10946 out of 11019)

Everytime after generating next logits 99.3% (10946 out of 11019)

Everytime after generating next logits 99.3% (10946 out of 11019)

Everytime after generating next logits 99.3% (10946 out of 11019)

Everytime after generating next logits 51.1% (5630 out of 11019)

Everytime after generating next logits 54.1% (5966 out of 11019)

Everytime after generating next logits 54.1% (5966 out of 11019)

Everytime after generating next logits 54.1% (5966 out of 11019)

Everytime after generating next logits 54.1% (5966 out of 11019)

Everytime after generating next logits 54.1% (5966 out of 11019)

Everytime after generating next logits 57.2% (6306 out of 11019)

Everytime after generating next logits 60.3% (6646 out of 11019)

Everytime after generating next logits 60.3% (6646 out of 11019)

Everytime after generating next logits 60.3% (6646 out of 11019)

Everytime after generating next logits 60.3% (6646 out of 11019)

Everytime after generating next logits 63.4% (6990 out of 11019)

Everytime after generating next logits 66.6% (7334 out of 11019)

Everytime after generating next logits 66.6% (7334 out of 11019)

Everytime after generating next logits 66.6% (7334 out of 11019)

Everytime after generating next logits 66.6% (7334 out of 11019)

Everytime after generating next logits 66.6% (7334 out of 11019)

Everytime after generating next logits 69.7% (7682 out of 11019)

Everytime after generating next logits 72.9% (8030 out of 11019)

Everytime after generating next logits 72.9% (8030 out of 11019)

Everytime after generating next logits 72.9% (8030 out of 11019)

Everytime after generating next logits 72.9% (8030 out of 11019)

Everytime after generating next logits 76.1% (8382 out of 11019)

Everytime after generating next logits 79.3% (8734 out of 11019)

Everytime after generating next logits 79.3% (8734 out of 11019)

Everytime after generating next logits 79.3% (8734 out of 11019)

Everytime after generating next logits 79.3% (8734 out of 11019)

Everytime after generating next logits 79.3% (8734 out of 11019)

Everytime after generating next logits 82.5% (9090 out of 11019)

Everytime after generating next logits 85.7% (9446 out of 11019)

Everytime after generating next logits 85.7% (9446 out of 11019)

Everytime after generating next logits 85.7% (9446 out of 11019)

Everytime after generating next logits 85.7% (9446 out of 11019)

Everytime after generating next logits 89.0% (9806 out of 11019)

Everytime after generating next logits 92.3% (10166 out of 11019)

Everytime after generating next logits 92.3% (10166 out of 11019)

Everytime after generating next logits 92.3% (10166 out of 11019)

Everytime after generating next logits 92.3% (10166 out of 11019)

Everytime after generating next logits 95.6% (10530 out of 11019)

Everytime after generating next logits 98.9% (10894 out of 11019)

Everytime after generating next logits 98.9% (10894 out of 11019)

Everytime after generating next logits 98.9% (10894 out of 11019)

Everytime after generating next logits 98.9% (10894 out of 11019)

Everytime after generating next logits 98.9% (10894 out of 11019)

Everytime after generating next logits 51.7% (5694 out of 11019)

Everytime after generating next logits 55.0% (6062 out of 11019)

Everytime after generating next logits 55.0% (6062 out of 11019)

Everytime after generating next logits 55.0% (6062 out of 11019)

Everytime after generating next logits 55.0% (6062 out of 11019)

Everytime after generating next logits 58.4% (6434 out of 11019)

Everytime after generating next logits 61.8% (6806 out of 11019)

Everytime after generating next logits 61.8% (6806 out of 11019)

Everytime after generating next logits 61.8% (6806 out of 11019)

Everytime after generating next logits 61.8% (6806 out of 11019)

Everytime after generating next logits 61.8% (6806 out of 11019)

Everytime after generating next logits 65.2% (7182 out of 11019)

Everytime after generating next logits 68.6% (7558 out of 11019)

Everytime after generating next logits 68.6% (7558 out of 11019)

Everytime after generating next logits 68.6% (7558 out of 11019)

Everytime after generating next logits 68.6% (7558 out of 11019)

Everytime after generating next logits 72.0% (7938 out of 11019)

Everytime after generating next logits 75.5% (8318 out of 11019)

Everytime after generating next logits 75.5% (8318 out of 11019)

Everytime after generating next logits 75.5% (8318 out of 11019)

Everytime after generating next logits 75.5% (8318 out of 11019)

Everytime after generating next logits 75.5% (8318 out of 11019)

Everytime after generating next logits 79.0% (8702 out of 11019)

Everytime after generating next logits 82.5% (9086 out of 11019)

Everytime after generating next logits 82.5% (9086 out of 11019)

Everytime after generating next logits 82.5% (9086 out of 11019)

Everytime after generating next logits 82.5% (9086 out of 11019)

Everytime after generating next logits 86.0% (9474 out of 11019)

Everytime after generating next logits 89.5% (9862 out of 11019)

Everytime after generating next logits 89.5% (9862 out of 11019)

Everytime after generating next logits 89.5% (9862 out of 11019)

Everytime after generating next logits 89.5% (9862 out of 11019)

Everytime after generating next logits 89.5% (9862 out of 11019)

Everytime after generating next logits 93.1% (10254 out of 11019)

Everytime after generating next logits 96.6% (10646 out of 11019)

Everytime after generating next logits 96.6% (10646 out of 11019)

Everytime after generating next logits 96.6% (10646 out of 11019)

Everytime after generating next logits 96.6% (10646 out of 11019)

Everytime after generating next logits 52.2% (5750 out of 11019)

Everytime after generating next logits 55.8% (6146 out of 11019)

Everytime after generating next logits 55.8% (6146 out of 11019)

Everytime after generating next logits 55.8% (6146 out of 11019)

Everytime after generating next logits 55.8% (6146 out of 11019)

Everytime after generating next logits 55.8% (6146 out of 11019)

Everytime after generating next logits 59.4% (6546 out of 11019)

Everytime after generating next logits 63.0% (6946 out of 11019)

Everytime after generating next logits 63.0% (6946 out of 11019)

Everytime after generating next logits 63.0% (6946 out of 11019)

Everytime after generating next logits 63.0% (6946 out of 11019)

Everytime after generating next logits 66.7% (7350 out of 11019)

Everytime after generating next logits 70.4% (7754 out of 11019)

Everytime after generating next logits 70.4% (7754 out of 11019)

Everytime after generating next logits 70.4% (7754 out of 11019)

Everytime after generating next logits 70.4% (7754 out of 11019)

Everytime after generating next logits 70.4% (7754 out of 11019)

Everytime after generating next logits 74.1% (8162 out of 11019)

Everytime after generating next logits 77.8% (8570 out of 11019)

Everytime after generating next logits 77.8% (8570 out of 11019)

Everytime after generating next logits 77.8% (8570 out of 11019)

Everytime after generating next logits 77.8% (8570 out of 11019)

Everytime after generating next logits 81.5% (8982 out of 11019)

Everytime after generating next logits 85.3% (9394 out of 11019)

Everytime after generating next logits 85.3% (9394 out of 11019)

Everytime after generating next logits 85.3% (9394 out of 11019)

Everytime after generating next logits 85.3% (9394 out of 11019)

Everytime after generating next logits 89.0% (9810 out of 11019)

Everytime after generating next logits 92.8% (10226 out of 11019)

Everytime after generating next logits 92.8% (10226 out of 11019)

Everytime after generating next logits 92.8% (10226 out of 11019)

Everytime after generating next logits 92.8% (10226 out of 11019)

Everytime after generating next logits 92.8% (10226 out of 11019)

Everytime after generating next logits 96.6% (10646 out of 11019)

Everytime after generating next logits 52.7% (5802 out of 11019)

Everytime after generating next logits 52.7% (5802 out of 11019)

Everytime after generating next logits 52.7% (5802 out of 11019)

Everytime after generating next logits 52.7% (5802 out of 11019)

Everytime after generating next logits 56.5% (6226 out of 11019)

Everytime after generating next logits 60.4% (6650 out of 11019)

Everytime after generating next logits 60.4% (6650 out of 11019)

Everytime after generating next logits 60.4% (6650 out of 11019)

Everytime after generating next logits 60.4% (6650 out of 11019)

Everytime after generating next logits 60.4% (6650 out of 11019)

Everytime after generating next logits 64.2% (7078 out of 11019)

Everytime after generating next logits 68.1% (7506 out of 11019)

Everytime after generating next logits 68.1% (7506 out of 11019)

Everytime after generating next logits 68.1% (7506 out of 11019)

Everytime after generating next logits 68.1% (7506 out of 11019)

Everytime after generating next logits 72.0% (7938 out of 11019)

Everytime after generating next logits 76.0% (8370 out of 11019)

Everytime after generating next logits 76.0% (8370 out of 11019)

Everytime after generating next logits 76.0% (8370 out of 11019)

Everytime after generating next logits 76.0% (8370 out of 11019)

Everytime after generating next logits 76.0% (8370 out of 11019)

Everytime after generating next logits 79.9% (8806 out of 11019)

Everytime after generating next logits 83.9% (9242 out of 11019)

Everytime after generating next logits 83.9% (9242 out of 11019)

Everytime after generating next logits 83.9% (9242 out of 11019)

Everytime after generating next logits 83.9% (9242 out of 11019)

Everytime after generating next logits 87.9% (9682 out of 11019)

Everytime after generating next logits 91.9% (10122 out of 11019)

Everytime after generating next logits 91.9% (10122 out of 11019)

Everytime after generating next logits 91.9% (10122 out of 11019)

Everytime after generating next logits 91.9% (10122 out of 11019)

Everytime after generating next logits 91.9% (10122 out of 11019)

Everytime after generating next logits 95.9% (10566 out of 11019)

Everytime after generating next logits 99.9% (11010 out of 11019)

Everytime after generating next logits 99.9% (11010 out of 11019)

Everytime after generating next logits 99.9% (11010 out of 11019)

Everytime after generating next logits 99.9% (11010 out of 11019)

Everytime after generating next logits 53.1% (5854 out of 11019)

Everytime after generating next logits 57.2% (6302 out of 11019)

Everytime after generating next logits 57.2% (6302 out of 11019)

Everytime after generating next logits 57.2% (6302 out of 11019)

Everytime after generating next logits 57.2% (6302 out of 11019)

Everytime after generating next logits 57.2% (6302 out of 11019)

Everytime after generating next logits 61.3% (6754 out of 11019)

Everytime after generating next logits 65.4% (7206 out of 11019)

Everytime after generating next logits 65.4% (7206 out of 11019)

Everytime after generating next logits 65.4% (7206 out of 11019)

Everytime after generating next logits 65.4% (7206 out of 11019)

Everytime after generating next logits 69.5% (7662 out of 11019)

Everytime after generating next logits 73.7% (8118 out of 11019)

Everytime after generating next logits 73.7% (8118 out of 11019)

Everytime after generating next logits 73.7% (8118 out of 11019)

Everytime after generating next logits 73.7% (8118 out of 11019)

Everytime after generating next logits 77.8% (8578 out of 11019)

Everytime after generating next logits 82.0% (9038 out of 11019)

Everytime after generating next logits 82.0% (9038 out of 11019)

Everytime after generating next logits 82.0% (9038 out of 11019)

Everytime after generating next logits 82.0% (9038 out of 11019)

Everytime after generating next logits 82.0% (9038 out of 11019)

Everytime after generating next logits 86.2% (9502 out of 11019)

Everytime after generating next logits 90.4% (9966 out of 11019)

Everytime after generating next logits 90.4% (9966 out of 11019)

Everytime after generating next logits 90.4% (9966 out of 11019)

Everytime after generating next logits 90.4% (9966 out of 11019)

Everytime after generating next logits 94.7% (10434 out of 11019)

Everytime after generating next logits 98.9% (10902 out of 11019)

Everytime after generating next logits 98.9% (10902 out of 11019)

Everytime after generating next logits 98.9% (10902 out of 11019)

Everytime after generating next logits 98.9% (10902 out of 11019)

Everytime after generating next logits 98.9% (10902 out of 11019)

Everytime after generating next logits 53.6% (5902 out of 11019)

Everytime after generating next logits 57.8% (6374 out of 11019)

Everytime after generating next logits 57.8% (6374 out of 11019)

Everytime after generating next logits 57.8% (6374 out of 11019)

Everytime after generating next logits 57.8% (6374 out of 11019)

Everytime after generating next logits 62.2% (6850 out of 11019)

Everytime after generating next logits 66.5% (7326 out of 11019)

Everytime after generating next logits 66.5% (7326 out of 11019)

Everytime after generating next logits 66.5% (7326 out of 11019)

Everytime after generating next logits 66.5% (7326 out of 11019)

Everytime after generating next logits 66.5% (7326 out of 11019)

Everytime after generating next logits 70.8% (7806 out of 11019)

Everytime after generating next logits 75.2% (8286 out of 11019)

Everytime after generating next logits 75.2% (8286 out of 11019)

Everytime after generating next logits 75.2% (8286 out of 11019)

Everytime after generating next logits 75.2% (8286 out of 11019)

Everytime after generating next logits 79.6% (8770 out of 11019)

Everytime after generating next logits 84.0% (9254 out of 11019)

Everytime after generating next logits 84.0% (9254 out of 11019)

Everytime after generating next logits 84.0% (9254 out of 11019)

Everytime after generating next logits 84.0% (9254 out of 11019)

Everytime after generating next logits 84.0% (9254 out of 11019)

Everytime after generating next logits 88.4% (9742 out of 11019)

Everytime after generating next logits 92.8% (10230 out of 11019)

Everytime after generating next logits 92.8% (10230 out of 11019)

Everytime after generating next logits 92.8% (10230 out of 11019)

Everytime after generating next logits 92.8% (10230 out of 11019)

Everytime after generating next logits 97.3% (10722 out of 11019)

Everytime after generating next logits 54.0% (5946 out of 11019)

Everytime after generating next logits 54.0% (5946 out of 11019)

Everytime after generating next logits 54.0% (5946 out of 11019)

Everytime after generating next logits 54.0% (5946 out of 11019)

Everytime after generating next logits 54.0% (5946 out of 11019)

Everytime after generating next logits 58.5% (6442 out of 11019)

Everytime after generating next logits 63.0% (6938 out of 11019)

Everytime after generating next logits 63.0% (6938 out of 11019)

Everytime after generating next logits 63.0% (6938 out of 11019)

Everytime after generating next logits 63.0% (6938 out of 11019)

Everytime after generating next logits 67.5% (7438 out of 11019)

Everytime after generating next logits 72.0% (7938 out of 11019)

Everytime after generating next logits 72.0% (7938 out of 11019)

Everytime after generating next logits 72.0% (7938 out of 11019)

Everytime after generating next logits 72.0% (7938 out of 11019)

Everytime after generating next logits 76.6% (8442 out of 11019)

Everytime after generating next logits 81.2% (8946 out of 11019)

Everytime after generating next logits 81.2% (8946 out of 11019)

Everytime after generating next logits 81.2% (8946 out of 11019)

Everytime after generating next logits 81.2% (8946 out of 11019)

Everytime after generating next logits 81.2% (8946 out of 11019)

Everytime after generating next logits 85.8% (9454 out of 11019)

Everytime after generating next logits 90.4% (9962 out of 11019)

Everytime after generating next logits 90.4% (9962 out of 11019)

Everytime after generating next logits 90.4% (9962 out of 11019)

Everytime after generating next logits 90.4% (9962 out of 11019)

Everytime after generating next logits 95.1% (10474 out of 11019)

Everytime after generating next logits 99.7% (10986 out of 11019)

Everytime after generating next logits 99.7% (10986 out of 11019)

Everytime after generating next logits 99.7% (10986 out of 11019)

Everytime after generating next logits 99.7% (10986 out of 11019)

Everytime after generating next logits 99.7% (10986 out of 11019)

Everytime after generating next logits 54.4% (5990 out of 11019)

Everytime after generating next logits 59.0% (6506 out of 11019)

Everytime after generating next logits 59.0% (6506 out of 11019)

Everytime after generating next logits 59.0% (6506 out of 11019)

Everytime after generating next logits 59.0% (6506 out of 11019)

Everytime after generating next logits 63.8% (7026 out of 11019)

Everytime after generating next logits 68.5% (7546 out of 11019)

Everytime after generating next logits 68.5% (7546 out of 11019)

Everytime after generating next logits 68.5% (7546 out of 11019)

Everytime after generating next logits 68.5% (7546 out of 11019)

Everytime after generating next logits 68.5% (7546 out of 11019)

Everytime after generating next logits 73.2% (8070 out of 11019)

Everytime after generating next logits 78.0% (8594 out of 11019)

Everytime after generating next logits 78.0% (8594 out of 11019)

Everytime after generating next logits 78.0% (8594 out of 11019)

Everytime after generating next logits 78.0% (8594 out of 11019)

Everytime after generating next logits 82.8% (9122 out of 11019)

Everytime after generating next logits 87.6% (9650 out of 11019)

Everytime after generating next logits 87.6% (9650 out of 11019)

Everytime after generating next logits 87.6% (9650 out of 11019)

Everytime after generating next logits 87.6% (9650 out of 11019)

Everytime after generating next logits 87.6% (9650 out of 11019)

Everytime after generating next logits 92.4% (10182 out of 11019)

Everytime after generating next logits 97.2% (10714 out of 11019)

Everytime after generating next logits 97.2% (10714 out of 11019)

Everytime after generating next logits 97.2% (10714 out of 11019)

Everytime after generating next logits 97.2% (10714 out of 11019)

Everytime after generating next logits 54.7% (6030 out of 11019)

Everytime after generating next logits 59.6% (6566 out of 11019)

Everytime after generating next logits 59.6% (6566 out of 11019)

Everytime after generating next logits 59.6% (6566 out of 11019)

Everytime after generating next logits 59.6% (6566 out of 11019)

Everytime after generating next logits 59.6% (6566 out of 11019)

Everytime after generating next logits 64.5% (7106 out of 11019)

Everytime after generating next logits 69.4% (7646 out of 11019)

Everytime after generating next logits 69.4% (7646 out of 11019)

Everytime after generating next logits 69.4% (7646 out of 11019)

Everytime after generating next logits 69.4% (7646 out of 11019)

Everytime after generating next logits 74.3% (8190 out of 11019)

Everytime after generating next logits 79.3% (8734 out of 11019)

Everytime after generating next logits 79.3% (8734 out of 11019)

Everytime after generating next logits 79.3% (8734 out of 11019)

Everytime after generating next logits 79.3% (8734 out of 11019)

Everytime after generating next logits 84.2% (9282 out of 11019)

Everytime after generating next logits 89.2% (9830 out of 11019)

Everytime after generating next logits 89.2% (9830 out of 11019)

Everytime after generating next logits 89.2% (9830 out of 11019)

Everytime after generating next logits 89.2% (9830 out of 11019)

Everytime after generating next logits 89.2% (9830 out of 11019)

Everytime after generating next logits 94.2% (10382 out of 11019)

Everytime after generating next logits 99.2% (10934 out of 11019)

Everytime after generating next logits 99.2% (10934 out of 11019)

Everytime after generating next logits 99.2% (10934 out of 11019)

Everytime after generating next logits 99.2% (10934 out of 11019)

Everytime after generating next logits 55.1% (6070 out of 11019)

Everytime after generating next logits 60.1% (6626 out of 11019)

Everytime after generating next logits 60.1% (6626 out of 11019)

Everytime after generating next logits 60.1% (6626 out of 11019)

Everytime after generating next logits 60.1% (6626 out of 11019)

Everytime after generating next logits 60.1% (6626 out of 11019)

Everytime after generating next logits 65.2% (7186 out of 11019)

Everytime after generating next logits 70.3% (7746 out of 11019)

Everytime after generating next logits 70.3% (7746 out of 11019)

Everytime after generating next logits 70.3% (7746 out of 11019)

Everytime after generating next logits 70.3% (7746 out of 11019)

Everytime after generating next logits 75.4% (8310 out of 11019)

Everytime after generating next logits 80.5% (8874 out of 11019)

Everytime after generating next logits 80.5% (8874 out of 11019)

Everytime after generating next logits 80.5% (8874 out of 11019)

Everytime after generating next logits 80.5% (8874 out of 11019)

Everytime after generating next logits 80.5% (8874 out of 11019)

Everytime after generating next logits 85.7% (9442 out of 11019)

Everytime after generating next logits 90.8% (10010 out of 11019)

Everytime after generating next logits 90.8% (10010 out of 11019)

Everytime after generating next logits 90.8% (10010 out of 11019)

Everytime after generating next logits 90.8% (10010 out of 11019)

Everytime after generating next logits 96.0% (10582 out of 11019)

Everytime after generating next logits 55.4% (6106 out of 11019)

Everytime after generating next logits 55.4% (6106 out of 11019)

Everytime after generating next logits 55.4% (6106 out of 11019)

Everytime after generating next logits 55.4% (6106 out of 11019)

Everytime after generating next logits 55.4% (6106 out of 11019)

Everytime after generating next logits 60.6% (6682 out of 11019)

Everytime after generating next logits 65.9% (7258 out of 11019)

Everytime after generating next logits 65.9% (7258 out of 11019)

Everytime after generating next logits 65.9% (7258 out of 11019)

Everytime after generating next logits 65.9% (7258 out of 11019)

Everytime after generating next logits 71.1% (7838 out of 11019)

Everytime after generating next logits 76.4% (8418 out of 11019)

Everytime after generating next logits 76.4% (8418 out of 11019)

Everytime after generating next logits 76.4% (8418 out of 11019)

Everytime after generating next logits 76.4% (8418 out of 11019)

Everytime after generating next logits 76.4% (8418 out of 11019)

Everytime after generating next logits 81.7% (9002 out of 11019)

Everytime after generating next logits 87.0% (9586 out of 11019)

Everytime after generating next logits 87.0% (9586 out of 11019)

Everytime after generating next logits 87.0% (9586 out of 11019)

Everytime after generating next logits 87.0% (9586 out of 11019)

Everytime after generating next logits 92.3% (10174 out of 11019)

Everytime after generating next logits 97.7% (10762 out of 11019)

Everytime after generating next logits 97.7% (10762 out of 11019)

Everytime after generating next logits 97.7% (10762 out of 11019)

Everytime after generating next logits 97.7% (10762 out of 11019)

Everytime after generating next logits 55.7% (6142 out of 11019)

Everytime after generating next logits 61.1% (6734 out of 11019)

Everytime after generating next logits 61.1% (6734 out of 11019)

Everytime after generating next logits 61.1% (6734 out of 11019)

Everytime after generating next logits 61.1% (6734 out of 11019)

Everytime after generating next logits 61.1% (6734 out of 11019)

Everytime after generating next logits 66.5% (7330 out of 11019)

Everytime after generating next logits 71.9% (7926 out of 11019)

Everytime after generating next logits 71.9% (7926 out of 11019)

Everytime after generating next logits 71.9% (7926 out of 11019)

Everytime after generating next logits 71.9% (7926 out of 11019)

Everytime after generating next logits 77.4% (8526 out of 11019)

Everytime after generating next logits 82.8% (9126 out of 11019)

Everytime after generating next logits 82.8% (9126 out of 11019)

Everytime after generating next logits 82.8% (9126 out of 11019)

Everytime after generating next logits 82.8% (9126 out of 11019)

Everytime after generating next logits 82.8% (9126 out of 11019)

Everytime after generating next logits 88.3% (9730 out of 11019)

Everytime after generating next logits 93.8% (10334 out of 11019)

Everytime after generating next logits 93.8% (10334 out of 11019)

Everytime after generating next logits 93.8% (10334 out of 11019)

Everytime after generating next logits 93.8% (10334 out of 11019)

Everytime after generating next logits 99.3% (10942 out of 11019)

Everytime after generating next logits 56.1% (6178 out of 11019)

Everytime after generating next logits 56.1% (6178 out of 11019)

Everytime after generating next logits 56.1% (6178 out of 11019)

Everytime after generating next logits 56.1% (6178 out of 11019)

Everytime after generating next logits 56.1% (6178 out of 11019)

Everytime after generating next logits 61.6% (6790 out of 11019)

Everytime after generating next logits 67.2% (7402 out of 11019)

Everytime after generating next logits 67.2% (7402 out of 11019)

Everytime after generating next logits 67.2% (7402 out of 11019)

Everytime after generating next logits 67.2% (7402 out of 11019)

Everytime after generating next logits 72.8% (8018 out of 11019)

Everytime after generating next logits 78.4% (8634 out of 11019)

Everytime after generating next logits 78.4% (8634 out of 11019)

Everytime after generating next logits 78.4% (8634 out of 11019)

Everytime after generating next logits 78.4% (8634 out of 11019)

Everytime after generating next logits 78.4% (8634 out of 11019)

Everytime after generating next logits 84.0% (9254 out of 11019)

Everytime after generating next logits 89.6% (9874 out of 11019)

Everytime after generating next logits 89.6% (9874 out of 11019)

Everytime after generating next logits 89.6% (9874 out of 11019)

Everytime after generating next logits 89.6% (9874 out of 11019)

Everytime after generating next logits 95.3% (10498 out of 11019)

Everytime after generating next logits 56.4% (6210 out of 11019)

Everytime after generating next logits 56.4% (6210 out of 11019)

Everytime after generating next logits 56.4% (6210 out of 11019)

Everytime after generating next logits 56.4% (6210 out of 11019)

Everytime after generating next logits 56.4% (6210 out of 11019)

Everytime after generating next logits 62.1% (6838 out of 11019)

Everytime after generating next logits 67.8% (7466 out of 11019)

Everytime after generating next logits 67.8% (7466 out of 11019)

Everytime after generating next logits 67.8% (7466 out of 11019)

Everytime after generating next logits 67.8% (7466 out of 11019)

Everytime after generating next logits 73.5% (8098 out of 11019)

Everytime after generating next logits 79.2% (8730 out of 11019)

Everytime after generating next logits 79.2% (8730 out of 11019)

Everytime after generating next logits 79.2% (8730 out of 11019)

Everytime after generating next logits 79.2% (8730 out of 11019)

Everytime after generating next logits 85.0% (9366 out of 11019)

Everytime after generating next logits 90.8% (10002 out of 11019)

Everytime after generating next logits 90.8% (10002 out of 11019)

Everytime after generating next logits 90.8% (10002 out of 11019)

Everytime after generating next logits 90.8% (10002 out of 11019)

Everytime after generating next logits 90.8% (10002 out of 11019)

Everytime after generating next logits 96.6% (10642 out of 11019)

Everytime after generating next logits 56.6% (6242 out of 11019)

Everytime after generating next logits 56.6% (6242 out of 11019)

Everytime after generating next logits 56.6% (6242 out of 11019)

Everytime after generating next logits 56.6% (6242 out of 11019)

Everytime after generating next logits 62.5% (6886 out of 11019)

Everytime after generating next logits 68.3% (7530 out of 11019)

Everytime after generating next logits 68.3% (7530 out of 11019)

Everytime after generating next logits 68.3% (7530 out of 11019)

Everytime after generating next logits 68.3% (7530 out of 11019)

Everytime after generating next logits 68.3% (7530 out of 11019)

Everytime after generating next logits 74.2% (8178 out of 11019)

Everytime after generating next logits 80.1% (8826 out of 11019)

Everytime after generating next logits 80.1% (8826 out of 11019)

Everytime after generating next logits 80.1% (8826 out of 11019)

Everytime after generating next logits 80.1% (8826 out of 11019)

Everytime after generating next logits 86.0% (9478 out of 11019)

Everytime after generating next logits 91.9% (10130 out of 11019)

Everytime after generating next logits 91.9% (10130 out of 11019)

Everytime after generating next logits 91.9% (10130 out of 11019)

Everytime after generating next logits 91.9% (10130 out of 11019)

Everytime after generating next logits 91.9% (10130 out of 11019)

Everytime after generating next logits 97.9% (10786 out of 11019)

Everytime after generating next logits 56.9% (6274 out of 11019)

Everytime after generating next logits 56.9% (6274 out of 11019)

Everytime after generating next logits 56.9% (6274 out of 11019)

Everytime after generating next logits 56.9% (6274 out of 11019)

Everytime after generating next logits 62.9% (6934 out of 11019)

Everytime after generating next logits 68.9% (7594 out of 11019)

Everytime after generating next logits 68.9% (7594 out of 11019)

Everytime after generating next logits 68.9% (7594 out of 11019)

Everytime after generating next logits 68.9% (7594 out of 11019)

Everytime after generating next logits 68.9% (7594 out of 11019)

Everytime after generating next logits 74.9% (8258 out of 11019)

Everytime after generating next logits 81.0% (8922 out of 11019)

Everytime after generating next logits 81.0% (8922 out of 11019)

Everytime after generating next logits 81.0% (8922 out of 11019)

Everytime after generating next logits 81.0% (8922 out of 11019)

Everytime after generating next logits 87.0% (9590 out of 11019)

Everytime after generating next logits 93.1% (10258 out of 11019)

Everytime after generating next logits 93.1% (10258 out of 11019)

Everytime after generating next logits 93.1% (10258 out of 11019)

Everytime after generating next logits 93.1% (10258 out of 11019)

Everytime after generating next logits 93.1% (10258 out of 11019)

Everytime after generating next logits 99.2% (10930 out of 11019)

Everytime after generating next logits 57.2% (6306 out of 11019)

Everytime after generating next logits 57.2% (6306 out of 11019)

Everytime after generating next logits 57.2% (6306 out of 11019)

Everytime after generating next logits 57.2% (6306 out of 11019)

Everytime after generating next logits 63.4% (6982 out of 11019)

Everytime after generating next logits 69.5% (7658 out of 11019)

Everytime after generating next logits 69.5% (7658 out of 11019)

Everytime after generating next logits 69.5% (7658 out of 11019)

Everytime after generating next logits 69.5% (7658 out of 11019)

Everytime after generating next logits 75.7% (8338 out of 11019)

Everytime after generating next logits 81.8% (9018 out of 11019)

Everytime after generating next logits 81.8% (9018 out of 11019)

Everytime after generating next logits 81.8% (9018 out of 11019)

Everytime after generating next logits 81.8% (9018 out of 11019)

Everytime after generating next logits 81.8% (9018 out of 11019)

Everytime after generating next logits 88.0% (9702 out of 11019)

Everytime after generating next logits 94.3% (10386 out of 11019)

Everytime after generating next logits 94.3% (10386 out of 11019)

Everytime after generating next logits 94.3% (10386 out of 11019)

Everytime after generating next logits 94.3% (10386 out of 11019)

Everytime after generating next logits 57.5% (6334 out of 11019)

Everytime after generating next logits 63.7% (7022 out of 11019)

Everytime after generating next logits 63.7% (7022 out of 11019)

Everytime after generating next logits 63.7% (7022 out of 11019)

Everytime after generating next logits 63.7% (7022 out of 11019)

Everytime after generating next logits 63.7% (7022 out of 11019)

Everytime after generating next logits 70.0% (7714 out of 11019)

Everytime after generating next logits 76.3% (8406 out of 11019)

Everytime after generating next logits 76.3% (8406 out of 11019)

Everytime after generating next logits 76.3% (8406 out of 11019)

Everytime after generating next logits 76.3% (8406 out of 11019)

Everytime after generating next logits 82.6% (9102 out of 11019)

Everytime after generating next logits 88.9% (9798 out of 11019)

Everytime after generating next logits 88.9% (9798 out of 11019)

Everytime after generating next logits 88.9% (9798 out of 11019)

Everytime after generating next logits 88.9% (9798 out of 11019)

Everytime after generating next logits 88.9% (9798 out of 11019)

Everytime after generating next logits 95.3% (10498 out of 11019)

Everytime after generating next logits 57.7% (6362 out of 11019)

Everytime after generating next logits 57.7% (6362 out of 11019)

Everytime after generating next logits 57.7% (6362 out of 11019)

Everytime after generating next logits 57.7% (6362 out of 11019)

Everytime after generating next logits 64.1% (7066 out of 11019)

Everytime after generating next logits 70.5% (7770 out of 11019)

Everytime after generating next logits 70.5% (7770 out of 11019)

Everytime after generating next logits 70.5% (7770 out of 11019)

Everytime after generating next logits 70.5% (7770 out of 11019)

Everytime after generating next logits 70.5% (7770 out of 11019)

Everytime after generating next logits 76.9% (8478 out of 11019)

Everytime after generating next logits 83.4% (9186 out of 11019)

Everytime after generating next logits 83.4% (9186 out of 11019)

Everytime after generating next logits 83.4% (9186 out of 11019)

Everytime after generating next logits 83.4% (9186 out of 11019)

Everytime after generating next logits 89.8% (9898 out of 11019)

Everytime after generating next logits 96.3% (10610 out of 11019)

Everytime after generating next logits 96.3% (10610 out of 11019)

Everytime after generating next logits 96.3% (10610 out of 11019)

Everytime after generating next logits 96.3% (10610 out of 11019)

Everytime after generating next logits 96.3% (10610 out of 11019)

Everytime after generating next logits 58.0% (6390 out of 11019)

Everytime after generating next logits 64.5% (7106 out of 11019)

Everytime after generating next logits 64.5% (7106 out of 11019)

Everytime after generating next logits 64.5% (7106 out of 11019)

Everytime after generating next logits 64.5% (7106 out of 11019)

Everytime after generating next logits 71.0% (7826 out of 11019)

Everytime after generating next logits 77.6% (8546 out of 11019)

Everytime after generating next logits 77.6% (8546 out of 11019)

Everytime after generating next logits 77.6% (8546 out of 11019)

Everytime after generating next logits 77.6% (8546 out of 11019)

Everytime after generating next logits 84.1% (9270 out of 11019)

Everytime after generating next logits 90.7% (9994 out of 11019)

Everytime after generating next logits 90.7% (9994 out of 11019)

Everytime after generating next logits 90.7% (9994 out of 11019)

Everytime after generating next logits 90.7% (9994 out of 11019)

Everytime after generating next logits 90.7% (9994 out of 11019)

Everytime after generating next logits 97.3% (10722 out of 11019)

Everytime after generating next logits 58.2% (6418 out of 11019)

Everytime after generating next logits 58.2% (6418 out of 11019)

Everytime after generating next logits 58.2% (6418 out of 11019)

Everytime after generating next logits 58.2% (6418 out of 11019)

Everytime after generating next logits 64.9% (7150 out of 11019)

Everytime after generating next logits 71.5% (7882 out of 11019)

Everytime after generating next logits 71.5% (7882 out of 11019)

Everytime after generating next logits 71.5% (7882 out of 11019)

Everytime after generating next logits 71.5% (7882 out of 11019)

Everytime after generating next logits 71.5% (7882 out of 11019)

Everytime after generating next logits 78.2% (8618 out of 11019)

Everytime after generating next logits 84.9% (9354 out of 11019)

Everytime after generating next logits 84.9% (9354 out of 11019)

Everytime after generating next logits 84.9% (9354 out of 11019)

Everytime after generating next logits 84.9% (9354 out of 11019)

Everytime after generating next logits 91.6% (10094 out of 11019)

Everytime after generating next logits 98.3% (10834 out of 11019)

Everytime after generating next logits 98.3% (10834 out of 11019)

Everytime after generating next logits 98.3% (10834 out of 11019)

Everytime after generating next logits 98.3% (10834 out of 11019)

Everytime after generating next logits 98.3% (10834 out of 11019)

Everytime after generating next logits 58.5% (6446 out of 11019)

Everytime after generating next logits 65.3% (7190 out of 11019)

Everytime after generating next logits 65.3% (7190 out of 11019)

Everytime after generating next logits 65.3% (7190 out of 11019)

Everytime after generating next logits 65.3% (7190 out of 11019)

Everytime after generating next logits 72.0% (7938 out of 11019)

Everytime after generating next logits 78.8% (8686 out of 11019)

Everytime after generating next logits 78.8% (8686 out of 11019)

Everytime after generating next logits 78.8% (8686 out of 11019)

Everytime after generating next logits 78.8% (8686 out of 11019)

Everytime after generating next logits 78.8% (8686 out of 11019)

Everytime after generating next logits 85.7% (9438 out of 11019)

Everytime after generating next logits 92.5% (10190 out of 11019)

Everytime after generating next logits 92.5% (10190 out of 11019)
Over the Gulf of Mexico oil at 11 billion reported on BBC . Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater was the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
iter = 3 reward = 18
final_logits =  tensor([[[ 2.6147e-02,  3.8650e+00,  6.5816e-01,  ..., -2.6511e+00,
           7.5608e-01,  1.0366e+00],
         [-7.5358e-03,  3.5523e+00,  4.4137e-01,  ..., -3.4249e+00,
           1.6826e+00, -7.2364e-01],
         [-4.3654e-02,  2.6854e+00,  2.3416e-01,  ..., -4.2378e+00,
          -2.9806e-01, -1.4089e+00],
         ...,
         [-3.4094e-02,  9.4736e+00, -6.1320e-02,  ..., -4.0446e-01,
          -1.3725e+00,  4.2068e-01],
         [-3.4961e-02,  9.2752e+00, -5.2926e-02,  ..., -4.5512e-01,
          -1.3458e+00,  4.0799e-01],
         [-3.5567e-02,  9.2662e+00, -5.3088e-02,  ..., -4.6016e-01,
          -1.3360e+00,  3.9401e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.2791, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(11.9628, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 64.8% (7140 out of 11019)

Everytime after generating next logits 66.5% (7330 out of 11019)

Everytime after generating next logits 68.2% (7520 out of 11019)

Everytime after generating next logits 68.2% (7520 out of 11019)

Everytime after generating next logits 68.2% (7520 out of 11019)

Everytime after generating next logits 68.2% (7520 out of 11019)

Everytime after generating next logits 68.2% (7520 out of 11019)

Everytime after generating next logits 70.0% (7712 out of 11019)

Everytime after generating next logits 71.7% (7904 out of 11019)

Everytime after generating next logits 71.7% (7904 out of 11019)

Everytime after generating next logits 71.7% (7904 out of 11019)

Everytime after generating next logits 71.7% (7904 out of 11019)

Everytime after generating next logits 73.5% (8098 out of 11019)

Everytime after generating next logits 75.3% (8292 out of 11019)

Everytime after generating next logits 75.3% (8292 out of 11019)

Everytime after generating next logits 75.3% (8292 out of 11019)

Everytime after generating next logits 75.3% (8292 out of 11019)

Everytime after generating next logits 75.3% (8292 out of 11019)

Everytime after generating next logits 77.0% (8488 out of 11019)

Everytime after generating next logits 78.8% (8684 out of 11019)

Everytime after generating next logits 78.8% (8684 out of 11019)

Everytime after generating next logits 78.8% (8684 out of 11019)

Everytime after generating next logits 78.8% (8684 out of 11019)

Everytime after generating next logits 80.6% (8882 out of 11019)

Everytime after generating next logits 82.4% (9080 out of 11019)

Everytime after generating next logits 82.4% (9080 out of 11019)

Everytime after generating next logits 82.4% (9080 out of 11019)

Everytime after generating next logits 82.4% (9080 out of 11019)

Everytime after generating next logits 82.4% (9080 out of 11019)

Everytime after generating next logits 84.2% (9280 out of 11019)

Everytime after generating next logits 86.0% (9480 out of 11019)

Everytime after generating next logits 86.0% (9480 out of 11019)

Everytime after generating next logits 86.0% (9480 out of 11019)

Everytime after generating next logits 86.0% (9480 out of 11019)

Everytime after generating next logits 87.9% (9682 out of 11019)

Everytime after generating next logits 89.7% (9884 out of 11019)

Everytime after generating next logits 89.7% (9884 out of 11019)

Everytime after generating next logits 89.7% (9884 out of 11019)

Everytime after generating next logits 89.7% (9884 out of 11019)

Everytime after generating next logits 89.7% (9884 out of 11019)

Everytime after generating next logits 91.6% (10088 out of 11019)

Everytime after generating next logits 93.4% (10292 out of 11019)

Everytime after generating next logits 93.4% (10292 out of 11019)

Everytime after generating next logits 93.4% (10292 out of 11019)

Everytime after generating next logits 93.4% (10292 out of 11019)

Everytime after generating next logits 95.3% (10498 out of 11019)

Everytime after generating next logits 97.1% (10704 out of 11019)

Everytime after generating next logits 97.1% (10704 out of 11019)

Everytime after generating next logits 97.1% (10704 out of 11019)

Everytime after generating next logits 97.1% (10704 out of 11019)

Everytime after generating next logits 99.0% (10912 out of 11019)

Everytime after generating next logits 66.9% (7372 out of 11019)

Everytime after generating next logits 66.9% (7372 out of 11019)

Everytime after generating next logits 66.9% (7372 out of 11019)

Everytime after generating next logits 66.9% (7372 out of 11019)

Everytime after generating next logits 66.9% (7372 out of 11019)

Everytime after generating next logits 68.8% (7582 out of 11019)

Everytime after generating next logits 70.7% (7792 out of 11019)

Everytime after generating next logits 70.7% (7792 out of 11019)

Everytime after generating next logits 70.7% (7792 out of 11019)

Everytime after generating next logits 70.7% (7792 out of 11019)

Everytime after generating next logits 72.6% (8004 out of 11019)

Everytime after generating next logits 74.6% (8216 out of 11019)

Everytime after generating next logits 74.6% (8216 out of 11019)

Everytime after generating next logits 74.6% (8216 out of 11019)

Everytime after generating next logits 74.6% (8216 out of 11019)

Everytime after generating next logits 74.6% (8216 out of 11019)

Everytime after generating next logits 76.5% (8430 out of 11019)

Everytime after generating next logits 78.4% (8644 out of 11019)

Everytime after generating next logits 78.4% (8644 out of 11019)

Everytime after generating next logits 78.4% (8644 out of 11019)

Everytime after generating next logits 78.4% (8644 out of 11019)

Everytime after generating next logits 80.4% (8860 out of 11019)

Everytime after generating next logits 82.4% (9076 out of 11019)

Everytime after generating next logits 82.4% (9076 out of 11019)

Everytime after generating next logits 82.4% (9076 out of 11019)

Everytime after generating next logits 82.4% (9076 out of 11019)

Everytime after generating next logits 82.4% (9076 out of 11019)

Everytime after generating next logits 84.3% (9294 out of 11019)

Everytime after generating next logits 86.3% (9512 out of 11019)

Everytime after generating next logits 86.3% (9512 out of 11019)

Everytime after generating next logits 86.3% (9512 out of 11019)

Everytime after generating next logits 86.3% (9512 out of 11019)

Everytime after generating next logits 88.3% (9732 out of 11019)

Everytime after generating next logits 90.3% (9952 out of 11019)

Everytime after generating next logits 90.3% (9952 out of 11019)

Everytime after generating next logits 90.3% (9952 out of 11019)

Everytime after generating next logits 90.3% (9952 out of 11019)

Everytime after generating next logits 90.3% (9952 out of 11019)

Everytime after generating next logits 92.3% (10174 out of 11019)

Everytime after generating next logits 94.3% (10396 out of 11019)

Everytime after generating next logits 94.3% (10396 out of 11019)

Everytime after generating next logits 94.3% (10396 out of 11019)

Everytime after generating next logits 94.3% (10396 out of 11019)

Everytime after generating next logits 96.4% (10620 out of 11019)

Everytime after generating next logits 98.4% (10844 out of 11019)

Everytime after generating next logits 98.4% (10844 out of 11019)

Everytime after generating next logits 98.4% (10844 out of 11019)

Everytime after generating next logits 98.4% (10844 out of 11019)

Everytime after generating next logits 98.4% (10844 out of 11019)

Everytime after generating next logits 67.2% (7406 out of 11019)

Everytime after generating next logits 69.3% (7632 out of 11019)

Everytime after generating next logits 69.3% (7632 out of 11019)

Everytime after generating next logits 69.3% (7632 out of 11019)

Everytime after generating next logits 69.3% (7632 out of 11019)

Everytime after generating next logits 71.3% (7860 out of 11019)

Everytime after generating next logits 73.4% (8088 out of 11019)

Everytime after generating next logits 73.4% (8088 out of 11019)

Everytime after generating next logits 73.4% (8088 out of 11019)

Everytime after generating next logits 73.4% (8088 out of 11019)

Everytime after generating next logits 75.5% (8318 out of 11019)

Everytime after generating next logits 77.6% (8548 out of 11019)

Everytime after generating next logits 77.6% (8548 out of 11019)

Everytime after generating next logits 77.6% (8548 out of 11019)

Everytime after generating next logits 77.6% (8548 out of 11019)

Everytime after generating next logits 77.6% (8548 out of 11019)

Everytime after generating next logits 79.7% (8780 out of 11019)

Everytime after generating next logits 81.8% (9012 out of 11019)

Everytime after generating next logits 81.8% (9012 out of 11019)

Everytime after generating next logits 81.8% (9012 out of 11019)

Everytime after generating next logits 81.8% (9012 out of 11019)

Everytime after generating next logits 83.9% (9246 out of 11019)

Everytime after generating next logits 86.0% (9480 out of 11019)

Everytime after generating next logits 86.0% (9480 out of 11019)

Everytime after generating next logits 86.0% (9480 out of 11019)

Everytime after generating next logits 86.0% (9480 out of 11019)

Everytime after generating next logits 86.0% (9480 out of 11019)

Everytime after generating next logits 88.2% (9716 out of 11019)

Everytime after generating next logits 90.3% (9952 out of 11019)

Everytime after generating next logits 90.3% (9952 out of 11019)

Everytime after generating next logits 90.3% (9952 out of 11019)

Everytime after generating next logits 90.3% (9952 out of 11019)

Everytime after generating next logits 92.5% (10190 out of 11019)

Everytime after generating next logits 94.6% (10428 out of 11019)

Everytime after generating next logits 94.6% (10428 out of 11019)

Everytime after generating next logits 94.6% (10428 out of 11019)

Everytime after generating next logits 94.6% (10428 out of 11019)

Everytime after generating next logits 94.6% (10428 out of 11019)

Everytime after generating next logits 96.8% (10668 out of 11019)

Everytime after generating next logits 99.0% (10908 out of 11019)

Everytime after generating next logits 99.0% (10908 out of 11019)

Everytime after generating next logits 99.0% (10908 out of 11019)

Everytime after generating next logits 99.0% (10908 out of 11019)

Everytime after generating next logits 67.5% (7438 out of 11019)

Everytime after generating next logits 69.7% (7680 out of 11019)

Everytime after generating next logits 69.7% (7680 out of 11019)

Everytime after generating next logits 69.7% (7680 out of 11019)

Everytime after generating next logits 69.7% (7680 out of 11019)

Everytime after generating next logits 69.7% (7680 out of 11019)

Everytime after generating next logits 71.9% (7924 out of 11019)

Everytime after generating next logits 74.1% (8168 out of 11019)

Everytime after generating next logits 74.1% (8168 out of 11019)

Everytime after generating next logits 74.1% (8168 out of 11019)

Everytime after generating next logits 74.1% (8168 out of 11019)

Everytime after generating next logits 76.4% (8414 out of 11019)

Everytime after generating next logits 78.6% (8660 out of 11019)

Everytime after generating next logits 78.6% (8660 out of 11019)

Everytime after generating next logits 78.6% (8660 out of 11019)

Everytime after generating next logits 78.6% (8660 out of 11019)

Everytime after generating next logits 78.6% (8660 out of 11019)

Everytime after generating next logits 80.8% (8908 out of 11019)

Everytime after generating next logits 83.1% (9156 out of 11019)

Everytime after generating next logits 83.1% (9156 out of 11019)

Everytime after generating next logits 83.1% (9156 out of 11019)

Everytime after generating next logits 83.1% (9156 out of 11019)

Everytime after generating next logits 85.4% (9406 out of 11019)

Everytime after generating next logits 87.6% (9656 out of 11019)

Everytime after generating next logits 87.6% (9656 out of 11019)

Everytime after generating next logits 87.6% (9656 out of 11019)

Everytime after generating next logits 87.6% (9656 out of 11019)

Everytime after generating next logits 89.9% (9908 out of 11019)

Everytime after generating next logits 92.2% (10160 out of 11019)

Everytime after generating next logits 92.2% (10160 out of 11019)

Everytime after generating next logits 92.2% (10160 out of 11019)

Everytime after generating next logits 92.2% (10160 out of 11019)

Everytime after generating next logits 92.2% (10160 out of 11019)

Everytime after generating next logits 94.5% (10414 out of 11019)

Everytime after generating next logits 96.8% (10668 out of 11019)

Everytime after generating next logits 96.8% (10668 out of 11019)

Everytime after generating next logits 96.8% (10668 out of 11019)

Everytime after generating next logits 96.8% (10668 out of 11019)

Everytime after generating next logits 99.1% (10924 out of 11019)

Everytime after generating next logits 67.8% (7468 out of 11019)

Everytime after generating next logits 67.8% (7468 out of 11019)

Everytime after generating next logits 67.8% (7468 out of 11019)

Everytime after generating next logits 67.8% (7468 out of 11019)

Everytime after generating next logits 67.8% (7468 out of 11019)

Everytime after generating next logits 70.1% (7726 out of 11019)

Everytime after generating next logits 72.5% (7984 out of 11019)

Everytime after generating next logits 72.5% (7984 out of 11019)

Everytime after generating next logits 72.5% (7984 out of 11019)

Everytime after generating next logits 72.5% (7984 out of 11019)

Everytime after generating next logits 74.8% (8244 out of 11019)

Everytime after generating next logits 77.2% (8504 out of 11019)

Everytime after generating next logits 77.2% (8504 out of 11019)

Everytime after generating next logits 77.2% (8504 out of 11019)

Everytime after generating next logits 77.2% (8504 out of 11019)

Everytime after generating next logits 77.2% (8504 out of 11019)

Everytime after generating next logits 79.6% (8766 out of 11019)

Everytime after generating next logits 81.9% (9028 out of 11019)

Everytime after generating next logits 81.9% (9028 out of 11019)

Everytime after generating next logits 81.9% (9028 out of 11019)

Everytime after generating next logits 81.9% (9028 out of 11019)

Everytime after generating next logits 84.3% (9292 out of 11019)

Everytime after generating next logits 86.7% (9556 out of 11019)

Everytime after generating next logits 86.7% (9556 out of 11019)

Everytime after generating next logits 86.7% (9556 out of 11019)

Everytime after generating next logits 86.7% (9556 out of 11019)

Everytime after generating next logits 86.7% (9556 out of 11019)

Everytime after generating next logits 89.1% (9822 out of 11019)

Everytime after generating next logits 91.6% (10088 out of 11019)

Everytime after generating next logits 91.6% (10088 out of 11019)

Everytime after generating next logits 91.6% (10088 out of 11019)

Everytime after generating next logits 91.6% (10088 out of 11019)

Everytime after generating next logits 94.0% (10356 out of 11019)

Everytime after generating next logits 96.4% (10624 out of 11019)

Everytime after generating next logits 96.4% (10624 out of 11019)

Everytime after generating next logits 96.4% (10624 out of 11019)

Everytime after generating next logits 96.4% (10624 out of 11019)

Everytime after generating next logits 96.4% (10624 out of 11019)

Everytime after generating next logits 98.9% (10894 out of 11019)

Everytime after generating next logits 68.0% (7496 out of 11019)

Everytime after generating next logits 68.0% (7496 out of 11019)

Everytime after generating next logits 68.0% (7496 out of 11019)

Everytime after generating next logits 68.0% (7496 out of 11019)

Everytime after generating next logits 70.5% (7768 out of 11019)

Everytime after generating next logits 73.0% (8040 out of 11019)

Everytime after generating next logits 73.0% (8040 out of 11019)

Everytime after generating next logits 73.0% (8040 out of 11019)

Everytime after generating next logits 73.0% (8040 out of 11019)

Everytime after generating next logits 75.5% (8314 out of 11019)

Everytime after generating next logits 77.9% (8588 out of 11019)

Everytime after generating next logits 77.9% (8588 out of 11019)

Everytime after generating next logits 77.9% (8588 out of 11019)

Everytime after generating next logits 77.9% (8588 out of 11019)

Everytime after generating next logits 77.9% (8588 out of 11019)

Everytime after generating next logits 80.4% (8864 out of 11019)

Everytime after generating next logits 82.9% (9140 out of 11019)

Everytime after generating next logits 82.9% (9140 out of 11019)

Everytime after generating next logits 82.9% (9140 out of 11019)

Everytime after generating next logits 82.9% (9140 out of 11019)

Everytime after generating next logits 85.5% (9418 out of 11019)

Everytime after generating next logits 88.0% (9696 out of 11019)

Everytime after generating next logits 88.0% (9696 out of 11019)

Everytime after generating next logits 88.0% (9696 out of 11019)

Everytime after generating next logits 88.0% (9696 out of 11019)

Everytime after generating next logits 88.0% (9696 out of 11019)

Everytime after generating next logits 90.5% (9976 out of 11019)

Everytime after generating next logits 93.1% (10256 out of 11019)

Everytime after generating next logits 93.1% (10256 out of 11019)

Everytime after generating next logits 93.1% (10256 out of 11019)

Everytime after generating next logits 93.1% (10256 out of 11019)

Everytime after generating next logits 95.6% (10538 out of 11019)

Everytime after generating next logits 98.2% (10820 out of 11019)

Everytime after generating next logits 98.2% (10820 out of 11019)

Everytime after generating next logits 98.2% (10820 out of 11019)

Everytime after generating next logits 98.2% (10820 out of 11019)

Everytime after generating next logits 98.2% (10820 out of 11019)

Everytime after generating next logits 68.3% (7522 out of 11019)

Everytime after generating next logits 70.8% (7806 out of 11019)

Everytime after generating next logits 70.8% (7806 out of 11019)

Everytime after generating next logits 70.8% (7806 out of 11019)

Everytime after generating next logits 70.8% (7806 out of 11019)

Everytime after generating next logits 73.4% (8092 out of 11019)

Everytime after generating next logits 76.0% (8378 out of 11019)

Everytime after generating next logits 76.0% (8378 out of 11019)

Everytime after generating next logits 76.0% (8378 out of 11019)

Everytime after generating next logits 76.0% (8378 out of 11019)

Everytime after generating next logits 76.0% (8378 out of 11019)

Everytime after generating next logits 78.6% (8666 out of 11019)

Everytime after generating next logits 81.3% (8954 out of 11019)

Everytime after generating next logits 81.3% (8954 out of 11019)

Everytime after generating next logits 81.3% (8954 out of 11019)

Everytime after generating next logits 81.3% (8954 out of 11019)

Everytime after generating next logits 83.9% (9244 out of 11019)

Everytime after generating next logits 86.5% (9534 out of 11019)

Everytime after generating next logits 86.5% (9534 out of 11019)

Everytime after generating next logits 86.5% (9534 out of 11019)

Everytime after generating next logits 86.5% (9534 out of 11019)

Everytime after generating next logits 86.5% (9534 out of 11019)

Everytime after generating next logits 89.2% (9826 out of 11019)

Everytime after generating next logits 91.8% (10118 out of 11019)

Everytime after generating next logits 91.8% (10118 out of 11019)

Everytime after generating next logits 91.8% (10118 out of 11019)

Everytime after generating next logits 91.8% (10118 out of 11019)

Everytime after generating next logits 94.5% (10412 out of 11019)

Everytime after generating next logits 97.2% (10706 out of 11019)

Everytime after generating next logits 97.2% (10706 out of 11019)

Everytime after generating next logits 97.2% (10706 out of 11019)

Everytime after generating next logits 97.2% (10706 out of 11019)

Everytime after generating next logits 99.8% (11002 out of 11019)

Everytime after generating next logits 68.5% (7548 out of 11019)

Everytime after generating next logits 68.5% (7548 out of 11019)

Everytime after generating next logits 68.5% (7548 out of 11019)

Everytime after generating next logits 68.5% (7548 out of 11019)

Everytime after generating next logits 68.5% (7548 out of 11019)

Everytime after generating next logits 71.2% (7846 out of 11019)

Everytime after generating next logits 73.9% (8144 out of 11019)

Everytime after generating next logits 73.9% (8144 out of 11019)

Everytime after generating next logits 73.9% (8144 out of 11019)

Everytime after generating next logits 73.9% (8144 out of 11019)

Everytime after generating next logits 76.6% (8444 out of 11019)

Everytime after generating next logits 79.4% (8744 out of 11019)

Everytime after generating next logits 79.4% (8744 out of 11019)

Everytime after generating next logits 79.4% (8744 out of 11019)

Everytime after generating next logits 79.4% (8744 out of 11019)

Everytime after generating next logits 79.4% (8744 out of 11019)

Everytime after generating next logits 82.1% (9046 out of 11019)

Everytime after generating next logits 84.8% (9348 out of 11019)

Everytime after generating next logits 84.8% (9348 out of 11019)

Everytime after generating next logits 84.8% (9348 out of 11019)

Everytime after generating next logits 84.8% (9348 out of 11019)

Everytime after generating next logits 87.6% (9652 out of 11019)

Everytime after generating next logits 90.4% (9956 out of 11019)

Everytime after generating next logits 90.4% (9956 out of 11019)

Everytime after generating next logits 90.4% (9956 out of 11019)

Everytime after generating next logits 90.4% (9956 out of 11019)

Everytime after generating next logits 90.4% (9956 out of 11019)

Everytime after generating next logits 93.1% (10262 out of 11019)

Everytime after generating next logits 95.9% (10568 out of 11019)

Everytime after generating next logits 95.9% (10568 out of 11019)

Everytime after generating next logits 95.9% (10568 out of 11019)

Everytime after generating next logits 95.9% (10568 out of 11019)

Everytime after generating next logits 98.7% (10876 out of 11019)

Everytime after generating next logits 68.7% (7572 out of 11019)

Everytime after generating next logits 68.7% (7572 out of 11019)

Everytime after generating next logits 68.7% (7572 out of 11019)

Everytime after generating next logits 68.7% (7572 out of 11019)

Everytime after generating next logits 68.7% (7572 out of 11019)

Everytime after generating next logits 71.5% (7882 out of 11019)

Everytime after generating next logits 74.3% (8192 out of 11019)

Everytime after generating next logits 74.3% (8192 out of 11019)

Everytime after generating next logits 74.3% (8192 out of 11019)

Everytime after generating next logits 74.3% (8192 out of 11019)

Everytime after generating next logits 77.2% (8504 out of 11019)

Everytime after generating next logits 80.0% (8816 out of 11019)

Everytime after generating next logits 80.0% (8816 out of 11019)

Everytime after generating next logits 80.0% (8816 out of 11019)

Everytime after generating next logits 80.0% (8816 out of 11019)

Everytime after generating next logits 80.0% (8816 out of 11019)

Everytime after generating next logits 82.9% (9130 out of 11019)

Everytime after generating next logits 85.7% (9444 out of 11019)

Everytime after generating next logits 85.7% (9444 out of 11019)

Everytime after generating next logits 85.7% (9444 out of 11019)

Everytime after generating next logits 85.7% (9444 out of 11019)

Everytime after generating next logits 88.6% (9760 out of 11019)

Everytime after generating next logits 91.4% (10076 out of 11019)

Everytime after generating next logits 91.4% (10076 out of 11019)

Everytime after generating next logits 91.4% (10076 out of 11019)

Everytime after generating next logits 91.4% (10076 out of 11019)

Everytime after generating next logits 94.3% (10394 out of 11019)

Everytime after generating next logits 97.2% (10712 out of 11019)

Everytime after generating next logits 97.2% (10712 out of 11019)

Everytime after generating next logits 97.2% (10712 out of 11019)

Everytime after generating next logits 97.2% (10712 out of 11019)

Everytime after generating next logits 97.2% (10712 out of 11019)

Everytime after generating next logits 68.9% (7594 out of 11019)

Everytime after generating next logits 71.8% (7914 out of 11019)

Everytime after generating next logits 71.8% (7914 out of 11019)

Everytime after generating next logits 71.8% (7914 out of 11019)

Everytime after generating next logits 71.8% (7914 out of 11019)

Everytime after generating next logits 74.7% (8236 out of 11019)

Everytime after generating next logits 77.7% (8558 out of 11019)

Everytime after generating next logits 77.7% (8558 out of 11019)

Everytime after generating next logits 77.7% (8558 out of 11019)

Everytime after generating next logits 77.7% (8558 out of 11019)

Everytime after generating next logits 77.7% (8558 out of 11019)

Everytime after generating next logits 80.6% (8882 out of 11019)

Everytime after generating next logits 83.5% (9206 out of 11019)

Everytime after generating next logits 83.5% (9206 out of 11019)

Everytime after generating next logits 83.5% (9206 out of 11019)

Everytime after generating next logits 83.5% (9206 out of 11019)

Everytime after generating next logits 86.5% (9532 out of 11019)

Everytime after generating next logits 89.5% (9858 out of 11019)

Everytime after generating next logits 89.5% (9858 out of 11019)

Everytime after generating next logits 89.5% (9858 out of 11019)

Everytime after generating next logits 89.5% (9858 out of 11019)

Everytime after generating next logits 89.5% (9858 out of 11019)

Everytime after generating next logits 92.4% (10186 out of 11019)

Everytime after generating next logits 95.4% (10514 out of 11019)

Everytime after generating next logits 95.4% (10514 out of 11019)

Everytime after generating next logits 95.4% (10514 out of 11019)

Everytime after generating next logits 95.4% (10514 out of 11019)

Everytime after generating next logits 98.4% (10844 out of 11019)

Everytime after generating next logits 69.1% (7616 out of 11019)

Everytime after generating next logits 69.1% (7616 out of 11019)

Everytime after generating next logits 69.1% (7616 out of 11019)

Everytime after generating next logits 69.1% (7616 out of 11019)

Everytime after generating next logits 69.1% (7616 out of 11019)

Everytime after generating next logits 72.1% (7948 out of 11019)

Everytime after generating next logits 75.1% (8280 out of 11019)

Everytime after generating next logits 75.1% (8280 out of 11019)

Everytime after generating next logits 75.1% (8280 out of 11019)

Everytime after generating next logits 75.1% (8280 out of 11019)

Everytime after generating next logits 78.2% (8614 out of 11019)

Everytime after generating next logits 81.2% (8948 out of 11019)

Everytime after generating next logits 81.2% (8948 out of 11019)

Everytime after generating next logits 81.2% (8948 out of 11019)

Everytime after generating next logits 81.2% (8948 out of 11019)

Everytime after generating next logits 81.2% (8948 out of 11019)

Everytime after generating next logits 84.3% (9284 out of 11019)

Everytime after generating next logits 87.3% (9620 out of 11019)

Everytime after generating next logits 87.3% (9620 out of 11019)

Everytime after generating next logits 87.3% (9620 out of 11019)

Everytime after generating next logits 87.3% (9620 out of 11019)

Everytime after generating next logits 90.4% (9958 out of 11019)

Everytime after generating next logits 93.4% (10296 out of 11019)

Everytime after generating next logits 93.4% (10296 out of 11019)

Everytime after generating next logits 93.4% (10296 out of 11019)

Everytime after generating next logits 93.4% (10296 out of 11019)

Everytime after generating next logits 96.5% (10636 out of 11019)

Everytime after generating next logits 99.6% (10976 out of 11019)

Everytime after generating next logits 99.6% (10976 out of 11019)

Everytime after generating next logits 99.6% (10976 out of 11019)

Everytime after generating next logits 99.6% (10976 out of 11019)

Everytime after generating next logits 99.6% (10976 out of 11019)

Everytime after generating next logits 72.3% (7968 out of 11019)

Everytime after generating next logits 75.4% (8310 out of 11019)

Everytime after generating next logits 75.4% (8310 out of 11019)

Everytime after generating next logits 75.4% (8310 out of 11019)

Everytime after generating next logits 75.4% (8310 out of 11019)

Everytime after generating next logits 78.5% (8654 out of 11019)

Everytime after generating next logits 81.7% (8998 out of 11019)

Everytime after generating next logits 81.7% (8998 out of 11019)

Everytime after generating next logits 81.7% (8998 out of 11019)

Everytime after generating next logits 81.7% (8998 out of 11019)

Everytime after generating next logits 81.7% (8998 out of 11019)

Everytime after generating next logits 84.8% (9344 out of 11019)

Everytime after generating next logits 87.9% (9690 out of 11019)

Everytime after generating next logits 87.9% (9690 out of 11019)

Everytime after generating next logits 87.9% (9690 out of 11019)

Everytime after generating next logits 87.9% (9690 out of 11019)

Everytime after generating next logits 91.1% (10038 out of 11019)

Everytime after generating next logits 94.3% (10386 out of 11019)

Everytime after generating next logits 94.3% (10386 out of 11019)

Everytime after generating next logits 94.3% (10386 out of 11019)

Everytime after generating next logits 94.3% (10386 out of 11019)

Everytime after generating next logits 94.3% (10386 out of 11019)

Everytime after generating next logits 97.4% (10736 out of 11019)

Everytime after generating next logits 72.5% (7986 out of 11019)

Everytime after generating next logits 72.5% (7986 out of 11019)

Everytime after generating next logits 72.5% (7986 out of 11019)

Everytime after generating next logits 72.5% (7986 out of 11019)

Everytime after generating next logits 75.7% (8338 out of 11019)

Everytime after generating next logits 78.9% (8690 out of 11019)

Everytime after generating next logits 78.9% (8690 out of 11019)

Everytime after generating next logits 78.9% (8690 out of 11019)

Everytime after generating next logits 78.9% (8690 out of 11019)

Everytime after generating next logits 78.9% (8690 out of 11019)

Everytime after generating next logits 82.1% (9044 out of 11019)

Everytime after generating next logits 85.3% (9398 out of 11019)

Everytime after generating next logits 85.3% (9398 out of 11019)

Everytime after generating next logits 85.3% (9398 out of 11019)

Everytime after generating next logits 85.3% (9398 out of 11019)

Everytime after generating next logits 88.5% (9754 out of 11019)

Everytime after generating next logits 91.8% (10110 out of 11019)

Everytime after generating next logits 91.8% (10110 out of 11019)

Everytime after generating next logits 91.8% (10110 out of 11019)

Everytime after generating next logits 91.8% (10110 out of 11019)

Everytime after generating next logits 91.8% (10110 out of 11019)

Everytime after generating next logits 95.0% (10468 out of 11019)

Everytime after generating next logits 98.2% (10826 out of 11019)

Everytime after generating next logits 98.2% (10826 out of 11019)

Everytime after generating next logits 98.2% (10826 out of 11019)

Everytime after generating next logits 98.2% (10826 out of 11019)

Everytime after generating next logits 72.6% (8004 out of 11019)

Everytime after generating next logits 75.9% (8364 out of 11019)

Everytime after generating next logits 75.9% (8364 out of 11019)

Everytime after generating next logits 75.9% (8364 out of 11019)

Everytime after generating next logits 75.9% (8364 out of 11019)

Everytime after generating next logits 79.2% (8726 out of 11019)

Everytime after generating next logits 82.5% (9088 out of 11019)

Everytime after generating next logits 82.5% (9088 out of 11019)

Everytime after generating next logits 82.5% (9088 out of 11019)

Everytime after generating next logits 82.5% (9088 out of 11019)

Everytime after generating next logits 82.5% (9088 out of 11019)

Everytime after generating next logits 85.8% (9452 out of 11019)

Everytime after generating next logits 89.1% (9816 out of 11019)

Everytime after generating next logits 89.1% (9816 out of 11019)

Everytime after generating next logits 89.1% (9816 out of 11019)

Everytime after generating next logits 89.1% (9816 out of 11019)

Everytime after generating next logits 92.4% (10182 out of 11019)

Everytime after generating next logits 95.7% (10548 out of 11019)

Everytime after generating next logits 95.7% (10548 out of 11019)

Everytime after generating next logits 95.7% (10548 out of 11019)

Everytime after generating next logits 95.7% (10548 out of 11019)

Everytime after generating next logits 95.7% (10548 out of 11019)

Everytime after generating next logits 99.1% (10916 out of 11019)

Everytime after generating next logits 72.8% (8022 out of 11019)

Everytime after generating next logits 72.8% (8022 out of 11019)

Everytime after generating next logits 72.8% (8022 out of 11019)

Everytime after generating next logits 72.8% (8022 out of 11019)

Everytime after generating next logits 76.2% (8392 out of 11019)

Everytime after generating next logits 79.5% (8762 out of 11019)

Everytime after generating next logits 79.5% (8762 out of 11019)

Everytime after generating next logits 79.5% (8762 out of 11019)

Everytime after generating next logits 79.5% (8762 out of 11019)

Everytime after generating next logits 79.5% (8762 out of 11019)

Everytime after generating next logits 82.9% (9134 out of 11019)

Everytime after generating next logits 86.3% (9506 out of 11019)

Everytime after generating next logits 86.3% (9506 out of 11019)

Everytime after generating next logits 86.3% (9506 out of 11019)

Everytime after generating next logits 86.3% (9506 out of 11019)

Everytime after generating next logits 89.7% (9880 out of 11019)

Everytime after generating next logits 93.1% (10254 out of 11019)

Everytime after generating next logits 93.1% (10254 out of 11019)

Everytime after generating next logits 93.1% (10254 out of 11019)

Everytime after generating next logits 93.1% (10254 out of 11019)

Everytime after generating next logits 93.1% (10254 out of 11019)

Everytime after generating next logits 96.5% (10630 out of 11019)

Everytime after generating next logits 99.9% (11006 out of 11019)

Everytime after generating next logits 99.9% (11006 out of 11019)
Over the Gulf of Mexico oil reports on BBC 11 billion dollars . Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater Deepwater was the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
iter = 4 reward = 20
final_logits =  tensor([[[ 3.9630e-03,  3.7034e+00,  6.6278e-01,  ..., -2.7127e+00,
           1.0537e+00,  9.8922e-01],
         [-2.1505e-02,  3.6122e+00,  4.2692e-01,  ..., -3.3417e+00,
           1.7472e+00, -8.2609e-01],
         [-5.7272e-02,  2.7284e+00,  2.3839e-01,  ..., -4.1425e+00,
          -2.3184e-01, -1.4408e+00],
         ...,
         [-7.1210e-02,  9.8014e+00, -1.3788e-01,  ..., -5.2599e-01,
          -1.6327e+00,  4.4688e-01],
         [-7.1661e-02,  9.6088e+00, -1.3006e-01,  ..., -5.5965e-01,
          -1.6207e+00,  4.4443e-01],
         [-7.2075e-02,  9.6146e+00, -1.3053e-01,  ..., -5.5411e-01,
          -1.6187e+00,  4.3193e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.7704, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(12.3974, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  20
20
{'input_ids': tensor([[[ 7915, 26939,  4355,  1194,   244, 16036,   762, 14567,  5823,  8437,
            117,   364,   109, 19954,   113,   109,  7175,   113,  3064,   235,
            149,   314,   210,   110,   107,   139,   908,   317,  6299,   111,
           1174,   117,  8988,   153, 19347,   578,   110,   107,   343,   136,
            232,   205,   113,   109, 27736,   127,  2609,   110,   107,   139,
          27736,   127,   146,  1622,   115,   762,   110,   108,   130,   109,
           6442,  1034,   116,  2040,  9518,  1574,   135,  7915,   110,   108,
            155,  3040,   299,   141,  2926, 21615, 22611,   116,   120,   195,
           2443,   112,  2512,   109, 15737,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)

Everytime after generating next logits 63.1% (6954 out of 11019)
The Gulf oil beds are busiest reported in Mexico . The Gulf oil beds are still affected by the spill .
iter = 0 reward = 5
final_logits =  tensor([[[-5.6691e-02,  3.7849e+00,  7.2384e-01,  ..., -2.9514e+00,
          -1.4956e+00, -3.9080e-01],
         [-5.3463e-02,  1.5309e+00,  5.3099e-01,  ..., -6.4335e+00,
          -1.0677e+00, -1.2190e+00],
         [-1.4753e-01,  9.5422e-01, -3.2980e-01,  ..., -6.2805e+00,
          -3.2532e+00,  3.4268e-01],
         ...,
         [-2.8004e-01,  2.2086e+00, -8.8132e-01,  ..., -2.6315e+00,
          -1.9085e+00, -6.3735e-01],
         [-1.6718e-02,  4.2530e+00, -1.6653e-03,  ..., -1.6267e+00,
          -1.5165e+00,  6.0591e-01],
         [-3.0434e-02,  1.2238e+01,  4.4819e-01,  ..., -3.9222e-01,
          -1.9127e+00, -4.7628e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-38.4202, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(933.2924, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)

Everytime after generating next logits 41.4% (4558 out of 11019)
Reports of Christmas oil hit most of Mexico . Gulf oil beds contain massive oil spill .
iter = 1 reward = 9
final_logits =  tensor([[[-0.0818,  3.7003,  0.6223,  ..., -3.0723, -0.8533, -0.9364],
         [-0.1317,  1.2285, -0.4989,  ..., -3.9977, -0.2625, -1.0709],
         [-0.1666,  1.5730,  0.0629,  ..., -3.0485, -1.1088, -2.1759],
         ...,
         [-0.3283,  1.4633, -0.5397,  ..., -1.1903, -1.8918, -1.3095],
         [-0.0260,  3.7674,  0.0678,  ..., -1.8038, -1.1106,  0.3042],
         [-0.0942, 10.7031,  0.2534,  ..., -0.7723, -2.1530, -0.7708]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-36.5138, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(780.6843, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)

Everytime after generating next logits 41.4% (4566 out of 11019)
Around Christmas oil is most reported year . Gulf oil hit beds contain massive but killed .
iter = 2 reward = 7
final_logits =  tensor([[[-0.0413,  3.6250,  0.6591,  ..., -3.3261, -0.2278, -0.8296],
         [-0.0660,  3.2910,  0.1731,  ..., -3.9521,  2.1574, -2.1887],
         [-0.1564,  2.4117, -0.4242,  ..., -6.2747, -3.0980, -1.5468],
         ...,
         [-0.1845,  2.3655, -0.4031,  ..., -2.6599, -1.0374, -2.4987],
         [-0.0235,  3.4096,  0.1253,  ..., -0.8525, -0.9116, -0.0771],
         [-0.0444, 11.5457,  0.3336,  ..., -0.6558, -1.2855, -0.6121]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-19.2351, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(356.3022, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)
Christmas oil hits are most by Mexico but killed . Gulf oil hit fishermen are busiest year .
iter = 3 reward = 5
final_logits =  tensor([[[-0.0154,  3.7845,  0.6533,  ..., -3.2983,  0.0821, -0.7509],
         [-0.1138,  2.3148, -0.2365,  ..., -5.7046, -2.6102, -1.5625],
         [-0.1365,  1.2717, -0.1435,  ..., -4.5163, -1.6979, -1.0009],
         ...,
         [-0.1167,  3.3378, -0.7808,  ..., -2.8240, -0.7355, -1.6246],
         [ 0.0189,  3.8519, -0.0533,  ..., -0.9123, -0.6929,  0.1481],
         [-0.0260, 12.2425,  0.3630,  ..., -0.4380, -0.6237, -0.4699]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-12.1717, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(111.1363, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)

Everytime after generating next logits 41.5% (4568 out of 11019)
Christmas oil hits are most of Mexico but killed . Gulf oil hit fishermen are busiest year .
iter = 4 reward = 5
final_logits =  tensor([[[ 8.2020e-03,  3.8548e+00,  6.5326e-01,  ..., -3.3196e+00,
           1.4862e-01, -6.0491e-01],
         [-8.6838e-02,  2.2147e+00, -1.9459e-01,  ..., -5.6605e+00,
          -2.6243e+00, -1.5315e+00],
         [-1.0545e-01,  1.1751e+00, -1.4058e-01,  ..., -4.6640e+00,
          -1.6252e+00, -9.9006e-01],
         ...,
         [-1.0368e-01,  3.4645e+00, -7.9758e-01,  ..., -2.7685e+00,
          -7.6956e-01, -1.8305e+00],
         [ 2.5174e-02,  3.9358e+00, -7.6227e-02,  ..., -9.3903e-01,
          -6.4601e-01,  6.4291e-02],
         [-1.7818e-02,  1.2327e+01,  3.2305e-01,  ..., -6.8234e-01,
          -6.4207e-01, -2.4585e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-2.1853, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.1168, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  5
5
topic:  ('bpoil_bbc',)
env initialized...

Before training:  41.5% (4572 out of 11019)
{'input_ids': tensor([[[  110,   105,  3893, 15737,  1034,  8255,   464, 16036,   134,   109,
           1816,  2447,   139,  1816,  2447,   649,   126,   117,  4562,   112,
          16036,   118,   203,   337,   983,   762,   121,  2134,  5626,   148,
            174, 12832,   279,   156,   113,   109,  1816,  2447,  1034,   116,
          18072,   141, 10162, 18232,   126,   112,   370,   203, 10971,   818,
            122, 16036,   110,   107,   139,  4635, 41588,   110,   108, 50595,
          81452, 60490,   457,   131,   304,   110,   108,   117,   114,  4055,
          17225,   113,   114,   883,   693,   111, 28286,   111,   117,   160,
           6155,   231,   459,   110,   107,   202,   456,   568,  7200,  9221,
           3893,  2777,   165,   109,  8255,   110,   107,   202,   984,  8255,
           4635, 20678,  4329,   115,  1185,   110,   107,   139,  1816,  2447,
            243,   109,  5626,  1065,   140, 39059,   110,   108,   162,   196,
            146,  3954,   109,  4976,  2098, 11976,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)
An oil giant has targeted Britain's Easter Island at June sponsorship . An oil giant has targeted Britain's Easter Easter Island at June .
iter = 0 reward = 4
final_logits =  tensor([[[ 1.1502e-01,  3.6947e+00,  9.2082e-01,  ..., -3.0825e+00,
           7.2634e-01, -1.1099e+00],
         [-6.3381e-03,  3.2254e+00,  3.5635e-01,  ..., -5.2887e+00,
           2.7150e-01,  1.5016e-01],
         [-9.2997e-02,  2.8263e+00, -1.6961e-02,  ..., -2.8418e+00,
           1.1440e+00, -1.2157e+00],
         ...,
         [-1.0109e-01,  2.8522e+00, -4.2437e-01,  ..., -4.2192e+00,
          -6.1325e-01, -3.9593e+00],
         [ 4.9710e-02,  2.1735e+00,  3.6165e-01,  ..., -1.9784e+00,
           1.3142e+00, -2.8124e-01],
         [-1.7987e-03,  1.1084e+01, -9.2202e-02,  ..., -3.1544e+00,
          -1.4703e+00, -2.9039e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.1602, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(11.2081, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)
An oil giant has targeted Britain's Easter Island at June sponsorship . An oil giant has targeted Britain's Easter Island at June .
iter = 1 reward = 4
final_logits =  tensor([[[ 1.1399e-01,  3.7184e+00,  9.3866e-01,  ..., -3.1247e+00,
           7.4359e-01, -1.0378e+00],
         [-2.8906e-03,  3.2843e+00,  3.3421e-01,  ..., -5.3871e+00,
           2.0562e-01,  1.4153e-01],
         [-8.9787e-02,  2.8357e+00, -1.1981e-02,  ..., -2.7911e+00,
           1.0952e+00, -1.2360e+00],
         ...,
         [-9.5061e-02,  2.8272e+00, -4.2231e-01,  ..., -4.1905e+00,
          -8.2270e-01, -4.2189e+00],
         [ 5.2453e-02,  2.5656e+00,  3.9572e-01,  ..., -2.1485e+00,
           1.2853e+00, -4.1763e-01],
         [-1.2886e-02,  1.1003e+01, -2.3192e-01,  ..., -3.1717e+00,
          -1.4031e+00, -3.6377e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(4.7538, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(46.4336, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)

Everytime after generating next logits 41.6% (4582 out of 11019)
An oil giant has targeted Britain's Easter Island at June sponsorship . An oil giant has targeted Britain's Easter Island at June sponsorship . An oil giant has targeted Britain's Easter Island at June .
iter = 2 reward = 6
final_logits =  tensor([[[ 9.4585e-02,  3.8235e+00,  8.9191e-01,  ..., -3.2077e+00,
           8.8362e-01, -1.1103e+00],
         [-4.4586e-03,  3.4709e+00,  3.0639e-01,  ..., -5.7191e+00,
           1.6172e-01, -6.7625e-02],
         [-9.0714e-02,  2.7535e+00, -2.3228e-02,  ..., -2.9117e+00,
           9.4173e-01, -1.2581e+00],
         ...,
         [-9.4032e-02,  3.7987e+00, -4.5885e-01,  ..., -4.2861e+00,
          -6.1870e-01, -4.2139e+00],
         [ 5.0369e-02,  2.7609e+00,  3.3004e-01,  ..., -2.1301e+00,
           1.3398e+00, -5.8717e-01],
         [-9.4048e-03,  1.1775e+01, -2.9700e-01,  ..., -3.4491e+00,
          -1.5142e+00, -2.8676e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(6.5346, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(113.6893, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)

Everytime after generating next logits 41.9% (4622 out of 11019)
An oil giant has targeted Britain's Easter Island at June sponsorship . An oil giant has targeted Britain's Easter Island at June sponsorship . An oil giant has targeted Britain's Easter Island at June .
iter = 3 reward = 6
final_logits =  tensor([[[ 8.8575e-02,  4.0368e+00,  8.1672e-01,  ..., -3.0574e+00,
           1.0763e+00, -1.3091e+00],
         [-4.4776e-03,  3.5096e+00,  3.3079e-01,  ..., -5.4482e+00,
           2.0066e-01, -3.8566e-01],
         [-8.9158e-02,  2.9274e+00, -3.2984e-02,  ..., -2.8698e+00,
           8.0901e-01, -1.3020e+00],
         ...,
         [-9.3516e-02,  3.9266e+00, -4.1602e-01,  ..., -4.0637e+00,
          -7.6198e-01, -4.2928e+00],
         [ 5.1830e-02,  4.1865e+00,  2.9072e-01,  ..., -2.3657e+00,
           7.3351e-01, -8.2546e-01],
         [-3.3248e-04,  1.1950e+01, -2.6486e-01,  ..., -3.3965e+00,
          -1.4163e+00, -2.9198e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(6.9374, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(151.8908, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)

Everytime after generating next logits 41.9% (4618 out of 11019)
An oil giant has targeted Britain's Easter Island at June sponsorship . An oil giant has targeted Britain's Easter Island at June sponsorship . An oil giant has targeted Britain's Easter Island at June sponsorship .
iter = 4 reward = 6
final_logits =  tensor([[[ 8.3038e-02,  4.2111e+00,  7.8425e-01,  ..., -3.2386e+00,
           9.7205e-01, -1.4190e+00],
         [-1.4287e-02,  3.5286e+00,  3.1426e-01,  ..., -5.4910e+00,
           1.4154e-01, -3.9252e-01],
         [-9.0513e-02,  2.9580e+00, -3.1912e-02,  ..., -2.9030e+00,
           7.9685e-01, -1.1997e+00],
         ...,
         [-9.3633e-02,  4.9453e+00, -3.8395e-01,  ..., -2.8030e+00,
           3.4266e-01, -3.0114e+00],
         [ 4.8116e-02,  3.6976e+00,  1.5634e-01,  ..., -2.1057e+00,
           8.8086e-01, -7.9971e-01],
         [ 6.2277e-03,  1.2817e+01, -1.7851e-01,  ..., -3.2039e+00,
          -1.2882e+00, -2.2887e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(6.9250, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(162.2392, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  6
6
{'input_ids': tensor([[[ 4123,   113,   219,  6234,   195,  6908,   115,   109, 75158,  1153,
           3893,  1419, 16036,   148,  6305,  3906,   142, 10580,   805,   113,
            203,  7175,   113,  3064,   762, 14567,  1407,  1104,   124,   203,
            387,   110,   107,   139,  1082,   110,   108,  1155,   204,   109,
           1339,   110,   108,   939,  1841,   115,   683,   113,   114,  1679,
            113,   461,  6234, 10361,  1055,   113,   203,  3954,   210,   124,
            109,  1917,  1030,   110,   107, 16036,  9619,  3582,  7217,   243,
            120,   339,  6234,   195,  6908,   115,   109,   856,  1153,   111,
          10390,   680,   196,   174,   263,   112,   535,  1055,   110,   107,
            139, 10580,   805,   140,  3530,   122,   109,   856,   244,   114,
            787,  9024,  8666,   126,   110,   107,  1263,  7217,   243,   109,
           4787,   170,   635,   109,  1153,   140, 10361,   169,   766,   122,
          10390,   680,   111,   186,   140,   220,  5313,  6596,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}

Before Sampling 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)

Everytime after generating next logits 42.1% (4636 out of 11019)
Images of the Gulf spill front of the spill front of the Gulf has been altered .
iter = 0 reward = 4
final_logits =  tensor([[[-5.3171e-02,  4.9523e+00, -1.6093e-01,  ..., -5.5049e+00,
           2.6607e-01,  2.0889e+00],
         [-5.9417e-02,  1.7207e+00, -4.2091e-01,  ..., -2.6631e+00,
          -3.8275e-01, -1.6474e+00],
         [-1.0534e-01,  1.7156e+00, -4.5636e-01,  ..., -5.8301e+00,
          -3.1054e-01,  9.5904e-01],
         ...,
         [-1.2909e-01,  2.9265e+00, -4.7834e-01,  ..., -2.6938e+00,
          -7.5643e-01, -1.5335e+00],
         [-4.1533e-03,  3.2406e+00,  1.4831e-01,  ..., -1.8215e+00,
          -3.8244e-01, -2.0343e-01],
         [-7.7687e-02,  1.1339e+01, -2.9010e-01,  ..., -3.6043e+00,
           2.3987e-02, -5.7739e-02]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(14.2924, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(221.4363, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)
Images of the Gulf oil spill front of Mexico were altered .
iter = 1 reward = 3
final_logits =  tensor([[[-5.0549e-02,  4.8131e+00, -1.1234e-01,  ..., -5.5659e+00,
          -2.1581e-01,  2.6698e+00],
         [-5.8078e-02,  1.2812e+00, -4.2974e-01,  ..., -2.9561e+00,
          -4.9443e-01, -1.2299e+00],
         [-1.0181e-01,  1.3472e+00, -3.9607e-01,  ..., -5.9949e+00,
          -2.2818e-01,  1.1813e+00],
         ...,
         [-1.2423e-01,  2.3951e+00, -7.8103e-01,  ..., -3.6377e+00,
          -6.5897e-01, -2.2510e+00],
         [ 5.8882e-03,  3.5740e+00,  4.9386e-01,  ..., -2.5792e+00,
           1.2055e-01, -2.1618e-01],
         [-1.0191e-01,  1.1000e+01, -4.6837e-01,  ..., -3.5264e+00,
           2.7305e-01, -5.3816e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(14.2283, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(178.2442, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)
Images of the Gulf spill front of Mexico were altered .
iter = 2 reward = 2
final_logits =  tensor([[[-7.7188e-02,  4.8692e+00, -1.5232e-01,  ..., -5.2680e+00,
           9.7792e-02,  1.8892e+00],
         [-7.9207e-02,  1.9389e+00, -4.1165e-01,  ..., -2.7571e+00,
          -2.9980e-01, -1.8021e+00],
         [-1.3566e-01,  1.6309e+00, -5.1051e-01,  ..., -5.9192e+00,
          -2.7851e-01,  4.5757e-01],
         ...,
         [-1.3362e-01,  2.6631e+00, -8.1459e-01,  ..., -3.1235e+00,
          -9.1412e-01, -1.9079e+00],
         [-7.3114e-03,  3.9826e+00,  4.5881e-01,  ..., -1.6293e+00,
           4.7007e-01,  3.4351e-01],
         [-1.1164e-01,  1.1361e+01, -3.1910e-01,  ..., -3.0736e+00,
           3.4385e-01, -7.9326e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(6.5297, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(81.3039, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)
Images of the Gulf spill front of Mexico were altered .
iter = 3 reward = 2
final_logits =  tensor([[[-0.0968,  4.8059, -0.1555,  ..., -5.1529, -0.1961,  1.8123],
         [-0.0963,  1.9888, -0.4308,  ..., -2.8025, -0.4237, -1.8599],
         [-0.1576,  1.6968, -0.5409,  ..., -6.0528, -0.5915,  0.2453],
         ...,
         [-0.1402,  2.8392, -0.8116,  ..., -2.8936, -1.1961, -1.7107],
         [-0.0199,  4.1937,  0.4894,  ..., -1.5092,  0.5554,  0.5297],
         [-0.1214, 11.5491, -0.2808,  ..., -2.8793,  0.3149, -0.8086]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(3.0356, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(33.0597, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)

Everytime after generating next logits 41.3% (4556 out of 11019)
Images of the Gulf spill front of Mexico were altered .
iter = 4 reward = 2
final_logits =  tensor([[[-0.1160,  4.7841, -0.1562,  ..., -5.0521, -0.5247,  1.7344],
         [-0.1114,  2.0083, -0.4554,  ..., -2.7600, -0.6086, -1.9425],
         [-0.1758,  1.9313, -0.5609,  ..., -6.1987, -0.9043,  0.0868],
         ...,
         [-0.1481,  2.9399, -0.8034,  ..., -2.9201, -1.3204, -1.6218],
         [-0.0307,  4.3732,  0.5187,  ..., -1.4955,  0.6154,  0.4881],
         [-0.1332, 11.6688, -0.2683,  ..., -2.8424,  0.2627, -0.7811]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.0656, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(9.3077, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  2
2
{'input_ids': tensor([[[599, 960, 110,  ..., 109, 804,   1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 41.4% (4560 out of 11019)

Everytime after generating next logits 41.4% (4560 out of 11019)

Everytime after generating next logits 41.4% (4560 out of 11019)

Everytime after generating next logits 41.4% (4560 out of 11019)

Everytime after generating next logits 41.4% (4560 out of 11019)

Everytime after generating next logits 41.4% (4560 out of 11019)

Everytime after generating next logits 41.4% (4560 out of 11019)

Everytime after generating next logits 41.4% (4560 out of 11019)

Everytime after generating next logits 41.4% (4560 out of 11019)

Everytime after generating next logits 41.4% (4560 out of 11019)
BP hopes to well well continue .
iter = 0 reward = 1
final_logits =  tensor([[[-0.1524,  2.9953,  0.3216,  ..., -4.5253,  1.4311, -0.8498],
         [-0.2219,  2.4288, -0.8222,  ..., -3.7599, -1.3513, -2.0098],
         [-0.3048,  0.7725, -0.9940,  ..., -2.9293,  1.3714, -2.5439],
         ...,
         [-0.2987,  3.1061, -1.0163,  ..., -0.8385, -1.5785, -0.7516],
         [-0.0954,  6.1619,  0.0311,  ..., -2.1482, -1.2464,  0.3886],
         [-0.2457, 10.3153, -0.6885,  ..., -0.4697, -1.4902, -0.9935]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-7.9417, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(34.2333, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)

Everytime after generating next logits 41.2% (4544 out of 11019)
BP well stopped its well due to its integrity test .
iter = 1 reward = 3
final_logits =  tensor([[[-0.1594,  3.0061,  0.3379,  ..., -4.4647,  1.6332, -0.9039],
         [-0.2410,  2.7906, -0.7777,  ..., -3.5123, -1.6195, -1.4871],
         [-0.2400,  0.9854, -0.5291,  ..., -1.5556, -0.9379,  0.7909],
         ...,
         [-0.2330,  1.6990, -0.9090,  ..., -3.3415, -1.8756, -0.9268],
         [-0.0707,  6.0522, -0.2502,  ..., -1.5306, -0.8312, -0.9436],
         [-0.1474, 11.8127, -0.3482,  ..., -1.0706,  0.2504, -0.6749]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-4.5388, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(41.1929, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)

Everytime after generating next logits 41.3% (4554 out of 11019)
BP well well well is ready to stop well well after its oil spill . BP will stop well well stopping its well from leaking .
iter = 2 reward = 5
final_logits =  tensor([[[-1.6923e-01,  2.9400e+00,  3.4151e-01,  ..., -4.2973e+00,
           1.4461e+00, -7.6586e-01],
         [-2.4772e-01,  2.4588e+00, -7.7661e-01,  ..., -3.8752e+00,
          -1.5693e+00, -1.4981e+00],
         [-2.6118e-01,  6.1881e-01, -5.0177e-01,  ..., -1.9617e+00,
          -7.3332e-01,  4.5622e-01],
         ...,
         [-2.9961e-01,  2.5062e+00, -9.5338e-01,  ..., -2.0168e+00,
          -1.7102e-02, -1.9840e+00],
         [-4.3045e-02,  5.3699e+00, -1.0805e-02,  ..., -1.6488e+00,
          -2.8580e-01, -2.8178e-01],
         [-1.8939e-01,  1.1539e+01, -4.4854e-01,  ..., -4.9036e-01,
          -3.4571e-03, -4.5241e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-10.6170, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(55.5442, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4570 out of 11019)

Everytime after generating next logits 41.5% (4572 out of 11019)
BP hopes of well well well will be stopped by oil spill .
iter = 3 reward = 3
final_logits =  tensor([[[-0.1574,  2.6660,  0.4282,  ..., -4.2556,  1.6273, -0.9873],
         [-0.2341,  1.9967, -0.8561,  ..., -4.0984, -1.4884, -2.0471],
         [-0.2877,  0.4635, -0.9802,  ..., -3.1399,  1.4652, -3.3919],
         ...,
         [-0.2934,  0.5275, -0.7132,  ..., -2.4707, -1.0912, -2.2993],
         [-0.0624,  3.8669,  0.1527,  ..., -1.5390, -0.2269,  0.3999],
         [-0.1537, 11.5666, -0.1723,  ..., -0.9737,  0.9499, -1.5735]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-13.3221, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(68.6378, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)
Images of oil spill spill spill spill spill spill . BP's new plan to stop stopping stopping capping pipe and stopping pipe .
iter = 4 reward = 10
final_logits =  tensor([[[-0.1458,  2.5182,  0.4612,  ..., -4.2551,  1.9152, -1.0540],
         [-0.1331,  0.6940, -0.4398,  ..., -4.7693,  0.6820, -1.8346],
         [-0.2960, -0.2766, -0.1900,  ..., -5.6059,  0.2663,  0.9749],
         ...,
         [-0.2003,  2.0043, -0.5023,  ..., -0.8159, -1.9473, -1.7644],
         [-0.0321,  4.6082, -0.1029,  ...,  0.2958, -1.4276, -0.2022],
         [-0.1540, 11.2152, -0.3575,  ..., -0.5302,  0.0735, -0.9342]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-1.4202, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(13.3139, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  10
10
{'input_ids': tensor([[[6174,  692,  110,  ...,  526,  115,    1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 41.7% (4590 out of 11019)

Everytime after generating next logits 41.7% (4590 out of 11019)

Everytime after generating next logits 41.7% (4590 out of 11019)

Everytime after generating next logits 41.7% (4590 out of 11019)

Everytime after generating next logits 41.7% (4590 out of 11019)

Everytime after generating next logits 41.7% (4590 out of 11019)

Everytime after generating next logits 41.7% (4590 out of 11019)

Everytime after generating next logits 41.7% (4590 out of 11019)

Everytime after generating next logits 41.7% (4590 out of 11019)

Everytime after generating next logits 41.7% (4590 out of 11019)

Everytime after generating next logits 41.7% (4590 out of 11019)

Everytime after generating next logits 41.7% (4590 out of 11019)

Everytime after generating next logits 41.7% (4590 out of 11019)
Researchers say environmental damage is likely to be costly .
iter = 0 reward = 3
final_logits =  tensor([[[-0.0749,  4.8132,  0.4993,  ..., -5.6433, -0.0605, -0.4123],
         [-0.0254,  3.4165, -0.3333,  ..., -3.9013, -1.4180, -1.7934],
         [-0.1547,  2.7004, -0.4407,  ..., -5.7670, -0.5511, -0.4809],
         ...,
         [-0.1379,  3.4604, -0.4368,  ..., -2.7510,  1.0818, -1.6803],
         [ 0.0123,  5.1550,  0.3326,  ..., -3.9117,  0.6319,  0.6059],
         [-0.0444, 11.7606,  0.2304,  ..., -2.8161, -0.6425, -0.1813]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(5.0570, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(15.1345, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)

Everytime after generating next logits 41.4% (4564 out of 11019)
Researchers say environmental damage is likely to be costly .
iter = 1 reward = 3
final_logits =  tensor([[[-0.0670,  4.6702,  0.5238,  ..., -5.7464, -0.4239, -0.5209],
         [-0.0215,  3.3191, -0.3387,  ..., -3.8267, -1.3823, -1.7924],
         [-0.1492,  2.6729, -0.4089,  ..., -5.8180, -0.7433, -0.4608],
         ...,
         [-0.1350,  3.3027, -0.4327,  ..., -2.7588,  1.2890, -1.4786],
         [ 0.0229,  4.9438,  0.4020,  ..., -3.9745,  0.5736,  0.7002],
         [-0.0388, 11.7442,  0.2182,  ..., -2.7654, -0.7611, -0.1905]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(5.4212, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(21.5844, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4210 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.9% (4402 out of 11019)

Everytime after generating next logits 39.9% (4402 out of 11019)

Everytime after generating next logits 39.9% (4402 out of 11019)
enviromental scientists say environmental damage is likely to be warmer .
iter = 2 reward = 4
final_logits =  tensor([[[-6.3784e-02,  4.5836e+00,  5.2202e-01,  ..., -5.8623e+00,
          -6.5531e-01, -3.9324e-01],
         [-4.2959e-02,  4.1963e+00,  2.6120e-01,  ..., -2.2121e+00,
          -2.2367e+00,  1.9138e-01],
         [-4.9861e-02,  1.3323e+00, -1.2027e-02,  ..., -3.3645e+00,
          -2.7403e+00, -6.6433e-01],
         ...,
         [-1.3815e-01,  5.1124e+00, -6.0228e-01,  ..., -1.8198e+00,
           3.4322e-02, -6.3460e-01],
         [ 8.2258e-03,  5.5393e+00,  6.8948e-02,  ..., -2.7851e+00,
           3.6542e-02,  1.3647e-01],
         [-3.1866e-02,  1.2166e+01,  1.8074e-01,  ..., -2.5828e+00,
          -1.0968e+00, -2.1539e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(5.5427, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(23.1840, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4268 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)
Study: Environmental scientist says environmental damage is likely to be warm .
iter = 3 reward = 4
final_logits =  tensor([[[-6.4379e-02,  4.1547e+00,  4.9879e-01,  ..., -6.3598e+00,
          -6.8599e-01, -3.3359e-01],
         [-1.1080e-01,  2.1521e+00, -5.5257e-01,  ..., -3.3730e+00,
          -2.2279e+00, -1.6860e+00],
         [-5.5004e-02,  6.0868e+00,  3.2631e-01,  ..., -4.8437e+00,
          -8.7138e-01, -3.9039e-01],
         ...,
         [-1.9188e-01,  4.8714e+00, -5.3145e-01,  ..., -2.4013e+00,
          -2.2312e-01, -7.6244e-01],
         [-4.1440e-04,  4.9007e+00,  1.3179e-01,  ..., -2.7661e+00,
          -5.2660e-02,  7.0054e-01],
         [-5.1888e-02,  1.1845e+01,  7.4051e-02,  ..., -2.8202e+00,
          -1.0270e+00, -6.9173e-02]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(4.3536, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(18.1993, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4268 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)
Study: Environmental scientist says environmental damage is likely to be warm .
iter = 4 reward = 4
final_logits =  tensor([[[-7.4761e-02,  3.9046e+00,  4.5022e-01,  ..., -6.5172e+00,
          -6.6797e-01, -1.9860e-01],
         [-1.1082e-01,  2.1451e+00, -5.7716e-01,  ..., -3.5181e+00,
          -2.5518e+00, -1.7081e+00],
         [-5.8825e-02,  6.0364e+00,  3.0271e-01,  ..., -4.9625e+00,
          -9.2020e-01, -4.4259e-01],
         ...,
         [-1.9627e-01,  4.7192e+00, -5.5785e-01,  ..., -2.2943e+00,
          -2.2177e-01, -6.8905e-01],
         [-3.2528e-03,  4.7472e+00,  1.3082e-01,  ..., -2.7723e+00,
           6.2598e-03,  7.3420e-01],
         [-5.8144e-02,  1.1874e+01,  2.9344e-02,  ..., -2.8869e+00,
          -1.0206e+00, -5.8080e-02]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(2.5437, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(12.1967, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  4
4
{'input_ids': tensor([[[  139,  5543,  9254,   320,  3702,   142,   865,   728,   113,   109,
          16036,   762, 14567,   110,   107,  8916,   114,  1713,   113,  5012,
            122,   662,   503, 32908,   110,   108,   330,   278,  2668,   116,
            122,  4605, 35522,   110,   108,   109,   177,  3378,   113, 16036,
            111,  6061, 32886,   110,   108,   169, 15978,   110,   107,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1]]])}

Before Sampling 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)

Everytime after generating next logits 38.8% (4270 out of 11019)
Bob Dudley presents the new oil insider host of the oil industry . Bob Dudley presents the new oil world inside oil insiders . Bob Dudley presents Money Head of a world of oil insiders including a new host . Bob Dudley presents an interview with Money .
iter = 0 reward = 9
final_logits =  tensor([[[ 4.1654e-02,  3.5688e+00,  6.6616e-01,  ..., -2.4250e+00,
           3.2385e+00, -7.2230e-01],
         [ 1.6210e-02,  4.1078e+00,  1.0390e-01,  ..., -1.3670e+00,
          -7.8656e-01,  1.0489e+00],
         [-1.1903e-02,  4.5422e+00, -2.8160e-01,  ..., -2.1762e+00,
          -3.6122e+00, -1.9936e+00],
         ...,
         [-7.8110e-02,  5.4039e+00, -6.0404e-01,  ..., -3.8524e+00,
          -2.3539e+00, -3.0839e+00],
         [-8.1625e-03,  3.6833e+00, -2.3583e-02,  ..., -2.3762e+00,
          -9.9559e-01, -3.0164e-01],
         [-6.6288e-02,  1.1631e+01,  7.6755e-03,  ..., -3.0543e+00,
          -2.0216e+00, -7.6229e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.7630, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(15.5960, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.2% (4324 out of 11019)

Everytime after generating next logits 39.2% (4324 out of 11019)

Everytime after generating next logits 39.2% (4324 out of 11019)

Everytime after generating next logits 39.2% (4324 out of 11019)

Everytime after generating next logits 39.2% (4324 out of 11019)

Everytime after generating next logits 39.2% (4324 out of 11019)

Everytime after generating next logits 39.5% (4352 out of 11019)

Everytime after generating next logits 39.5% (4352 out of 11019)

Everytime after generating next logits 39.5% (4352 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.8% (4384 out of 11019)

Everytime after generating next logits 39.9% (4402 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.1% (4420 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)
Bob Dudley presents the Head of Money programme . Bob Dudley presents an oil world inside oil insiders . Bob Dudley presents an interview inside oil insiders including a new host . Bob Dudley presents an interview . Bob Dudley presents an interview .
iter = 1 reward = 6
final_logits =  tensor([[[ 3.9097e-02,  3.4121e+00,  6.8078e-01,  ..., -2.3083e+00,
           3.4612e+00, -6.4638e-01],
         [ 1.7376e-02,  4.0991e+00,  1.0368e-01,  ..., -1.3578e+00,
          -7.8541e-01,  1.0310e+00],
         [-1.6790e-02,  4.6123e+00, -2.6913e-01,  ..., -2.2541e+00,
          -3.6586e+00, -2.0459e+00],
         ...,
         [-1.5435e-01,  4.4749e+00, -6.7313e-01,  ..., -3.1996e+00,
          -2.5107e+00, -2.0222e+00],
         [ 2.3464e-03,  3.2638e+00, -3.1672e-02,  ..., -8.9966e-01,
          -7.0359e-01, -2.7036e-01],
         [-9.4110e-02,  1.2384e+01, -3.5642e-01,  ..., -2.3730e+00,
          -3.7822e-01, -2.3668e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-1.5312, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(13.6759, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)

Everytime after generating next logits 39.6% (4362 out of 11019)
Bob Dudley interviews oil insiders including Tony Hayward . Tony Hayward's new spill account features a exclusive account .
iter = 2 reward = 4
final_logits =  tensor([[[ 3.9733e-02,  3.2609e+00,  6.8517e-01,  ..., -2.1909e+00,
           3.3508e+00, -6.8816e-01],
         [ 1.8596e-02,  4.0743e+00,  1.0199e-01,  ..., -1.3484e+00,
          -7.7563e-01,  9.8809e-01],
         [-2.0037e-02,  4.6988e+00, -2.5036e-01,  ..., -2.1884e+00,
          -3.6458e+00, -2.0161e+00],
         ...,
         [-1.6911e-01,  3.3955e+00, -8.1681e-01,  ..., -4.0520e+00,
          -2.9684e-01, -1.9257e+00],
         [-4.1757e-04,  3.3752e+00, -1.0844e-01,  ..., -1.1170e+00,
          -3.0304e-01, -7.4298e-01],
         [-5.3150e-02,  1.0942e+01, -3.2238e-01,  ..., -1.7770e+00,
           7.3152e-01, -2.9085e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-2.3002, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(16.3473, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.0% (4298 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.1% (4312 out of 11019)

Everytime after generating next logits 39.3% (4328 out of 11019)

Everytime after generating next logits 39.3% (4328 out of 11019)

Everytime after generating next logits 39.3% (4328 out of 11019)

Everytime after generating next logits 39.3% (4328 out of 11019)

Everytime after generating next logits 39.3% (4328 out of 11019)

Everytime after generating next logits 39.4% (4346 out of 11019)

Everytime after generating next logits 39.4% (4346 out of 11019)

Everytime after generating next logits 39.4% (4346 out of 11019)

Everytime after generating next logits 39.4% (4346 out of 11019)
Bob Dudley interviews oil insiders including Tony Hayward . The . . . . . . is BP's new spill team and the the the the the the the the the the the industry spill team .
iter = 3 reward = 5
final_logits =  tensor([[[ 0.0397,  3.2057,  0.6756,  ..., -2.3626,  3.2067, -0.6771],
         [ 0.0203,  4.0755,  0.0988,  ..., -1.3620, -0.8260,  0.9674],
         [-0.0169,  4.7726, -0.2394,  ..., -2.1671, -3.6470, -1.9437],
         ...,
         [-0.1391,  3.9163, -0.9300,  ..., -2.6727, -1.0854, -2.4116],
         [-0.0133,  3.8614, -0.2782,  ..., -0.7945, -0.9730, -0.5046],
         [-0.0482, 11.4957, -0.4802,  ..., -1.3221,  0.1166, -2.1390]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-2.6483, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(18.9676, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)

Everytime after generating next logits 39.4% (4338 out of 11019)
Bob Dudley interviews oil insiders including BP's Tony Hayward .
iter = 4 reward = 2
final_logits =  tensor([[[ 4.0200e-02,  3.1886e+00,  6.7943e-01,  ..., -2.3955e+00,
           3.1825e+00, -6.5945e-01],
         [ 2.1482e-02,  4.1015e+00,  1.0201e-01,  ..., -1.3015e+00,
          -8.4369e-01,  9.5864e-01],
         [-1.2823e-02,  4.7199e+00, -2.2691e-01,  ..., -2.0828e+00,
          -3.6322e+00, -1.8349e+00],
         ...,
         [-1.2513e-01,  3.1255e+00, -8.7008e-01,  ..., -2.8440e+00,
          -9.8042e-01, -3.3103e+00],
         [ 5.0148e-03,  2.8723e+00, -1.1583e-01,  ..., -2.1453e+00,
          -1.1150e+00, -1.3154e+00],
         [-3.9985e-02,  1.0620e+01, -2.4675e-01,  ..., -2.9244e+00,
           4.3481e-01, -2.4774e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-3.9569, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(17.7946, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  2
2
{'input_ids': tensor([[[ 1084, 18756,   110,  ...,  5135,   378,     1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 38.2% (4206 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)

Everytime after generating next logits 40.1% (4418 out of 11019)
Estimated 5,000 barrels of oil could spill 72 miles off Mexico .
iter = 0 reward = 4
final_logits =  tensor([[[-1.0111e-01,  3.6418e+00,  2.7026e-01,  ..., -4.9930e+00,
           1.7462e+00,  9.9536e-02],
         [-1.4219e-01,  2.9972e+00, -1.0916e-01,  ..., -4.7864e+00,
           2.4859e+00, -2.9305e+00],
         [-1.4191e-01,  3.8326e+00,  1.4795e-01,  ..., -3.8314e+00,
          -1.1764e+00, -9.6117e-01],
         ...,
         [-2.0312e-01,  3.2768e+00, -5.2395e-01,  ..., -2.3088e+00,
          -2.6606e+00, -9.5376e-01],
         [-1.5893e-02,  3.9491e+00,  6.5608e-03,  ..., -1.2251e+00,
          -3.2763e-01,  8.0756e-02],
         [-9.7420e-02,  1.2237e+01, -2.7674e-01,  ..., -2.7592e+00,
           4.6758e-01, -1.4507e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.6930, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.0393, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.5% (4244 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)

Everytime after generating next logits 39.7% (4372 out of 11019)
Estimated Estimated 5,000 gallons of oil could spill 72 miles off the Gulf .
iter = 1 reward = 6
final_logits =  tensor([[[-0.0961,  3.6926,  0.2741,  ..., -4.9686,  1.8120,  0.1126],
         [-0.1463,  3.0407, -0.1092,  ..., -5.1451,  2.9164, -2.9778],
         [-0.1353,  2.9183, -0.0687,  ..., -4.7465,  2.7467, -2.9876],
         ...,
         [-0.2770,  3.9312, -0.5727,  ..., -2.9556, -2.7868, -1.2715],
         [-0.1365,  5.3796,  0.5123,  ..., -2.4830, -0.4461,  0.4429],
         [-0.1614, 11.6084, -0.3067,  ..., -2.4411,  0.1226, -1.3356]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(1.2350, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(15.2381, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.6% (4248 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 39.7% (4376 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.3% (4440 out of 11019)

Everytime after generating next logits 40.9% (4504 out of 11019)

Everytime after generating next logits 40.9% (4504 out of 11019)

Everytime after generating next logits 40.9% (4504 out of 11019)

Everytime after generating next logits 40.9% (4504 out of 11019)

Everytime after generating next logits 40.9% (4504 out of 11019)

Everytime after generating next logits 40.9% (4504 out of 11019)

Everytime after generating next logits 40.9% (4504 out of 11019)

Everytime after generating next logits 40.9% (4504 out of 11019)

Everytime after generating next logits 40.9% (4504 out of 11019)

Everytime after generating next logits 40.9% (4504 out of 11019)
Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated
iter = 2 reward = 66
final_logits =  tensor([[[-0.0893,  3.7601,  0.2969,  ..., -4.7376,  2.1503,  0.1995],
         [-0.1588,  3.1509, -0.0938,  ..., -5.6399,  3.8033, -2.8783],
         [-0.1493,  2.9722, -0.0546,  ..., -5.3108,  3.7373, -2.9336],
         ...,
         [-0.1186, 11.7101, -0.1580,  ..., -2.7458,  2.7740, -2.5215],
         [-0.1170, 11.8864, -0.1539,  ..., -2.7418,  2.7376, -2.5393],
         [-0.1132, 12.1147, -0.1228,  ..., -2.6898,  2.7871, -2.5882]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(24.9261, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(1392.8130, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 39.9% (4402 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)

Everytime after generating next logits 41.1% (4530 out of 11019)
Estimated 5,000 barrels of oil could spill 72 miles off Mexico .
iter = 3 reward = 4
final_logits =  tensor([[[-0.1182,  3.4833,  0.3198,  ..., -5.6466,  1.3069, -0.3689],
         [-0.1714,  2.9744, -0.1066,  ..., -4.4847,  1.6728, -2.9208],
         [-0.1575,  3.6798,  0.0962,  ..., -3.7410, -1.3703, -0.5029],
         ...,
         [-0.2067,  3.2767, -0.5227,  ..., -2.4473, -2.7673, -0.8323],
         [-0.0377,  4.4385,  0.0163,  ..., -1.3960, -0.0969,  0.0590],
         [-0.1044, 12.2106, -0.2248,  ..., -2.9722,  0.6300, -1.3611]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-0.8941, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(6.4153, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.2% (4208 out of 11019)

Everytime after generating next logits 39.4% (4336 out of 11019)

Everytime after generating next logits 39.4% (4336 out of 11019)

Everytime after generating next logits 39.4% (4336 out of 11019)

Everytime after generating next logits 39.4% (4336 out of 11019)

Everytime after generating next logits 39.4% (4336 out of 11019)

Everytime after generating next logits 39.4% (4336 out of 11019)

Everytime after generating next logits 39.4% (4336 out of 11019)

Everytime after generating next logits 39.9% (4400 out of 11019)

Everytime after generating next logits 39.9% (4400 out of 11019)

Everytime after generating next logits 39.9% (4400 out of 11019)

Everytime after generating next logits 39.9% (4400 out of 11019)

Everytime after generating next logits 39.9% (4400 out of 11019)

Everytime after generating next logits 39.9% (4400 out of 11019)

Everytime after generating next logits 39.9% (4400 out of 11019)
Estimated 5,000 barrels of oil could spill 72 miles off Mexico .
iter = 4 reward = 4
final_logits =  tensor([[[-1.3735e-01,  3.7936e+00,  2.7151e-01,  ..., -5.4704e+00,
           1.2731e+00, -3.5567e-01],
         [-2.0637e-01,  3.1497e+00, -1.4348e-01,  ..., -4.6061e+00,
           1.5962e+00, -2.7330e+00],
         [-1.6781e-01,  3.8495e+00,  6.3914e-02,  ..., -3.6958e+00,
          -1.7145e+00, -4.1761e-01],
         ...,
         [-2.1215e-01,  3.4962e+00, -5.1778e-01,  ..., -2.3083e+00,
          -2.7053e+00, -8.5457e-01],
         [-4.5730e-02,  4.6124e+00,  2.6150e-02,  ..., -1.4838e+00,
          -5.2138e-02,  9.0052e-03],
         [-1.1389e-01,  1.2340e+01, -2.2111e-01,  ..., -3.0118e+00,
           6.3733e-01, -1.2584e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-2.4931, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(14.2220, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  4
4
{'input_ids': tensor([[[16036,   243,   186,  ...,   111,  2684,     1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 38.4% (4226 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)
Gulf has average of shares of oil company he has .
iter = 0 reward = 3
final_logits =  tensor([[[-0.1801,  6.4372,  0.1210,  ..., -2.9205,  1.6976, -3.2316],
         [-0.2323,  2.8210, -0.4200,  ..., -3.2042, -1.2968, -1.3873],
         [-0.2152,  2.5098, -0.3602,  ..., -2.0234, -0.2532, -0.8308],
         ...,
         [-0.2172,  2.4551, -0.5289,  ..., -0.6643,  0.3248, -1.1011],
         [-0.0502,  5.9994,  0.5851,  ..., -1.3909, -0.4341,  0.4781],
         [-0.1442, 12.0970, -0.0826,  ..., -2.4618,  1.2714, -1.1320]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-10.2974, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(48.5834, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.4% (4226 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)

Everytime after generating next logits 39.5% (4354 out of 11019)
Average company dividend is 1.72 per cent .
iter = 1 reward = 2
final_logits =  tensor([[[-0.1538,  6.4134,  0.0768,  ..., -3.7193,  2.0529, -2.7414],
         [-0.2130,  4.9496, -0.2907,  ..., -3.6728,  0.4404, -2.5123],
         [-0.2046,  5.2327, -0.5676,  ..., -2.0355,  1.2373, -1.6673],
         ...,
         [-0.2391,  5.9539, -0.5031,  ..., -2.1917,  1.0261, -1.1799],
         [-0.1043,  7.9829,  0.2509,  ..., -5.3540,  1.4014,  0.1533],
         [-0.0625, 12.5340,  0.1514,  ..., -2.7490,  1.9259, -1.1862]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-11.6598, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(63.4977, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.3% (4220 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 39.5% (4348 out of 11019)

Everytime after generating next logits 40.0% (4412 out of 11019)

Everytime after generating next logits 40.0% (4412 out of 11019)

Everytime after generating next logits 40.0% (4412 out of 11019)

Everytime after generating next logits 40.0% (4412 out of 11019)

Everytime after generating next logits 40.0% (4412 out of 11019)

Everytime after generating next logits 40.0% (4412 out of 11019)

Everytime after generating next logits 40.0% (4412 out of 11019)
Analysts say company is too much damage to the spill spill is a 'loss of life'
iter = 2 reward = 5
final_logits =  tensor([[[-0.1519,  7.0660,  0.1144,  ..., -3.5609,  2.0034, -2.8108],
         [-0.1479,  3.1783, -0.2109,  ..., -4.2530,  0.6850, -1.0949],
         [-0.2161,  3.5088, -0.6097,  ..., -2.2852, -0.0938, -2.1905],
         ...,
         [-0.2011,  2.8471, -0.1706,  ..., -0.9528,  1.4087, -0.2010],
         [-0.2165,  5.0703, -0.7711,  ..., -4.1446,  0.4748, -1.5481],
         [-0.1910, 11.5766, -0.6347,  ..., -2.6471,  0.4168, -2.2155]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(-7.0782, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(115.8175, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4268 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)

Everytime after generating next logits 39.9% (4396 out of 11019)
Report: BP CEO has been accused of 'incompeted'
iter = 3 reward = 1
final_logits =  tensor([[[-0.1517,  5.5675, -0.0732,  ..., -2.9049,  1.6390, -3.2137],
         [-0.1551,  4.5960, -0.7491,  ..., -2.9983, -1.3120, -0.7198],
         [-0.1019,  7.2457, -0.1171,  ..., -2.5676,  1.5395, -1.9995],
         ...,
         [-0.0186,  2.0308,  0.2249,  ...,  0.5180, -1.4915, -1.1744],
         [-0.1244,  2.1885, -0.3750,  ...,  0.6829, -1.0072, -1.6287],
         [-0.2488, 10.9163, -0.9995,  ..., -2.1150, -0.0686, -2.7930]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(22.9297, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(968.1333, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 38.7% (4262 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 39.8% (4390 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)

Everytime after generating next logits 40.4% (4454 out of 11019)
Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report: Report
iter = 4 reward = 1
final_logits =  tensor([[[-0.1184,  5.7819,  0.0705,  ..., -3.5897,  2.0680, -2.3883],
         [-0.1590,  4.6706, -0.7787,  ..., -3.5687, -0.8825, -1.6371],
         [-0.0850,  7.2626, -0.0335,  ..., -3.1953,  1.7126, -1.6086],
         ...,
         [-0.0761,  9.6541, -1.0187,  ..., -1.6467, -1.6521, -1.7652],
         [-0.0224, 10.3211, -0.1105,  ..., -2.6307,  0.6935, -0.9808],
         [-0.0704, 10.1616, -1.0053,  ..., -1.5185, -1.7570, -1.7343]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(0.0989, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(18.0905, device='cuda:0', grad_fn=<MseLossBackward>)
final reward =  1
1
{'input_ids': tensor([[[16036, 35640,   116,  ...,  3598, 32220,     1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]]])}

Before Sampling 40.1% (4422 out of 11019)

Everytime after generating next logits 40.1% (4422 out of 11019)

Everytime after generating next logits 40.1% (4422 out of 11019)

Everytime after generating next logits 40.1% (4422 out of 11019)

Everytime after generating next logits 40.1% (4422 out of 11019)

Everytime after generating next logits 40.1% (4422 out of 11019)

Everytime after generating next logits 40.1% (4422 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 40.7% (4486 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)

Everytime after generating next logits 41.3% (4550 out of 11019)
Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated
iter = 0 reward = 116
final_logits =  tensor([[[-0.1862,  3.9317,  0.4747,  ..., -5.2312,  0.7962,  0.8642],
         [-0.3465,  2.9843, -0.3888,  ..., -5.2840,  1.5077, -2.1686],
         [-0.3431,  2.9019, -0.3913,  ..., -5.0586,  1.5774, -2.2826],
         ...,
         [-0.3210, 11.5908, -0.1821,  ..., -2.7049,  1.6949, -1.6440],
         [-0.3165, 11.5861, -0.1697,  ..., -2.7061,  1.7318, -1.5612],
         [-0.3140, 11.5886, -0.1628,  ..., -2.7181,  1.7440, -1.4994]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(16.2847, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(3849.9634, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 40.8% (4496 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.0% (4624 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.5% (4688 out of 11019)

Everytime after generating next logits 42.6% (4692 out of 11019)

Everytime after generating next logits 42.6% (4692 out of 11019)

Everytime after generating next logits 42.6% (4692 out of 11019)

Everytime after generating next logits 42.6% (4692 out of 11019)
Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated
iter = 1 reward = 139
final_logits =  tensor([[[-0.1939,  3.9098,  0.4659,  ..., -5.3259,  0.4561,  0.5793],
         [-0.3782,  2.8258, -0.4755,  ..., -5.2426,  1.0758, -2.3432],
         [-0.3803,  2.7948, -0.4788,  ..., -5.1113,  1.0225, -2.3792],
         ...,
         [-0.3362, 11.5308, -0.1004,  ..., -2.8140,  1.1913, -1.5050],
         [-0.3247, 11.4758, -0.0912,  ..., -2.6642,  1.1331, -1.3061],
         [-0.3132, 11.4422, -0.0811,  ..., -2.5316,  1.0965, -1.1253]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(8.4151, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(4187.8755, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 41.2% (4540 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.4% (4668 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 42.9% (4732 out of 11019)

Everytime after generating next logits 44.1% (4864 out of 11019)

Everytime after generating next logits 45.3% (4996 out of 11019)

Everytime after generating next logits 45.3% (4996 out of 11019)

Everytime after generating next logits 45.3% (4996 out of 11019)

Everytime after generating next logits 45.3% (4996 out of 11019)

Everytime after generating next logits 45.3% (4996 out of 11019)

Everytime after generating next logits 46.6% (5132 out of 11019)

Everytime after generating next logits 47.8% (5268 out of 11019)

Everytime after generating next logits 47.8% (5268 out of 11019)

Everytime after generating next logits 47.8% (5268 out of 11019)

Everytime after generating next logits 47.8% (5268 out of 11019)

Everytime after generating next logits 49.1% (5408 out of 11019)

Everytime after generating next logits 50.3% (5548 out of 11019)

Everytime after generating next logits 50.3% (5548 out of 11019)

Everytime after generating next logits 50.3% (5548 out of 11019)

Everytime after generating next logits 50.3% (5548 out of 11019)

Everytime after generating next logits 51.7% (5692 out of 11019)

Everytime after generating next logits 53.0% (5836 out of 11019)

Everytime after generating next logits 53.0% (5836 out of 11019)

Everytime after generating next logits 53.0% (5836 out of 11019)

Everytime after generating next logits 53.0% (5836 out of 11019)

Everytime after generating next logits 53.0% (5836 out of 11019)

Everytime after generating next logits 54.3% (5984 out of 11019)

Everytime after generating next logits 55.6% (6132 out of 11019)

Everytime after generating next logits 55.6% (6132 out of 11019)

Everytime after generating next logits 55.6% (6132 out of 11019)

Everytime after generating next logits 55.6% (6132 out of 11019)

Everytime after generating next logits 57.0% (6284 out of 11019)

Everytime after generating next logits 58.4% (6436 out of 11019)

Everytime after generating next logits 58.4% (6436 out of 11019)

Everytime after generating next logits 58.4% (6436 out of 11019)

Everytime after generating next logits 58.4% (6436 out of 11019)

Everytime after generating next logits 58.4% (6436 out of 11019)

Everytime after generating next logits 59.8% (6592 out of 11019)

Everytime after generating next logits 61.2% (6748 out of 11019)

Everytime after generating next logits 61.2% (6748 out of 11019)

Everytime after generating next logits 61.2% (6748 out of 11019)

Everytime after generating next logits 61.2% (6748 out of 11019)

Everytime after generating next logits 62.7% (6908 out of 11019)

Everytime after generating next logits 64.1% (7068 out of 11019)

Everytime after generating next logits 64.1% (7068 out of 11019)

Everytime after generating next logits 64.1% (7068 out of 11019)

Everytime after generating next logits 64.1% (7068 out of 11019)

Everytime after generating next logits 64.1% (7068 out of 11019)

Everytime after generating next logits 65.6% (7232 out of 11019)

Everytime after generating next logits 67.1% (7396 out of 11019)

Everytime after generating next logits 67.1% (7396 out of 11019)

Everytime after generating next logits 67.1% (7396 out of 11019)

Everytime after generating next logits 67.1% (7396 out of 11019)

Everytime after generating next logits 68.6% (7564 out of 11019)

Everytime after generating next logits 70.2% (7732 out of 11019)

Everytime after generating next logits 70.2% (7732 out of 11019)

Everytime after generating next logits 70.2% (7732 out of 11019)

Everytime after generating next logits 70.2% (7732 out of 11019)

Everytime after generating next logits 70.2% (7732 out of 11019)

Everytime after generating next logits 71.7% (7904 out of 11019)

Everytime after generating next logits 73.3% (8076 out of 11019)

Everytime after generating next logits 73.3% (8076 out of 11019)

Everytime after generating next logits 73.3% (8076 out of 11019)

Everytime after generating next logits 73.3% (8076 out of 11019)

Everytime after generating next logits 74.9% (8252 out of 11019)

Everytime after generating next logits 76.5% (8428 out of 11019)

Everytime after generating next logits 76.5% (8428 out of 11019)

Everytime after generating next logits 76.5% (8428 out of 11019)

Everytime after generating next logits 76.5% (8428 out of 11019)

Everytime after generating next logits 76.5% (8428 out of 11019)

Everytime after generating next logits 78.1% (8608 out of 11019)

Everytime after generating next logits 79.8% (8788 out of 11019)

Everytime after generating next logits 79.8% (8788 out of 11019)

Everytime after generating next logits 79.8% (8788 out of 11019)

Everytime after generating next logits 79.8% (8788 out of 11019)

Everytime after generating next logits 81.4% (8972 out of 11019)

Everytime after generating next logits 83.1% (9156 out of 11019)

Everytime after generating next logits 83.1% (9156 out of 11019)

Everytime after generating next logits 83.1% (9156 out of 11019)

Everytime after generating next logits 83.1% (9156 out of 11019)

Everytime after generating next logits 84.8% (9344 out of 11019)

Everytime after generating next logits 86.5% (9532 out of 11019)

Everytime after generating next logits 86.5% (9532 out of 11019)

Everytime after generating next logits 86.5% (9532 out of 11019)

Everytime after generating next logits 86.5% (9532 out of 11019)

Everytime after generating next logits 86.5% (9532 out of 11019)

Everytime after generating next logits 88.2% (9724 out of 11019)

Everytime after generating next logits 90.0% (9916 out of 11019)

Everytime after generating next logits 90.0% (9916 out of 11019)

Everytime after generating next logits 90.0% (9916 out of 11019)

Everytime after generating next logits 90.0% (9916 out of 11019)

Everytime after generating next logits 91.8% (10112 out of 11019)

Everytime after generating next logits 93.5% (10308 out of 11019)

Everytime after generating next logits 93.5% (10308 out of 11019)

Everytime after generating next logits 93.5% (10308 out of 11019)

Everytime after generating next logits 93.5% (10308 out of 11019)

Everytime after generating next logits 93.5% (10308 out of 11019)

Everytime after generating next logits 95.4% (10508 out of 11019)

Everytime after generating next logits 97.2% (10708 out of 11019)

Everytime after generating next logits 97.2% (10708 out of 11019)

Everytime after generating next logits 97.2% (10708 out of 11019)

Everytime after generating next logits 97.2% (10708 out of 11019)

Everytime after generating next logits 99.0% (10912 out of 11019)

Everytime after generating next logits 44.3% (4884 out of 11019)

Everytime after generating next logits 44.3% (4884 out of 11019)

Everytime after generating next logits 44.3% (4884 out of 11019)

Everytime after generating next logits 44.3% (4884 out of 11019)

Everytime after generating next logits 44.3% (4884 out of 11019)

Everytime after generating next logits 46.2% (5092 out of 11019)

Everytime after generating next logits 48.1% (5300 out of 11019)

Everytime after generating next logits 48.1% (5300 out of 11019)

Everytime after generating next logits 48.1% (5300 out of 11019)

Everytime after generating next logits 48.1% (5300 out of 11019)

Everytime after generating next logits 50.0% (5512 out of 11019)

Everytime after generating next logits 51.9% (5724 out of 11019)

Everytime after generating next logits 51.9% (5724 out of 11019)

Everytime after generating next logits 51.9% (5724 out of 11019)

Everytime after generating next logits 51.9% (5724 out of 11019)

Everytime after generating next logits 51.9% (5724 out of 11019)

Everytime after generating next logits 53.9% (5940 out of 11019)

Everytime after generating next logits 55.9% (6156 out of 11019)
Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated
iter = 2 reward = 290
final_logits =  tensor([[[-0.1880,  3.6267,  0.4010,  ..., -5.6708,  0.0174,  0.4822],
         [-0.4122,  2.5355, -0.4873,  ..., -5.0277,  0.6945, -2.2579],
         [-0.4176,  2.4778, -0.4820,  ..., -4.9540,  0.5722, -2.2782],
         ...,
         [-0.4303, 12.0109, -0.1570,  ..., -3.5706,  0.8808, -1.8733],
         [-0.4290, 12.0013, -0.1528,  ..., -3.5730,  0.8837, -1.8512],
         [-0.4172, 11.9574, -0.1667,  ..., -3.4980,  0.7825, -1.6598]]],
       device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(34.8303, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(19115.2383, device='cuda:0', grad_fn=<MseLossBackward>)

Before Sampling 44.7% (4920 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 45.8% (5048 out of 11019)

Everytime after generating next logits 46.4% (5114 out of 11019)

Everytime after generating next logits 47.0% (5180 out of 11019)

Everytime after generating next logits 47.0% (5180 out of 11019)

Everytime after generating next logits 47.0% (5180 out of 11019)

Everytime after generating next logits 47.0% (5180 out of 11019)

Everytime after generating next logits 47.0% (5180 out of 11019)

Everytime after generating next logits 47.6% (5248 out of 11019)

Everytime after generating next logits 48.2% (5316 out of 11019)

Everytime after generating next logits 48.2% (5316 out of 11019)

Everytime after generating next logits 48.2% (5316 out of 11019)

Everytime after generating next logits 48.2% (5316 out of 11019)

Everytime after generating next logits 48.9% (5386 out of 11019)

Everytime after generating next logits 49.5% (5456 out of 11019)

Everytime after generating next logits 49.5% (5456 out of 11019)

Everytime after generating next logits 49.5% (5456 out of 11019)

Everytime after generating next logits 49.5% (5456 out of 11019)

Everytime after generating next logits 50.2% (5528 out of 11019)

Everytime after generating next logits 50.8% (5600 out of 11019)

Everytime after generating next logits 50.8% (5600 out of 11019)

Everytime after generating next logits 50.8% (5600 out of 11019)

Everytime after generating next logits 50.8% (5600 out of 11019)

Everytime after generating next logits 50.8% (5600 out of 11019)

Everytime after generating next logits 51.5% (5674 out of 11019)

Everytime after generating next logits 52.2% (5748 out of 11019)

Everytime after generating next logits 52.2% (5748 out of 11019)

Everytime after generating next logits 52.2% (5748 out of 11019)

Everytime after generating next logits 52.2% (5748 out of 11019)

Everytime after generating next logits 52.9% (5824 out of 11019)

Everytime after generating next logits 53.5% (5900 out of 11019)

Everytime after generating next logits 53.5% (5900 out of 11019)

Everytime after generating next logits 53.5% (5900 out of 11019)

Everytime after generating next logits 53.5% (5900 out of 11019)

Everytime after generating next logits 53.5% (5900 out of 11019)

Everytime after generating next logits 54.3% (5978 out of 11019)

Everytime after generating next logits 55.0% (6056 out of 11019)

Everytime after generating next logits 55.0% (6056 out of 11019)

Everytime after generating next logits 55.0% (6056 out of 11019)

Everytime after generating next logits 55.0% (6056 out of 11019)

Everytime after generating next logits 55.7% (6136 out of 11019)

Everytime after generating next logits 56.4% (6216 out of 11019)

Everytime after generating next logits 56.4% (6216 out of 11019)

Everytime after generating next logits 56.4% (6216 out of 11019)

Everytime after generating next logits 56.4% (6216 out of 11019)

Everytime after generating next logits 56.4% (6216 out of 11019)

Everytime after generating next logits 57.2% (6298 out of 11019)

Everytime after generating next logits 57.9% (6380 out of 11019)

Everytime after generating next logits 57.9% (6380 out of 11019)

Everytime after generating next logits 57.9% (6380 out of 11019)

Everytime after generating next logits 57.9% (6380 out of 11019)

Everytime after generating next logits 58.7% (6464 out of 11019)

Everytime after generating next logits 59.4% (6548 out of 11019)

Everytime after generating next logits 59.4% (6548 out of 11019)

Everytime after generating next logits 59.4% (6548 out of 11019)

Everytime after generating next logits 59.4% (6548 out of 11019)

Everytime after generating next logits 59.4% (6548 out of 11019)

Everytime after generating next logits 60.2% (6634 out of 11019)

Everytime after generating next logits 61.1% (6732 out of 11019)

Everytime after generating next logits 61.1% (6732 out of 11019)

Everytime after generating next logits 61.1% (6732 out of 11019)

Everytime after generating next logits 61.1% (6732 out of 11019)

Everytime after generating next logits 61.9% (6820 out of 11019)

Everytime after generating next logits 62.7% (6908 out of 11019)

Everytime after generating next logits 62.7% (6908 out of 11019)

Everytime after generating next logits 62.7% (6908 out of 11019)

Everytime after generating next logits 62.7% (6908 out of 11019)

Everytime after generating next logits 62.7% (6908 out of 11019)

Everytime after generating next logits 63.5% (6998 out of 11019)

Everytime after generating next logits 64.3% (7088 out of 11019)

Everytime after generating next logits 64.3% (7088 out of 11019)

Everytime after generating next logits 64.3% (7088 out of 11019)

Everytime after generating next logits 64.3% (7088 out of 11019)

Everytime after generating next logits 65.2% (7180 out of 11019)

Everytime after generating next logits 66.0% (7272 out of 11019)

Everytime after generating next logits 66.0% (7272 out of 11019)

Everytime after generating next logits 66.0% (7272 out of 11019)

Everytime after generating next logits 66.0% (7272 out of 11019)

Everytime after generating next logits 66.8% (7366 out of 11019)

Everytime after generating next logits 67.7% (7460 out of 11019)

Everytime after generating next logits 67.7% (7460 out of 11019)

Everytime after generating next logits 67.7% (7460 out of 11019)

Everytime after generating next logits 67.7% (7460 out of 11019)

Everytime after generating next logits 67.7% (7460 out of 11019)

Everytime after generating next logits 68.6% (7556 out of 11019)

Everytime after generating next logits 69.4% (7652 out of 11019)

Everytime after generating next logits 69.4% (7652 out of 11019)

Everytime after generating next logits 69.4% (7652 out of 11019)

Everytime after generating next logits 69.4% (7652 out of 11019)

Everytime after generating next logits 70.3% (7750 out of 11019)

Everytime after generating next logits 71.2% (7848 out of 11019)

Everytime after generating next logits 71.2% (7848 out of 11019)

Everytime after generating next logits 71.2% (7848 out of 11019)

Everytime after generating next logits 71.2% (7848 out of 11019)

Everytime after generating next logits 71.2% (7848 out of 11019)

Everytime after generating next logits 72.1% (7948 out of 11019)

Everytime after generating next logits 73.0% (8048 out of 11019)

Everytime after generating next logits 73.0% (8048 out of 11019)

Everytime after generating next logits 73.0% (8048 out of 11019)

Everytime after generating next logits 73.0% (8048 out of 11019)

Everytime after generating next logits 74.0% (8150 out of 11019)

Everytime after generating next logits 74.9% (8252 out of 11019)

Everytime after generating next logits 74.9% (8252 out of 11019)

Everytime after generating next logits 74.9% (8252 out of 11019)

Everytime after generating next logits 74.9% (8252 out of 11019)

Everytime after generating next logits 74.9% (8252 out of 11019)

Everytime after generating next logits 75.8% (8356 out of 11019)

Everytime after generating next logits 76.8% (8460 out of 11019)

Everytime after generating next logits 76.8% (8460 out of 11019)

Everytime after generating next logits 76.8% (8460 out of 11019)

Everytime after generating next logits 76.8% (8460 out of 11019)

Everytime after generating next logits 77.7% (8566 out of 11019)

Everytime after generating next logits 78.7% (8672 out of 11019)

Everytime after generating next logits 78.7% (8672 out of 11019)

Everytime after generating next logits 78.7% (8672 out of 11019)

Everytime after generating next logits 78.7% (8672 out of 11019)

Everytime after generating next logits 78.7% (8672 out of 11019)

Everytime after generating next logits 79.7% (8780 out of 11019)

Everytime after generating next logits 80.7% (8888 out of 11019)

Everytime after generating next logits 80.7% (8888 out of 11019)

Everytime after generating next logits 80.7% (8888 out of 11019)

Everytime after generating next logits 80.7% (8888 out of 11019)

Everytime after generating next logits 82.7% (9108 out of 11019)

Everytime after generating next logits 84.7% (9328 out of 11019)

Everytime after generating next logits 84.7% (9328 out of 11019)

Everytime after generating next logits 84.7% (9328 out of 11019)

Everytime after generating next logits 84.7% (9328 out of 11019)

Everytime after generating next logits 84.7% (9328 out of 11019)

Everytime after generating next logits 86.7% (9552 out of 11019)

Everytime after generating next logits 88.7% (9776 out of 11019)

Everytime after generating next logits 88.7% (9776 out of 11019)

Everytime after generating next logits 88.7% (9776 out of 11019)

Everytime after generating next logits 88.7% (9776 out of 11019)

Everytime after generating next logits 90.8% (10004 out of 11019)

Everytime after generating next logits 92.9% (10232 out of 11019)

Everytime after generating next logits 92.9% (10232 out of 11019)

Everytime after generating next logits 92.9% (10232 out of 11019)

Everytime after generating next logits 92.9% (10232 out of 11019)

Everytime after generating next logits 95.0% (10464 out of 11019)

Everytime after generating next logits 97.1% (10696 out of 11019)

Everytime after generating next logits 97.1% (10696 out of 11019)

Everytime after generating next logits 97.1% (10696 out of 11019)

Everytime after generating next logits 97.1% (10696 out of 11019)

Everytime after generating next logits 97.1% (10696 out of 11019)

Everytime after generating next logits 99.2% (10932 out of 11019)

Everytime after generating next logits 47.6% (5240 out of 11019)

Everytime after generating next logits 47.6% (5240 out of 11019)

Everytime after generating next logits 47.6% (5240 out of 11019)

Everytime after generating next logits 47.6% (5240 out of 11019)

Everytime after generating next logits 49.7% (5480 out of 11019)

Everytime after generating next logits 51.9% (5720 out of 11019)

Everytime after generating next logits 51.9% (5720 out of 11019)

Everytime after generating next logits 51.9% (5720 out of 11019)

Everytime after generating next logits 51.9% (5720 out of 11019)

Everytime after generating next logits 51.9% (5720 out of 11019)

Everytime after generating next logits 54.1% (5964 out of 11019)

Everytime after generating next logits 56.3% (6208 out of 11019)

Everytime after generating next logits 56.3% (6208 out of 11019)

Everytime after generating next logits 56.3% (6208 out of 11019)

Everytime after generating next logits 56.3% (6208 out of 11019)

Everytime after generating next logits 58.6% (6456 out of 11019)

Everytime after generating next logits 60.8% (6704 out of 11019)

Everytime after generating next logits 60.8% (6704 out of 11019)

Everytime after generating next logits 60.8% (6704 out of 11019)

Everytime after generating next logits 60.8% (6704 out of 11019)

Everytime after generating next logits 60.8% (6704 out of 11019)

Everytime after generating next logits 63.1% (6956 out of 11019)

Everytime after generating next logits 65.4% (7208 out of 11019)

Everytime after generating next logits 65.4% (7208 out of 11019)

Everytime after generating next logits 65.4% (7208 out of 11019)

Everytime after generating next logits 65.4% (7208 out of 11019)

Everytime after generating next logits 67.7% (7464 out of 11019)

Everytime after generating next logits 70.1% (7720 out of 11019)

Everytime after generating next logits 70.1% (7720 out of 11019)

Everytime after generating next logits 70.1% (7720 out of 11019)

Everytime after generating next logits 70.1% (7720 out of 11019)

Everytime after generating next logits 70.1% (7720 out of 11019)

Everytime after generating next logits 72.4% (7980 out of 11019)

Everytime after generating next logits 74.8% (8240 out of 11019)

Everytime after generating next logits 74.8% (8240 out of 11019)

Everytime after generating next logits 74.8% (8240 out of 11019)

Everytime after generating next logits 74.8% (8240 out of 11019)

Everytime after generating next logits 77.2% (8504 out of 11019)

Everytime after generating next logits 79.6% (8768 out of 11019)

Everytime after generating next logits 79.6% (8768 out of 11019)

Everytime after generating next logits 79.6% (8768 out of 11019)

Everytime after generating next logits 79.6% (8768 out of 11019)

Everytime after generating next logits 79.6% (8768 out of 11019)

Everytime after generating next logits 82.0% (9036 out of 11019)

Everytime after generating next logits 84.4% (9304 out of 11019)

Everytime after generating next logits 84.4% (9304 out of 11019)

Everytime after generating next logits 84.4% (9304 out of 11019)

Everytime after generating next logits 84.4% (9304 out of 11019)

Everytime after generating next logits 86.9% (9576 out of 11019)

Everytime after generating next logits 89.4% (9848 out of 11019)

Everytime after generating next logits 89.4% (9848 out of 11019)

Everytime after generating next logits 89.4% (9848 out of 11019)

Everytime after generating next logits 89.4% (9848 out of 11019)

Everytime after generating next logits 91.9% (10124 out of 11019)

Everytime after generating next logits 94.4% (10400 out of 11019)

Everytime after generating next logits 94.4% (10400 out of 11019)

Everytime after generating next logits 94.4% (10400 out of 11019)

Everytime after generating next logits 94.4% (10400 out of 11019)

Everytime after generating next logits 94.4% (10400 out of 11019)

Everytime after generating next logits 96.9% (10680 out of 11019)

Everytime after generating next logits 99.5% (10960 out of 11019)

Everytime after generating next logits 99.5% (10960 out of 11019)

Everytime after generating next logits 99.5% (10960 out of 11019)

Everytime after generating next logits 99.5% (10960 out of 11019)

Everytime after generating next logits 48.4% (5332 out of 11019)

Everytime after generating next logits 51.0% (5616 out of 11019)

Everytime after generating next logits 51.0% (5616 out of 11019)

Everytime after generating next logits 51.0% (5616 out of 11019)

Everytime after generating next logits 51.0% (5616 out of 11019)

Everytime after generating next logits 51.0% (5616 out of 11019)

Everytime after generating next logits 53.6% (5904 out of 11019)

Everytime after generating next logits 56.2% (6192 out of 11019)

Everytime after generating next logits 56.2% (6192 out of 11019)

Everytime after generating next logits 56.2% (6192 out of 11019)

Everytime after generating next logits 56.2% (6192 out of 11019)

Everytime after generating next logits 58.8% (6484 out of 11019)

Everytime after generating next logits 61.5% (6776 out of 11019)

Everytime after generating next logits 61.5% (6776 out of 11019)

Everytime after generating next logits 61.5% (6776 out of 11019)

Everytime after generating next logits 61.5% (6776 out of 11019)

Everytime after generating next logits 61.5% (6776 out of 11019)

Everytime after generating next logits 64.2% (7072 out of 11019)

Everytime after generating next logits 66.9% (7368 out of 11019)

Everytime after generating next logits 66.9% (7368 out of 11019)

Everytime after generating next logits 66.9% (7368 out of 11019)

Everytime after generating next logits 66.9% (7368 out of 11019)

Everytime after generating next logits 69.6% (7668 out of 11019)

Everytime after generating next logits 72.3% (7968 out of 11019)

Everytime after generating next logits 72.3% (7968 out of 11019)

Everytime after generating next logits 72.3% (7968 out of 11019)

Everytime after generating next logits 72.3% (7968 out of 11019)

Everytime after generating next logits 72.3% (7968 out of 11019)

Everytime after generating next logits 75.1% (8272 out of 11019)

Everytime after generating next logits 77.8% (8576 out of 11019)

Everytime after generating next logits 77.8% (8576 out of 11019)

Everytime after generating next logits 77.8% (8576 out of 11019)

Everytime after generating next logits 77.8% (8576 out of 11019)

Everytime after generating next logits 80.6% (8884 out of 11019)

Everytime after generating next logits 83.4% (9192 out of 11019)

Everytime after generating next logits 83.4% (9192 out of 11019)

Everytime after generating next logits 83.4% (9192 out of 11019)

Everytime after generating next logits 83.4% (9192 out of 11019)

Everytime after generating next logits 83.4% (9192 out of 11019)

Everytime after generating next logits 86.3% (9504 out of 11019)

Everytime after generating next logits 89.1% (9816 out of 11019)

Everytime after generating next logits 89.1% (9816 out of 11019)

Everytime after generating next logits 89.1% (9816 out of 11019)

Everytime after generating next logits 89.1% (9816 out of 11019)

Everytime after generating next logits 92.0% (10132 out of 11019)

Everytime after generating next logits 94.8% (10448 out of 11019)

Everytime after generating next logits 94.8% (10448 out of 11019)

Everytime after generating next logits 94.8% (10448 out of 11019)

Everytime after generating next logits 94.8% (10448 out of 11019)

Everytime after generating next logits 97.7% (10768 out of 11019)

Everytime after generating next logits 49.1% (5408 out of 11019)

Everytime after generating next logits 49.1% (5408 out of 11019)

Everytime after generating next logits 49.1% (5408 out of 11019)

Everytime after generating next logits 49.1% (5408 out of 11019)

Everytime after generating next logits 49.1% (5408 out of 11019)

Everytime after generating next logits 52.0% (5732 out of 11019)

Everytime after generating next logits 55.0% (6056 out of 11019)

Everytime after generating next logits 55.0% (6056 out of 11019)

Everytime after generating next logits 55.0% (6056 out of 11019)

Everytime after generating next logits 55.0% (6056 out of 11019)

Everytime after generating next logits 57.9% (6384 out of 11019)

Everytime after generating next logits 60.9% (6712 out of 11019)

Everytime after generating next logits 60.9% (6712 out of 11019)

Everytime after generating next logits 60.9% (6712 out of 11019)

Everytime after generating next logits 60.9% (6712 out of 11019)

Everytime after generating next logits 60.9% (6712 out of 11019)

Everytime after generating next logits 63.9% (7044 out of 11019)

Everytime after generating next logits 66.9% (7376 out of 11019)

Everytime after generating next logits 66.9% (7376 out of 11019)

Everytime after generating next logits 66.9% (7376 out of 11019)

Everytime after generating next logits 66.9% (7376 out of 11019)

Everytime after generating next logits 70.0% (7712 out of 11019)

Everytime after generating next logits 73.0% (8048 out of 11019)

Everytime after generating next logits 73.0% (8048 out of 11019)

Everytime after generating next logits 73.0% (8048 out of 11019)

Everytime after generating next logits 73.0% (8048 out of 11019)

Everytime after generating next logits 73.0% (8048 out of 11019)

Everytime after generating next logits 76.1% (8388 out of 11019)

Everytime after generating next logits 79.2% (8728 out of 11019)

Everytime after generating next logits 79.2% (8728 out of 11019)

Everytime after generating next logits 79.2% (8728 out of 11019)

Everytime after generating next logits 79.2% (8728 out of 11019)

Everytime after generating next logits 82.3% (9072 out of 11019)

Everytime after generating next logits 85.5% (9416 out of 11019)

Everytime after generating next logits 85.5% (9416 out of 11019)

Everytime after generating next logits 85.5% (9416 out of 11019)

Everytime after generating next logits 85.5% (9416 out of 11019)

Everytime after generating next logits 85.5% (9416 out of 11019)

Everytime after generating next logits 88.6% (9764 out of 11019)

Everytime after generating next logits 91.8% (10112 out of 11019)

Everytime after generating next logits 91.8% (10112 out of 11019)

Everytime after generating next logits 91.8% (10112 out of 11019)

Everytime after generating next logits 91.8% (10112 out of 11019)

Everytime after generating next logits 95.0% (10464 out of 11019)

Everytime after generating next logits 98.2% (10816 out of 11019)

Everytime after generating next logits 98.2% (10816 out of 11019)

Everytime after generating next logits 98.2% (10816 out of 11019)

Everytime after generating next logits 98.2% (10816 out of 11019)

Everytime after generating next logits 98.2% (10816 out of 11019)

Everytime after generating next logits 49.7% (5476 out of 11019)

Everytime after generating next logits 52.9% (5832 out of 11019)

Everytime after generating next logits 52.9% (5832 out of 11019)

Everytime after generating next logits 52.9% (5832 out of 11019)

Everytime after generating next logits 52.9% (5832 out of 11019)

Everytime after generating next logits 56.2% (6192 out of 11019)

Everytime after generating next logits 59.5% (6552 out of 11019)

Everytime after generating next logits 59.5% (6552 out of 11019)

Everytime after generating next logits 59.5% (6552 out of 11019)

Everytime after generating next logits 59.5% (6552 out of 11019)

Everytime after generating next logits 62.8% (6916 out of 11019)

Everytime after generating next logits 66.1% (7280 out of 11019)

Everytime after generating next logits 66.1% (7280 out of 11019)

Everytime after generating next logits 66.1% (7280 out of 11019)

Everytime after generating next logits 66.1% (7280 out of 11019)

Everytime after generating next logits 66.1% (7280 out of 11019)

Everytime after generating next logits 69.4% (7648 out of 11019)

Everytime after generating next logits 72.7% (8016 out of 11019)

Everytime after generating next logits 72.7% (8016 out of 11019)

Everytime after generating next logits 72.7% (8016 out of 11019)

Everytime after generating next logits 72.7% (8016 out of 11019)

Everytime after generating next logits 76.1% (8388 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 82.9% (9136 out of 11019)

Everytime after generating next logits 86.3% (9512 out of 11019)

Everytime after generating next logits 86.3% (9512 out of 11019)

Everytime after generating next logits 86.3% (9512 out of 11019)

Everytime after generating next logits 86.3% (9512 out of 11019)

Everytime after generating next logits 89.8% (9892 out of 11019)

Everytime after generating next logits 93.2% (10272 out of 11019)

Everytime after generating next logits 93.2% (10272 out of 11019)

Everytime after generating next logits 93.2% (10272 out of 11019)

Everytime after generating next logits 93.2% (10272 out of 11019)

Everytime after generating next logits 93.2% (10272 out of 11019)

Everytime after generating next logits 96.7% (10656 out of 11019)

Everytime after generating next logits 51.8% (5712 out of 11019)

Everytime after generating next logits 51.8% (5712 out of 11019)

Everytime after generating next logits 51.8% (5712 out of 11019)

Everytime after generating next logits 51.8% (5712 out of 11019)

Everytime after generating next logits 55.4% (6100 out of 11019)

Everytime after generating next logits 58.9% (6488 out of 11019)

Everytime after generating next logits 58.9% (6488 out of 11019)

Everytime after generating next logits 58.9% (6488 out of 11019)

Everytime after generating next logits 58.9% (6488 out of 11019)

Everytime after generating next logits 58.9% (6488 out of 11019)

Everytime after generating next logits 62.4% (6880 out of 11019)

Everytime after generating next logits 66.0% (7272 out of 11019)

Everytime after generating next logits 66.0% (7272 out of 11019)

Everytime after generating next logits 66.0% (7272 out of 11019)

Everytime after generating next logits 66.0% (7272 out of 11019)

Everytime after generating next logits 69.6% (7668 out of 11019)

Everytime after generating next logits 73.2% (8064 out of 11019)

Everytime after generating next logits 73.2% (8064 out of 11019)

Everytime after generating next logits 73.2% (8064 out of 11019)

Everytime after generating next logits 73.2% (8064 out of 11019)

Everytime after generating next logits 73.2% (8064 out of 11019)

Everytime after generating next logits 76.8% (8464 out of 11019)

Everytime after generating next logits 80.4% (8864 out of 11019)

Everytime after generating next logits 80.4% (8864 out of 11019)

Everytime after generating next logits 80.4% (8864 out of 11019)

Everytime after generating next logits 80.4% (8864 out of 11019)

Everytime after generating next logits 84.1% (9268 out of 11019)

Everytime after generating next logits 87.8% (9672 out of 11019)

Everytime after generating next logits 87.8% (9672 out of 11019)

Everytime after generating next logits 87.8% (9672 out of 11019)

Everytime after generating next logits 87.8% (9672 out of 11019)

Everytime after generating next logits 87.8% (9672 out of 11019)

Everytime after generating next logits 91.5% (10080 out of 11019)

Everytime after generating next logits 95.2% (10488 out of 11019)

Everytime after generating next logits 95.2% (10488 out of 11019)

Everytime after generating next logits 95.2% (10488 out of 11019)

Everytime after generating next logits 95.2% (10488 out of 11019)

Everytime after generating next logits 98.9% (10900 out of 11019)

Everytime after generating next logits 52.3% (5768 out of 11019)

Everytime after generating next logits 52.3% (5768 out of 11019)

Everytime after generating next logits 52.3% (5768 out of 11019)

Everytime after generating next logits 52.3% (5768 out of 11019)

Everytime after generating next logits 56.1% (6184 out of 11019)

Everytime after generating next logits 59.9% (6600 out of 11019)

Everytime after generating next logits 59.9% (6600 out of 11019)

Everytime after generating next logits 59.9% (6600 out of 11019)

Everytime after generating next logits 59.9% (6600 out of 11019)

Everytime after generating next logits 59.9% (6600 out of 11019)

Everytime after generating next logits 63.7% (7020 out of 11019)

Everytime after generating next logits 67.5% (7440 out of 11019)

Everytime after generating next logits 67.5% (7440 out of 11019)

Everytime after generating next logits 67.5% (7440 out of 11019)

Everytime after generating next logits 67.5% (7440 out of 11019)

Everytime after generating next logits 71.4% (7864 out of 11019)

Everytime after generating next logits 75.2% (8288 out of 11019)

Everytime after generating next logits 75.2% (8288 out of 11019)

Everytime after generating next logits 75.2% (8288 out of 11019)

Everytime after generating next logits 75.2% (8288 out of 11019)

Everytime after generating next logits 75.2% (8288 out of 11019)

Everytime after generating next logits 79.1% (8716 out of 11019)

Everytime after generating next logits 83.0% (9144 out of 11019)

Everytime after generating next logits 83.0% (9144 out of 11019)

Everytime after generating next logits 83.0% (9144 out of 11019)

Everytime after generating next logits 83.0% (9144 out of 11019)

Everytime after generating next logits 86.9% (9576 out of 11019)

Everytime after generating next logits 90.8% (10008 out of 11019)

Everytime after generating next logits 90.8% (10008 out of 11019)

Everytime after generating next logits 90.8% (10008 out of 11019)

Everytime after generating next logits 90.8% (10008 out of 11019)

Everytime after generating next logits 90.8% (10008 out of 11019)

Everytime after generating next logits 94.8% (10444 out of 11019)

Everytime after generating next logits 98.7% (10880 out of 11019)

Everytime after generating next logits 98.7% (10880 out of 11019)

Everytime after generating next logits 98.7% (10880 out of 11019)

Everytime after generating next logits 98.7% (10880 out of 11019)

Everytime after generating next logits 52.8% (5820 out of 11019)

Everytime after generating next logits 56.8% (6260 out of 11019)

Everytime after generating next logits 56.8% (6260 out of 11019)

Everytime after generating next logits 56.8% (6260 out of 11019)

Everytime after generating next logits 56.8% (6260 out of 11019)

Everytime after generating next logits 56.8% (6260 out of 11019)

Everytime after generating next logits 60.8% (6704 out of 11019)

Everytime after generating next logits 64.9% (7148 out of 11019)

Everytime after generating next logits 64.9% (7148 out of 11019)

Everytime after generating next logits 64.9% (7148 out of 11019)

Everytime after generating next logits 64.9% (7148 out of 11019)

Everytime after generating next logits 68.9% (7596 out of 11019)

Everytime after generating next logits 73.0% (8044 out of 11019)

Everytime after generating next logits 73.0% (8044 out of 11019)

Everytime after generating next logits 73.0% (8044 out of 11019)

Everytime after generating next logits 73.0% (8044 out of 11019)

Everytime after generating next logits 73.0% (8044 out of 11019)

Everytime after generating next logits 77.1% (8496 out of 11019)

Everytime after generating next logits 81.2% (8948 out of 11019)

Everytime after generating next logits 81.2% (8948 out of 11019)

Everytime after generating next logits 81.2% (8948 out of 11019)

Everytime after generating next logits 81.2% (8948 out of 11019)

Everytime after generating next logits 85.3% (9404 out of 11019)

Everytime after generating next logits 89.5% (9860 out of 11019)

Everytime after generating next logits 89.5% (9860 out of 11019)

Everytime after generating next logits 89.5% (9860 out of 11019)

Everytime after generating next logits 89.5% (9860 out of 11019)

Everytime after generating next logits 93.7% (10320 out of 11019)

Everytime after generating next logits 97.8% (10780 out of 11019)

Everytime after generating next logits 97.8% (10780 out of 11019)

Everytime after generating next logits 97.8% (10780 out of 11019)

Everytime after generating next logits 97.8% (10780 out of 11019)

Everytime after generating next logits 97.8% (10780 out of 11019)

Everytime after generating next logits 53.3% (5868 out of 11019)

Everytime after generating next logits 57.5% (6332 out of 11019)

Everytime after generating next logits 57.5% (6332 out of 11019)

Everytime after generating next logits 57.5% (6332 out of 11019)

Everytime after generating next logits 57.5% (6332 out of 11019)

Everytime after generating next logits 61.7% (6800 out of 11019)

Everytime after generating next logits 66.0% (7268 out of 11019)

Everytime after generating next logits 66.0% (7268 out of 11019)

Everytime after generating next logits 66.0% (7268 out of 11019)

Everytime after generating next logits 66.0% (7268 out of 11019)

Everytime after generating next logits 66.0% (7268 out of 11019)

Everytime after generating next logits 70.2% (7740 out of 11019)

Everytime after generating next logits 74.5% (8212 out of 11019)

Everytime after generating next logits 74.5% (8212 out of 11019)

Everytime after generating next logits 74.5% (8212 out of 11019)

Everytime after generating next logits 74.5% (8212 out of 11019)

Everytime after generating next logits 78.8% (8688 out of 11019)

Everytime after generating next logits 83.2% (9164 out of 11019)

Everytime after generating next logits 83.2% (9164 out of 11019)

Everytime after generating next logits 83.2% (9164 out of 11019)

Everytime after generating next logits 83.2% (9164 out of 11019)

Everytime after generating next logits 83.2% (9164 out of 11019)

Everytime after generating next logits 87.5% (9644 out of 11019)

Everytime after generating next logits 91.9% (10124 out of 11019)

Everytime after generating next logits 91.9% (10124 out of 11019)

Everytime after generating next logits 91.9% (10124 out of 11019)

Everytime after generating next logits 91.9% (10124 out of 11019)

Everytime after generating next logits 96.3% (10608 out of 11019)

Everytime after generating next logits 53.7% (5912 out of 11019)

Everytime after generating next logits 53.7% (5912 out of 11019)

Everytime after generating next logits 53.7% (5912 out of 11019)

Everytime after generating next logits 53.7% (5912 out of 11019)

Everytime after generating next logits 53.7% (5912 out of 11019)

Everytime after generating next logits 58.1% (6400 out of 11019)

Everytime after generating next logits 62.5% (6888 out of 11019)

Everytime after generating next logits 62.5% (6888 out of 11019)

Everytime after generating next logits 62.5% (6888 out of 11019)

Everytime after generating next logits 62.5% (6888 out of 11019)

Everytime after generating next logits 67.0% (7380 out of 11019)

Everytime after generating next logits 71.4% (7872 out of 11019)

Everytime after generating next logits 71.4% (7872 out of 11019)

Everytime after generating next logits 71.4% (7872 out of 11019)

Everytime after generating next logits 71.4% (7872 out of 11019)

Everytime after generating next logits 71.4% (7872 out of 11019)

Everytime after generating next logits 75.9% (8368 out of 11019)

Everytime after generating next logits 80.4% (8864 out of 11019)

Everytime after generating next logits 80.4% (8864 out of 11019)

Everytime after generating next logits 80.4% (8864 out of 11019)

Everytime after generating next logits 80.4% (8864 out of 11019)

Everytime after generating next logits 85.0% (9364 out of 11019)

Everytime after generating next logits 89.5% (9864 out of 11019)

Everytime after generating next logits 89.5% (9864 out of 11019)

Everytime after generating next logits 89.5% (9864 out of 11019)

Everytime after generating next logits 89.5% (9864 out of 11019)

Everytime after generating next logits 94.1% (10368 out of 11019)

Everytime after generating next logits 98.7% (10872 out of 11019)

Everytime after generating next logits 98.7% (10872 out of 11019)

Everytime after generating next logits 98.7% (10872 out of 11019)

Everytime after generating next logits 98.7% (10872 out of 11019)

Everytime after generating next logits 98.7% (10872 out of 11019)

Everytime after generating next logits 54.1% (5956 out of 11019)

Everytime after generating next logits 58.7% (6464 out of 11019)

Everytime after generating next logits 58.7% (6464 out of 11019)

Everytime after generating next logits 58.7% (6464 out of 11019)

Everytime after generating next logits 58.7% (6464 out of 11019)

Everytime after generating next logits 63.3% (6976 out of 11019)

Everytime after generating next logits 68.0% (7488 out of 11019)

Everytime after generating next logits 68.0% (7488 out of 11019)

Everytime after generating next logits 68.0% (7488 out of 11019)

Everytime after generating next logits 68.0% (7488 out of 11019)

Everytime after generating next logits 68.0% (7488 out of 11019)

Everytime after generating next logits 72.6% (8004 out of 11019)

Everytime after generating next logits 77.3% (8520 out of 11019)

Everytime after generating next logits 77.3% (8520 out of 11019)

Everytime after generating next logits 77.3% (8520 out of 11019)

Everytime after generating next logits 77.3% (8520 out of 11019)

Everytime after generating next logits 82.0% (9040 out of 11019)

Everytime after generating next logits 86.8% (9560 out of 11019)

Everytime after generating next logits 86.8% (9560 out of 11019)

Everytime after generating next logits 86.8% (9560 out of 11019)

Everytime after generating next logits 86.8% (9560 out of 11019)

Everytime after generating next logits 86.8% (9560 out of 11019)

Everytime after generating next logits 91.5% (10084 out of 11019)

Everytime after generating next logits 96.3% (10608 out of 11019)

Everytime after generating next logits 96.3% (10608 out of 11019)

Everytime after generating next logits 96.3% (10608 out of 11019)

Everytime after generating next logits 96.3% (10608 out of 11019)

Everytime after generating next logits 54.4% (5996 out of 11019)

Everytime after generating next logits 59.2% (6524 out of 11019)

Everytime after generating next logits 59.2% (6524 out of 11019)

Everytime after generating next logits 59.2% (6524 out of 11019)

Everytime after generating next logits 59.2% (6524 out of 11019)

Everytime after generating next logits 59.2% (6524 out of 11019)

Everytime after generating next logits 64.0% (7056 out of 11019)

Everytime after generating next logits 68.9% (7588 out of 11019)

Everytime after generating next logits 68.9% (7588 out of 11019)

Everytime after generating next logits 68.9% (7588 out of 11019)

Everytime after generating next logits 68.9% (7588 out of 11019)

Everytime after generating next logits 73.7% (8124 out of 11019)

Everytime after generating next logits 78.6% (8660 out of 11019)

Everytime after generating next logits 78.6% (8660 out of 11019)

Everytime after generating next logits 78.6% (8660 out of 11019)

Everytime after generating next logits 78.6% (8660 out of 11019)

Everytime after generating next logits 78.6% (8660 out of 11019)

Everytime after generating next logits 83.5% (9200 out of 11019)

Everytime after generating next logits 88.4% (9740 out of 11019)

Everytime after generating next logits 88.4% (9740 out of 11019)

Everytime after generating next logits 88.4% (9740 out of 11019)

Everytime after generating next logits 88.4% (9740 out of 11019)

Everytime after generating next logits 93.3% (10284 out of 11019)

Everytime after generating next logits 98.3% (10828 out of 11019)

Everytime after generating next logits 98.3% (10828 out of 11019)

Everytime after generating next logits 98.3% (10828 out of 11019)

Everytime after generating next logits 98.3% (10828 out of 11019)

Everytime after generating next logits 54.8% (6036 out of 11019)

Everytime after generating next logits 59.8% (6584 out of 11019)

Everytime after generating next logits 59.8% (6584 out of 11019)

Everytime after generating next logits 59.8% (6584 out of 11019)

Everytime after generating next logits 59.8% (6584 out of 11019)

Everytime after generating next logits 59.8% (6584 out of 11019)

Everytime after generating next logits 64.8% (7136 out of 11019)

Everytime after generating next logits 69.8% (7688 out of 11019)

Everytime after generating next logits 69.8% (7688 out of 11019)

Everytime after generating next logits 69.8% (7688 out of 11019)

Everytime after generating next logits 69.8% (7688 out of 11019)

Everytime after generating next logits 74.8% (8244 out of 11019)

Everytime after generating next logits 79.9% (8800 out of 11019)

Everytime after generating next logits 79.9% (8800 out of 11019)

Everytime after generating next logits 79.9% (8800 out of 11019)

Everytime after generating next logits 79.9% (8800 out of 11019)

Everytime after generating next logits 79.9% (8800 out of 11019)

Everytime after generating next logits 84.9% (9360 out of 11019)

Everytime after generating next logits 90.0% (9920 out of 11019)

Everytime after generating next logits 90.0% (9920 out of 11019)

Everytime after generating next logits 90.0% (9920 out of 11019)

Everytime after generating next logits 90.0% (9920 out of 11019)

Everytime after generating next logits 95.1% (10484 out of 11019)

Everytime after generating next logits 55.1% (6072 out of 11019)

Everytime after generating next logits 55.1% (6072 out of 11019)

Everytime after generating next logits 55.1% (6072 out of 11019)

Everytime after generating next logits 55.1% (6072 out of 11019)

Everytime after generating next logits 55.1% (6072 out of 11019)

Everytime after generating next logits 60.3% (6640 out of 11019)

Everytime after generating next logits 65.4% (7208 out of 11019)

Everytime after generating next logits 65.4% (7208 out of 11019)

Everytime after generating next logits 65.4% (7208 out of 11019)

Everytime after generating next logits 65.4% (7208 out of 11019)

Everytime after generating next logits 70.6% (7780 out of 11019)

Everytime after generating next logits 75.8% (8352 out of 11019)

Everytime after generating next logits 75.8% (8352 out of 11019)

Everytime after generating next logits 75.8% (8352 out of 11019)

Everytime after generating next logits 75.8% (8352 out of 11019)

Everytime after generating next logits 75.8% (8352 out of 11019)

Everytime after generating next logits 81.0% (8928 out of 11019)

Everytime after generating next logits 86.3% (9504 out of 11019)

Everytime after generating next logits 86.3% (9504 out of 11019)

Everytime after generating next logits 86.3% (9504 out of 11019)

Everytime after generating next logits 86.3% (9504 out of 11019)

Everytime after generating next logits 91.5% (10084 out of 11019)

Everytime after generating next logits 96.8% (10664 out of 11019)

Everytime after generating next logits 96.8% (10664 out of 11019)

Everytime after generating next logits 96.8% (10664 out of 11019)

Everytime after generating next logits 96.8% (10664 out of 11019)

Everytime after generating next logits 96.8% (10664 out of 11019)

Everytime after generating next logits 55.4% (6108 out of 11019)

Everytime after generating next logits 60.7% (6692 out of 11019)

Everytime after generating next logits 60.7% (6692 out of 11019)

Everytime after generating next logits 60.7% (6692 out of 11019)

Everytime after generating next logits 60.7% (6692 out of 11019)

Everytime after generating next logits 66.1% (7280 out of 11019)

Everytime after generating next logits 71.4% (7868 out of 11019)

Everytime after generating next logits 71.4% (7868 out of 11019)

Everytime after generating next logits 71.4% (7868 out of 11019)

Everytime after generating next logits 71.4% (7868 out of 11019)

Everytime after generating next logits 76.8% (8460 out of 11019)

Everytime after generating next logits 82.1% (9052 out of 11019)

Everytime after generating next logits 82.1% (9052 out of 11019)

Everytime after generating next logits 82.1% (9052 out of 11019)

Everytime after generating next logits 82.1% (9052 out of 11019)

Everytime after generating next logits 82.1% (9052 out of 11019)

Everytime after generating next logits 87.6% (9648 out of 11019)

Everytime after generating next logits 93.0% (10244 out of 11019)

Everytime after generating next logits 93.0% (10244 out of 11019)

Everytime after generating next logits 93.0% (10244 out of 11019)

Everytime after generating next logits 93.0% (10244 out of 11019)

Everytime after generating next logits 98.4% (10844 out of 11019)

Everytime after generating next logits 55.8% (6144 out of 11019)

Everytime after generating next logits 55.8% (6144 out of 11019)

Everytime after generating next logits 55.8% (6144 out of 11019)

Everytime after generating next logits 55.8% (6144 out of 11019)

Everytime after generating next logits 55.8% (6144 out of 11019)

Everytime after generating next logits 61.2% (6748 out of 11019)

Everytime after generating next logits 66.7% (7352 out of 11019)

Everytime after generating next logits 66.7% (7352 out of 11019)

Everytime after generating next logits 66.7% (7352 out of 11019)

Everytime after generating next logits 66.7% (7352 out of 11019)

Everytime after generating next logits 72.2% (7960 out of 11019)

Everytime after generating next logits 77.8% (8568 out of 11019)

Everytime after generating next logits 77.8% (8568 out of 11019)

Everytime after generating next logits 77.8% (8568 out of 11019)

Everytime after generating next logits 77.8% (8568 out of 11019)

Everytime after generating next logits 77.8% (8568 out of 11019)

Everytime after generating next logits 83.3% (9180 out of 11019)

Everytime after generating next logits 88.9% (9792 out of 11019)

Everytime after generating next logits 88.9% (9792 out of 11019)

Everytime after generating next logits 88.9% (9792 out of 11019)

Everytime after generating next logits 88.9% (9792 out of 11019)

Everytime after generating next logits 94.5% (10408 out of 11019)

Everytime after generating next logits 56.0% (6176 out of 11019)

Everytime after generating next logits 56.0% (6176 out of 11019)

Everytime after generating next logits 56.0% (6176 out of 11019)

Everytime after generating next logits 56.0% (6176 out of 11019)

Everytime after generating next logits 56.0% (6176 out of 11019)

Everytime after generating next logits 61.7% (6796 out of 11019)

Everytime after generating next logits 67.3% (7416 out of 11019)

Everytime after generating next logits 67.3% (7416 out of 11019)

Everytime after generating next logits 67.3% (7416 out of 11019)

Everytime after generating next logits 67.3% (7416 out of 11019)

Everytime after generating next logits 73.0% (8040 out of 11019)

Everytime after generating next logits 78.6% (8664 out of 11019)

Everytime after generating next logits 78.6% (8664 out of 11019)

Everytime after generating next logits 78.6% (8664 out of 11019)

Everytime after generating next logits 78.6% (8664 out of 11019)

Everytime after generating next logits 78.6% (8664 out of 11019)

Everytime after generating next logits 84.3% (9292 out of 11019)

Everytime after generating next logits 90.0% (9920 out of 11019)

Everytime after generating next logits 90.0% (9920 out of 11019)

Everytime after generating next logits 90.0% (9920 out of 11019)

Everytime after generating next logits 90.0% (9920 out of 11019)

Everytime after generating next logits 95.8% (10552 out of 11019)

Everytime after generating next logits 56.3% (6208 out of 11019)

Everytime after generating next logits 56.3% (6208 out of 11019)

Everytime after generating next logits 56.3% (6208 out of 11019)

Everytime after generating next logits 56.3% (6208 out of 11019)

Everytime after generating next logits 62.1% (6844 out of 11019)

Everytime after generating next logits 67.9% (7480 out of 11019)

Everytime after generating next logits 67.9% (7480 out of 11019)

Everytime after generating next logits 67.9% (7480 out of 11019)

Everytime after generating next logits 67.9% (7480 out of 11019)

Everytime after generating next logits 67.9% (7480 out of 11019)

Everytime after generating next logits 73.7% (8120 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 79.5% (8760 out of 11019)

Everytime after generating next logits 85.3% (9404 out of 11019)

Everytime after generating next logits 91.2% (10048 out of 11019)

Everytime after generating next logits 91.2% (10048 out of 11019)

Everytime after generating next logits 91.2% (10048 out of 11019)

Everytime after generating next logits 91.2% (10048 out of 11019)

Everytime after generating next logits 91.2% (10048 out of 11019)

Everytime after generating next logits 97.1% (10696 out of 11019)

Everytime after generating next logits 56.6% (6240 out of 11019)

Everytime after generating next logits 56.6% (6240 out of 11019)

Everytime after generating next logits 56.6% (6240 out of 11019)

Everytime after generating next logits 56.6% (6240 out of 11019)

Everytime after generating next logits 62.5% (6892 out of 11019)

Everytime after generating next logits 68.5% (7544 out of 11019)

Everytime after generating next logits 68.5% (7544 out of 11019)

Everytime after generating next logits 68.5% (7544 out of 11019)

Everytime after generating next logits 68.5% (7544 out of 11019)

Everytime after generating next logits 68.5% (7544 out of 11019)

Everytime after generating next logits 74.4% (8200 out of 11019)

Everytime after generating next logits 80.4% (8856 out of 11019)

Everytime after generating next logits 80.4% (8856 out of 11019)

Everytime after generating next logits 80.4% (8856 out of 11019)

Everytime after generating next logits 80.4% (8856 out of 11019)

Everytime after generating next logits 86.4% (9516 out of 11019)

Everytime after generating next logits 92.3% (10176 out of 11019)

Everytime after generating next logits 92.3% (10176 out of 11019)

Everytime after generating next logits 92.3% (10176 out of 11019)

Everytime after generating next logits 92.3% (10176 out of 11019)

Everytime after generating next logits 92.3% (10176 out of 11019)

Everytime after generating next logits 98.4% (10840 out of 11019)

Everytime after generating next logits 56.9% (6272 out of 11019)

Everytime after generating next logits 56.9% (6272 out of 11019)

Everytime after generating next logits 56.9% (6272 out of 11019)

Everytime after generating next logits 56.9% (6272 out of 11019)

Everytime after generating next logits 63.0% (6940 out of 11019)

Everytime after generating next logits 69.0% (7608 out of 11019)

Everytime after generating next logits 69.0% (7608 out of 11019)

Everytime after generating next logits 69.0% (7608 out of 11019)

Everytime after generating next logits 69.0% (7608 out of 11019)

Everytime after generating next logits 69.0% (7608 out of 11019)

Everytime after generating next logits 75.1% (8280 out of 11019)

Everytime after generating next logits 81.2% (8952 out of 11019)

Everytime after generating next logits 81.2% (8952 out of 11019)

Everytime after generating next logits 81.2% (8952 out of 11019)

Everytime after generating next logits 81.2% (8952 out of 11019)

Everytime after generating next logits 87.4% (9628 out of 11019)

Everytime after generating next logits 93.5% (10304 out of 11019)

Everytime after generating next logits 93.5% (10304 out of 11019)

Everytime after generating next logits 93.5% (10304 out of 11019)

Everytime after generating next logits 93.5% (10304 out of 11019)

Everytime after generating next logits 99.7% (10984 out of 11019)

Everytime after generating next logits 57.2% (6304 out of 11019)

Everytime after generating next logits 57.2% (6304 out of 11019)

Everytime after generating next logits 57.2% (6304 out of 11019)

Everytime after generating next logits 57.2% (6304 out of 11019)

Everytime after generating next logits 57.2% (6304 out of 11019)

Everytime after generating next logits 63.4% (6988 out of 11019)

Everytime after generating next logits 69.6% (7672 out of 11019)

Everytime after generating next logits 69.6% (7672 out of 11019)

Everytime after generating next logits 69.6% (7672 out of 11019)

Everytime after generating next logits 69.6% (7672 out of 11019)

Everytime after generating next logits 75.9% (8360 out of 11019)

Everytime after generating next logits 82.1% (9048 out of 11019)

Everytime after generating next logits 82.1% (9048 out of 11019)

Everytime after generating next logits 82.1% (9048 out of 11019)

Everytime after generating next logits 82.1% (9048 out of 11019)

Everytime after generating next logits 82.1% (9048 out of 11019)

Everytime after generating next logits 88.4% (9740 out of 11019)

Everytime after generating next logits 94.7% (10432 out of 11019)

Everytime after generating next logits 94.7% (10432 out of 11019)

Everytime after generating next logits 94.7% (10432 out of 11019)

Everytime after generating next logits 94.7% (10432 out of 11019)

Everytime after generating next logits 57.5% (6332 out of 11019)

Everytime after generating next logits 63.8% (7028 out of 11019)

Everytime after generating next logits 63.8% (7028 out of 11019)

Everytime after generating next logits 63.8% (7028 out of 11019)

Everytime after generating next logits 63.8% (7028 out of 11019)

Everytime after generating next logits 63.8% (7028 out of 11019)
Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated Estimated
iter = 3 reward = 948
final_logits =  tensor([[[-1.8259e-01,  3.6951e+00,  2.3495e-01,  ..., -5.6791e+00,
          -7.4908e-01,  5.9522e-03],
         [-4.4687e-01,  2.7375e+00, -4.1052e-01,  ..., -4.8109e+00,
           1.1579e+00, -2.2200e+00],
         [-4.5341e-01,  2.6390e+00, -3.9726e-01,  ..., -4.6950e+00,
           1.0352e+00, -2.2946e+00],
         ...,
         [-4.7410e-01,  1.0919e+01, -7.1508e-02,  ..., -2.8262e+00,
           9.9165e-01, -1.7962e+00],
         [-4.7397e-01,  1.1015e+01, -9.5559e-02,  ..., -2.8274e+00,
           9.3508e-01, -1.7575e+00],
         [-4.7172e-01,  1.1210e+01, -1.3032e-01,  ..., -2.8055e+00,
           8.3252e-01, -1.7140e+00]]], device='cuda:0', grad_fn=<AddBackward0>)
actor_loss =  tensor(334.1351, device='cuda:0', grad_fn=<NegBackward>)
critic_loss =  tensor(253676.4844, device='cuda:0', grad_fn=<MseLossBackward>)
